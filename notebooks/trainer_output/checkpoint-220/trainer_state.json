{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 220,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 22.375,
      "learning_rate": 0.0,
      "loss": 2.4323,
      "step": 1
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 21.25,
      "learning_rate": 9.090909090909091e-07,
      "loss": 2.4362,
      "step": 2
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 22.75,
      "learning_rate": 1.8181818181818183e-06,
      "loss": 2.5685,
      "step": 3
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 20.125,
      "learning_rate": 2.7272727272727272e-06,
      "loss": 2.4956,
      "step": 4
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 19.125,
      "learning_rate": 3.6363636363636366e-06,
      "loss": 2.4373,
      "step": 5
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 21.75,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 2.2825,
      "step": 6
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 21.0,
      "learning_rate": 5.4545454545454545e-06,
      "loss": 2.3781,
      "step": 7
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 19.25,
      "learning_rate": 6.363636363636364e-06,
      "loss": 2.1521,
      "step": 8
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 21.625,
      "learning_rate": 7.272727272727273e-06,
      "loss": 2.357,
      "step": 9
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 19.125,
      "learning_rate": 8.181818181818183e-06,
      "loss": 2.187,
      "step": 10
    },
    {
      "epoch": 1.0,
      "grad_norm": 18.375,
      "learning_rate": 9.090909090909091e-06,
      "loss": 2.0214,
      "step": 11
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 18.5,
      "learning_rate": 1e-05,
      "loss": 1.993,
      "step": 12
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 17.0,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 1.7329,
      "step": 13
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 18.375,
      "learning_rate": 1.181818181818182e-05,
      "loss": 1.836,
      "step": 14
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 18.75,
      "learning_rate": 1.2727272727272728e-05,
      "loss": 1.6781,
      "step": 15
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 15.5,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 1.4001,
      "step": 16
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 12.5,
      "learning_rate": 1.4545454545454546e-05,
      "loss": 1.4803,
      "step": 17
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 11.9375,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 1.3439,
      "step": 18
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 16.125,
      "learning_rate": 1.6363636363636366e-05,
      "loss": 1.3052,
      "step": 19
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 10.3125,
      "learning_rate": 1.7272727272727274e-05,
      "loss": 1.2281,
      "step": 20
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 9.5625,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.2037,
      "step": 21
    },
    {
      "epoch": 2.0,
      "grad_norm": 8.6875,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 1.2665,
      "step": 22
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 5.75,
      "learning_rate": 2e-05,
      "loss": 1.2265,
      "step": 23
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 6.0,
      "learning_rate": 1.9998741276738753e-05,
      "loss": 1.074,
      "step": 24
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 6.25,
      "learning_rate": 1.9994965423831853e-05,
      "loss": 0.9745,
      "step": 25
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 5.53125,
      "learning_rate": 1.9988673391830082e-05,
      "loss": 1.0244,
      "step": 26
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 7.03125,
      "learning_rate": 1.9979866764718846e-05,
      "loss": 1.1355,
      "step": 27
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 6.5,
      "learning_rate": 1.9968547759519426e-05,
      "loss": 1.0229,
      "step": 28
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 6.78125,
      "learning_rate": 1.9954719225730847e-05,
      "loss": 1.0819,
      "step": 29
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 6.25,
      "learning_rate": 1.9938384644612542e-05,
      "loss": 1.0274,
      "step": 30
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 7.53125,
      "learning_rate": 1.9919548128307954e-05,
      "loss": 1.0938,
      "step": 31
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 6.21875,
      "learning_rate": 1.989821441880933e-05,
      "loss": 1.031,
      "step": 32
    },
    {
      "epoch": 3.0,
      "grad_norm": 6.53125,
      "learning_rate": 1.9874388886763944e-05,
      "loss": 1.0127,
      "step": 33
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 5.90625,
      "learning_rate": 1.9848077530122083e-05,
      "loss": 0.9732,
      "step": 34
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 5.1875,
      "learning_rate": 1.9819286972627066e-05,
      "loss": 0.9095,
      "step": 35
    },
    {
      "epoch": 3.2727272727272725,
      "grad_norm": 5.6875,
      "learning_rate": 1.978802446214779e-05,
      "loss": 0.8307,
      "step": 36
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 5.4375,
      "learning_rate": 1.9754297868854075e-05,
      "loss": 0.8332,
      "step": 37
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 9.375,
      "learning_rate": 1.9718115683235418e-05,
      "loss": 0.9016,
      "step": 38
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 8.9375,
      "learning_rate": 1.9679487013963566e-05,
      "loss": 0.7582,
      "step": 39
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 7.28125,
      "learning_rate": 1.9638421585599422e-05,
      "loss": 0.8949,
      "step": 40
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 6.3125,
      "learning_rate": 1.9594929736144978e-05,
      "loss": 0.8283,
      "step": 41
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 6.0625,
      "learning_rate": 1.9549022414440738e-05,
      "loss": 0.8504,
      "step": 42
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 5.65625,
      "learning_rate": 1.9500711177409456e-05,
      "loss": 0.7212,
      "step": 43
    },
    {
      "epoch": 4.0,
      "grad_norm": 5.65625,
      "learning_rate": 1.9450008187146685e-05,
      "loss": 0.8265,
      "step": 44
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 6.25,
      "learning_rate": 1.9396926207859085e-05,
      "loss": 0.6454,
      "step": 45
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 5.8125,
      "learning_rate": 1.9341478602651068e-05,
      "loss": 0.6797,
      "step": 46
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 5.84375,
      "learning_rate": 1.9283679330160726e-05,
      "loss": 0.693,
      "step": 47
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 8.0625,
      "learning_rate": 1.9223542941045817e-05,
      "loss": 0.7147,
      "step": 48
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 9.9375,
      "learning_rate": 1.9161084574320696e-05,
      "loss": 0.6258,
      "step": 49
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 9.3125,
      "learning_rate": 1.9096319953545186e-05,
      "loss": 0.644,
      "step": 50
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 6.5,
      "learning_rate": 1.9029265382866216e-05,
      "loss": 0.6398,
      "step": 51
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 6.21875,
      "learning_rate": 1.895993774291336e-05,
      "loss": 0.7459,
      "step": 52
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 5.9375,
      "learning_rate": 1.8888354486549238e-05,
      "loss": 0.6334,
      "step": 53
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 6.1875,
      "learning_rate": 1.881453363447582e-05,
      "loss": 0.578,
      "step": 54
    },
    {
      "epoch": 5.0,
      "grad_norm": 6.28125,
      "learning_rate": 1.873849377069785e-05,
      "loss": 0.6372,
      "step": 55
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 7.0,
      "learning_rate": 1.866025403784439e-05,
      "loss": 0.4828,
      "step": 56
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 6.0625,
      "learning_rate": 1.8579834132349773e-05,
      "loss": 0.5034,
      "step": 57
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 6.15625,
      "learning_rate": 1.8497254299495147e-05,
      "loss": 0.4695,
      "step": 58
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 8.125,
      "learning_rate": 1.8412535328311813e-05,
      "loss": 0.4902,
      "step": 59
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 8.3125,
      "learning_rate": 1.8325698546347714e-05,
      "loss": 0.5353,
      "step": 60
    },
    {
      "epoch": 5.545454545454545,
      "grad_norm": 12.5,
      "learning_rate": 1.8236765814298328e-05,
      "loss": 0.5385,
      "step": 61
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 9.3125,
      "learning_rate": 1.814575952050336e-05,
      "loss": 0.4836,
      "step": 62
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 6.65625,
      "learning_rate": 1.8052702575310588e-05,
      "loss": 0.4129,
      "step": 63
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 6.40625,
      "learning_rate": 1.7957618405308323e-05,
      "loss": 0.435,
      "step": 64
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 5.9375,
      "learning_rate": 1.7860530947427878e-05,
      "loss": 0.473,
      "step": 65
    },
    {
      "epoch": 6.0,
      "grad_norm": 6.1875,
      "learning_rate": 1.776146464291757e-05,
      "loss": 0.4671,
      "step": 66
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 6.6875,
      "learning_rate": 1.766044443118978e-05,
      "loss": 0.3343,
      "step": 67
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 6.65625,
      "learning_rate": 1.7557495743542586e-05,
      "loss": 0.3351,
      "step": 68
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 6.28125,
      "learning_rate": 1.745264449675755e-05,
      "loss": 0.3191,
      "step": 69
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 9.3125,
      "learning_rate": 1.734591708657533e-05,
      "loss": 0.2802,
      "step": 70
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 13.6875,
      "learning_rate": 1.72373403810507e-05,
      "loss": 0.3301,
      "step": 71
    },
    {
      "epoch": 6.545454545454545,
      "grad_norm": 14.375,
      "learning_rate": 1.7126941713788633e-05,
      "loss": 0.362,
      "step": 72
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 11.9375,
      "learning_rate": 1.7014748877063212e-05,
      "loss": 0.3827,
      "step": 73
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 12.8125,
      "learning_rate": 1.6900790114821122e-05,
      "loss": 0.3647,
      "step": 74
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 7.3125,
      "learning_rate": 1.6785094115571323e-05,
      "loss": 0.3062,
      "step": 75
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 6.9375,
      "learning_rate": 1.666769000516292e-05,
      "loss": 0.3387,
      "step": 76
    },
    {
      "epoch": 7.0,
      "grad_norm": 5.96875,
      "learning_rate": 1.6548607339452853e-05,
      "loss": 0.2764,
      "step": 77
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 9.0625,
      "learning_rate": 1.6427876096865394e-05,
      "loss": 0.2919,
      "step": 78
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 6.8125,
      "learning_rate": 1.6305526670845225e-05,
      "loss": 0.249,
      "step": 79
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 7.90625,
      "learning_rate": 1.6181589862206053e-05,
      "loss": 0.268,
      "step": 80
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 6.4375,
      "learning_rate": 1.6056096871376667e-05,
      "loss": 0.1972,
      "step": 81
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 7.28125,
      "learning_rate": 1.5929079290546408e-05,
      "loss": 0.2207,
      "step": 82
    },
    {
      "epoch": 7.545454545454545,
      "grad_norm": 8.0,
      "learning_rate": 1.5800569095711983e-05,
      "loss": 0.1962,
      "step": 83
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 9.125,
      "learning_rate": 1.5670598638627707e-05,
      "loss": 0.2191,
      "step": 84
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 12.625,
      "learning_rate": 1.5539200638661106e-05,
      "loss": 0.202,
      "step": 85
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 11.9375,
      "learning_rate": 1.5406408174555978e-05,
      "loss": 0.2251,
      "step": 86
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 13.125,
      "learning_rate": 1.5272254676105026e-05,
      "loss": 0.2096,
      "step": 87
    },
    {
      "epoch": 8.0,
      "grad_norm": 17.5,
      "learning_rate": 1.5136773915734067e-05,
      "loss": 0.2459,
      "step": 88
    },
    {
      "epoch": 8.090909090909092,
      "grad_norm": 8.125,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.1876,
      "step": 89
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 7.0,
      "learning_rate": 1.4861967361004687e-05,
      "loss": 0.1353,
      "step": 90
    },
    {
      "epoch": 8.272727272727273,
      "grad_norm": 10.5625,
      "learning_rate": 1.472271074772683e-05,
      "loss": 0.1595,
      "step": 91
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 6.28125,
      "learning_rate": 1.4582265217274105e-05,
      "loss": 0.1599,
      "step": 92
    },
    {
      "epoch": 8.454545454545455,
      "grad_norm": 6.4375,
      "learning_rate": 1.4440666126057743e-05,
      "loss": 0.1994,
      "step": 93
    },
    {
      "epoch": 8.545454545454545,
      "grad_norm": 5.1875,
      "learning_rate": 1.4297949120891718e-05,
      "loss": 0.1463,
      "step": 94
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 4.59375,
      "learning_rate": 1.4154150130018867e-05,
      "loss": 0.1429,
      "step": 95
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 6.5625,
      "learning_rate": 1.4009305354066138e-05,
      "loss": 0.1459,
      "step": 96
    },
    {
      "epoch": 8.818181818181818,
      "grad_norm": 5.21875,
      "learning_rate": 1.3863451256931286e-05,
      "loss": 0.1074,
      "step": 97
    },
    {
      "epoch": 8.909090909090908,
      "grad_norm": 5.28125,
      "learning_rate": 1.3716624556603275e-05,
      "loss": 0.1242,
      "step": 98
    },
    {
      "epoch": 9.0,
      "grad_norm": 6.6875,
      "learning_rate": 1.356886221591872e-05,
      "loss": 0.1338,
      "step": 99
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 4.65625,
      "learning_rate": 1.342020143325669e-05,
      "loss": 0.1129,
      "step": 100
    },
    {
      "epoch": 9.181818181818182,
      "grad_norm": 7.21875,
      "learning_rate": 1.3270679633174219e-05,
      "loss": 0.1089,
      "step": 101
    },
    {
      "epoch": 9.272727272727273,
      "grad_norm": 6.4375,
      "learning_rate": 1.3120334456984871e-05,
      "loss": 0.1169,
      "step": 102
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 5.125,
      "learning_rate": 1.296920375328275e-05,
      "loss": 0.1,
      "step": 103
    },
    {
      "epoch": 9.454545454545455,
      "grad_norm": 7.4375,
      "learning_rate": 1.2817325568414299e-05,
      "loss": 0.1244,
      "step": 104
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 7.9375,
      "learning_rate": 1.266473813690035e-05,
      "loss": 0.1118,
      "step": 105
    },
    {
      "epoch": 9.636363636363637,
      "grad_norm": 7.4375,
      "learning_rate": 1.2511479871810792e-05,
      "loss": 0.1334,
      "step": 106
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 6.53125,
      "learning_rate": 1.2357589355094275e-05,
      "loss": 0.0943,
      "step": 107
    },
    {
      "epoch": 9.818181818181818,
      "grad_norm": 6.15625,
      "learning_rate": 1.2203105327865407e-05,
      "loss": 0.0952,
      "step": 108
    },
    {
      "epoch": 9.909090909090908,
      "grad_norm": 5.5625,
      "learning_rate": 1.2048066680651908e-05,
      "loss": 0.0999,
      "step": 109
    },
    {
      "epoch": 10.0,
      "grad_norm": 3.078125,
      "learning_rate": 1.1892512443604103e-05,
      "loss": 0.0855,
      "step": 110
    },
    {
      "epoch": 10.090909090909092,
      "grad_norm": 3.625,
      "learning_rate": 1.1736481776669307e-05,
      "loss": 0.0752,
      "step": 111
    },
    {
      "epoch": 10.181818181818182,
      "grad_norm": 2.96875,
      "learning_rate": 1.15800139597335e-05,
      "loss": 0.0794,
      "step": 112
    },
    {
      "epoch": 10.272727272727273,
      "grad_norm": 3.453125,
      "learning_rate": 1.1423148382732854e-05,
      "loss": 0.0855,
      "step": 113
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 3.25,
      "learning_rate": 1.1265924535737494e-05,
      "loss": 0.072,
      "step": 114
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 3.59375,
      "learning_rate": 1.1108381999010111e-05,
      "loss": 0.0867,
      "step": 115
    },
    {
      "epoch": 10.545454545454545,
      "grad_norm": 5.375,
      "learning_rate": 1.0950560433041825e-05,
      "loss": 0.0878,
      "step": 116
    },
    {
      "epoch": 10.636363636363637,
      "grad_norm": 4.375,
      "learning_rate": 1.0792499568567885e-05,
      "loss": 0.0874,
      "step": 117
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 3.828125,
      "learning_rate": 1.0634239196565646e-05,
      "loss": 0.0876,
      "step": 118
    },
    {
      "epoch": 10.818181818181818,
      "grad_norm": 4.28125,
      "learning_rate": 1.0475819158237426e-05,
      "loss": 0.0825,
      "step": 119
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 5.84375,
      "learning_rate": 1.031727933498068e-05,
      "loss": 0.0839,
      "step": 120
    },
    {
      "epoch": 11.0,
      "grad_norm": 5.59375,
      "learning_rate": 1.015865963834808e-05,
      "loss": 0.0876,
      "step": 121
    },
    {
      "epoch": 11.090909090909092,
      "grad_norm": 3.109375,
      "learning_rate": 1e-05,
      "loss": 0.0601,
      "step": 122
    },
    {
      "epoch": 11.181818181818182,
      "grad_norm": 2.84375,
      "learning_rate": 9.841340361651921e-06,
      "loss": 0.0566,
      "step": 123
    },
    {
      "epoch": 11.272727272727273,
      "grad_norm": 3.40625,
      "learning_rate": 9.682720665019325e-06,
      "loss": 0.07,
      "step": 124
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 2.1875,
      "learning_rate": 9.524180841762577e-06,
      "loss": 0.061,
      "step": 125
    },
    {
      "epoch": 11.454545454545455,
      "grad_norm": 4.625,
      "learning_rate": 9.365760803434356e-06,
      "loss": 0.0812,
      "step": 126
    },
    {
      "epoch": 11.545454545454545,
      "grad_norm": 4.21875,
      "learning_rate": 9.207500431432115e-06,
      "loss": 0.0705,
      "step": 127
    },
    {
      "epoch": 11.636363636363637,
      "grad_norm": 3.5,
      "learning_rate": 9.049439566958176e-06,
      "loss": 0.0705,
      "step": 128
    },
    {
      "epoch": 11.727272727272727,
      "grad_norm": 2.765625,
      "learning_rate": 8.89161800098989e-06,
      "loss": 0.0583,
      "step": 129
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 2.96875,
      "learning_rate": 8.734075464262507e-06,
      "loss": 0.0659,
      "step": 130
    },
    {
      "epoch": 11.909090909090908,
      "grad_norm": 3.71875,
      "learning_rate": 8.576851617267151e-06,
      "loss": 0.0618,
      "step": 131
    },
    {
      "epoch": 12.0,
      "grad_norm": 5.0,
      "learning_rate": 8.419986040266502e-06,
      "loss": 0.0759,
      "step": 132
    },
    {
      "epoch": 12.090909090909092,
      "grad_norm": 2.625,
      "learning_rate": 8.263518223330698e-06,
      "loss": 0.0527,
      "step": 133
    },
    {
      "epoch": 12.181818181818182,
      "grad_norm": 2.90625,
      "learning_rate": 8.107487556395902e-06,
      "loss": 0.054,
      "step": 134
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 3.609375,
      "learning_rate": 7.951933319348095e-06,
      "loss": 0.073,
      "step": 135
    },
    {
      "epoch": 12.363636363636363,
      "grad_norm": 2.21875,
      "learning_rate": 7.796894672134594e-06,
      "loss": 0.0474,
      "step": 136
    },
    {
      "epoch": 12.454545454545455,
      "grad_norm": 2.78125,
      "learning_rate": 7.642410644905726e-06,
      "loss": 0.0612,
      "step": 137
    },
    {
      "epoch": 12.545454545454545,
      "grad_norm": 2.578125,
      "learning_rate": 7.488520128189209e-06,
      "loss": 0.059,
      "step": 138
    },
    {
      "epoch": 12.636363636363637,
      "grad_norm": 3.46875,
      "learning_rate": 7.335261863099652e-06,
      "loss": 0.0624,
      "step": 139
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 2.96875,
      "learning_rate": 7.182674431585703e-06,
      "loss": 0.0602,
      "step": 140
    },
    {
      "epoch": 12.818181818181818,
      "grad_norm": 4.0,
      "learning_rate": 7.0307962467172555e-06,
      "loss": 0.0543,
      "step": 141
    },
    {
      "epoch": 12.909090909090908,
      "grad_norm": 3.078125,
      "learning_rate": 6.87966554301513e-06,
      "loss": 0.0572,
      "step": 142
    },
    {
      "epoch": 13.0,
      "grad_norm": 3.25,
      "learning_rate": 6.729320366825785e-06,
      "loss": 0.0551,
      "step": 143
    },
    {
      "epoch": 13.090909090909092,
      "grad_norm": 3.28125,
      "learning_rate": 6.579798566743314e-06,
      "loss": 0.0611,
      "step": 144
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 2.90625,
      "learning_rate": 6.431137784081283e-06,
      "loss": 0.0576,
      "step": 145
    },
    {
      "epoch": 13.272727272727273,
      "grad_norm": 2.78125,
      "learning_rate": 6.283375443396726e-06,
      "loss": 0.0536,
      "step": 146
    },
    {
      "epoch": 13.363636363636363,
      "grad_norm": 2.6875,
      "learning_rate": 6.136548743068713e-06,
      "loss": 0.0512,
      "step": 147
    },
    {
      "epoch": 13.454545454545455,
      "grad_norm": 2.890625,
      "learning_rate": 5.990694645933866e-06,
      "loss": 0.056,
      "step": 148
    },
    {
      "epoch": 13.545454545454545,
      "grad_norm": 3.09375,
      "learning_rate": 5.845849869981137e-06,
      "loss": 0.0541,
      "step": 149
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 2.59375,
      "learning_rate": 5.702050879108284e-06,
      "loss": 0.0522,
      "step": 150
    },
    {
      "epoch": 13.727272727272727,
      "grad_norm": 3.03125,
      "learning_rate": 5.559333873942259e-06,
      "loss": 0.0492,
      "step": 151
    },
    {
      "epoch": 13.818181818181818,
      "grad_norm": 2.90625,
      "learning_rate": 5.417734782725896e-06,
      "loss": 0.0464,
      "step": 152
    },
    {
      "epoch": 13.909090909090908,
      "grad_norm": 2.890625,
      "learning_rate": 5.277289252273175e-06,
      "loss": 0.0588,
      "step": 153
    },
    {
      "epoch": 14.0,
      "grad_norm": 2.484375,
      "learning_rate": 5.138032638995315e-06,
      "loss": 0.0489,
      "step": 154
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 2.875,
      "learning_rate": 5.000000000000003e-06,
      "loss": 0.0505,
      "step": 155
    },
    {
      "epoch": 14.181818181818182,
      "grad_norm": 2.875,
      "learning_rate": 4.863226084265939e-06,
      "loss": 0.0571,
      "step": 156
    },
    {
      "epoch": 14.272727272727273,
      "grad_norm": 3.015625,
      "learning_rate": 4.727745323894976e-06,
      "loss": 0.0487,
      "step": 157
    },
    {
      "epoch": 14.363636363636363,
      "grad_norm": 2.484375,
      "learning_rate": 4.593591825444028e-06,
      "loss": 0.0468,
      "step": 158
    },
    {
      "epoch": 14.454545454545455,
      "grad_norm": 2.96875,
      "learning_rate": 4.460799361338898e-06,
      "loss": 0.0471,
      "step": 159
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 2.890625,
      "learning_rate": 4.3294013613722944e-06,
      "loss": 0.057,
      "step": 160
    },
    {
      "epoch": 14.636363636363637,
      "grad_norm": 2.515625,
      "learning_rate": 4.19943090428802e-06,
      "loss": 0.0502,
      "step": 161
    },
    {
      "epoch": 14.727272727272727,
      "grad_norm": 2.984375,
      "learning_rate": 4.070920709453597e-06,
      "loss": 0.055,
      "step": 162
    },
    {
      "epoch": 14.818181818181818,
      "grad_norm": 2.390625,
      "learning_rate": 3.943903128623336e-06,
      "loss": 0.0448,
      "step": 163
    },
    {
      "epoch": 14.909090909090908,
      "grad_norm": 3.25,
      "learning_rate": 3.818410137793947e-06,
      "loss": 0.0509,
      "step": 164
    },
    {
      "epoch": 15.0,
      "grad_norm": 3.625,
      "learning_rate": 3.6944733291547784e-06,
      "loss": 0.0514,
      "step": 165
    },
    {
      "epoch": 15.090909090909092,
      "grad_norm": 2.5625,
      "learning_rate": 3.5721239031346067e-06,
      "loss": 0.0517,
      "step": 166
    },
    {
      "epoch": 15.181818181818182,
      "grad_norm": 2.609375,
      "learning_rate": 3.4513926605471504e-06,
      "loss": 0.048,
      "step": 167
    },
    {
      "epoch": 15.272727272727273,
      "grad_norm": 3.078125,
      "learning_rate": 3.3323099948370853e-06,
      "loss": 0.0495,
      "step": 168
    },
    {
      "epoch": 15.363636363636363,
      "grad_norm": 2.171875,
      "learning_rate": 3.2149058844286796e-06,
      "loss": 0.0496,
      "step": 169
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 3.34375,
      "learning_rate": 3.099209885178882e-06,
      "loss": 0.0531,
      "step": 170
    },
    {
      "epoch": 15.545454545454545,
      "grad_norm": 3.0625,
      "learning_rate": 2.9852511229367862e-06,
      "loss": 0.0519,
      "step": 171
    },
    {
      "epoch": 15.636363636363637,
      "grad_norm": 2.546875,
      "learning_rate": 2.8730582862113743e-06,
      "loss": 0.0437,
      "step": 172
    },
    {
      "epoch": 15.727272727272727,
      "grad_norm": 3.015625,
      "learning_rate": 2.7626596189492983e-06,
      "loss": 0.0503,
      "step": 173
    },
    {
      "epoch": 15.818181818181818,
      "grad_norm": 2.5625,
      "learning_rate": 2.6540829134246683e-06,
      "loss": 0.0462,
      "step": 174
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 2.9375,
      "learning_rate": 2.5473555032424534e-06,
      "loss": 0.05,
      "step": 175
    },
    {
      "epoch": 16.0,
      "grad_norm": 2.59375,
      "learning_rate": 2.4425042564574186e-06,
      "loss": 0.0477,
      "step": 176
    },
    {
      "epoch": 16.09090909090909,
      "grad_norm": 2.5625,
      "learning_rate": 2.339555568810221e-06,
      "loss": 0.0498,
      "step": 177
    },
    {
      "epoch": 16.181818181818183,
      "grad_norm": 2.421875,
      "learning_rate": 2.2385353570824308e-06,
      "loss": 0.0424,
      "step": 178
    },
    {
      "epoch": 16.272727272727273,
      "grad_norm": 3.0,
      "learning_rate": 2.1394690525721275e-06,
      "loss": 0.051,
      "step": 179
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 2.640625,
      "learning_rate": 2.0423815946916783e-06,
      "loss": 0.0459,
      "step": 180
    },
    {
      "epoch": 16.454545454545453,
      "grad_norm": 2.84375,
      "learning_rate": 1.947297424689414e-06,
      "loss": 0.0493,
      "step": 181
    },
    {
      "epoch": 16.545454545454547,
      "grad_norm": 2.984375,
      "learning_rate": 1.854240479496643e-06,
      "loss": 0.048,
      "step": 182
    },
    {
      "epoch": 16.636363636363637,
      "grad_norm": 3.1875,
      "learning_rate": 1.7632341857016733e-06,
      "loss": 0.0469,
      "step": 183
    },
    {
      "epoch": 16.727272727272727,
      "grad_norm": 2.515625,
      "learning_rate": 1.6743014536522872e-06,
      "loss": 0.0495,
      "step": 184
    },
    {
      "epoch": 16.818181818181817,
      "grad_norm": 2.890625,
      "learning_rate": 1.587464671688187e-06,
      "loss": 0.051,
      "step": 185
    },
    {
      "epoch": 16.90909090909091,
      "grad_norm": 2.84375,
      "learning_rate": 1.5027457005048573e-06,
      "loss": 0.0512,
      "step": 186
    },
    {
      "epoch": 17.0,
      "grad_norm": 2.53125,
      "learning_rate": 1.4201658676502294e-06,
      "loss": 0.0455,
      "step": 187
    },
    {
      "epoch": 17.09090909090909,
      "grad_norm": 2.65625,
      "learning_rate": 1.339745962155613e-06,
      "loss": 0.0479,
      "step": 188
    },
    {
      "epoch": 17.181818181818183,
      "grad_norm": 2.8125,
      "learning_rate": 1.2615062293021508e-06,
      "loss": 0.0523,
      "step": 189
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 2.78125,
      "learning_rate": 1.1854663655241804e-06,
      "loss": 0.0443,
      "step": 190
    },
    {
      "epoch": 17.363636363636363,
      "grad_norm": 2.796875,
      "learning_rate": 1.1116455134507665e-06,
      "loss": 0.0431,
      "step": 191
    },
    {
      "epoch": 17.454545454545453,
      "grad_norm": 3.078125,
      "learning_rate": 1.0400622570866426e-06,
      "loss": 0.0446,
      "step": 192
    },
    {
      "epoch": 17.545454545454547,
      "grad_norm": 2.234375,
      "learning_rate": 9.707346171337895e-07,
      "loss": 0.0414,
      "step": 193
    },
    {
      "epoch": 17.636363636363637,
      "grad_norm": 2.859375,
      "learning_rate": 9.036800464548157e-07,
      "loss": 0.0551,
      "step": 194
    },
    {
      "epoch": 17.727272727272727,
      "grad_norm": 2.953125,
      "learning_rate": 8.389154256793042e-07,
      "loss": 0.0491,
      "step": 195
    },
    {
      "epoch": 17.818181818181817,
      "grad_norm": 2.625,
      "learning_rate": 7.764570589541876e-07,
      "loss": 0.0555,
      "step": 196
    },
    {
      "epoch": 17.90909090909091,
      "grad_norm": 2.671875,
      "learning_rate": 7.163206698392744e-07,
      "loss": 0.0448,
      "step": 197
    },
    {
      "epoch": 18.0,
      "grad_norm": 3.578125,
      "learning_rate": 6.585213973489335e-07,
      "loss": 0.0519,
      "step": 198
    },
    {
      "epoch": 18.09090909090909,
      "grad_norm": 2.703125,
      "learning_rate": 6.030737921409169e-07,
      "loss": 0.0503,
      "step": 199
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 2.46875,
      "learning_rate": 5.499918128533155e-07,
      "loss": 0.047,
      "step": 200
    },
    {
      "epoch": 18.272727272727273,
      "grad_norm": 2.53125,
      "learning_rate": 4.992888225905467e-07,
      "loss": 0.047,
      "step": 201
    },
    {
      "epoch": 18.363636363636363,
      "grad_norm": 2.578125,
      "learning_rate": 4.509775855592613e-07,
      "loss": 0.0456,
      "step": 202
    },
    {
      "epoch": 18.454545454545453,
      "grad_norm": 2.8125,
      "learning_rate": 4.0507026385502747e-07,
      "loss": 0.0459,
      "step": 203
    },
    {
      "epoch": 18.545454545454547,
      "grad_norm": 2.984375,
      "learning_rate": 3.615784144005796e-07,
      "loss": 0.0517,
      "step": 204
    },
    {
      "epoch": 18.636363636363637,
      "grad_norm": 2.609375,
      "learning_rate": 3.2051298603643754e-07,
      "loss": 0.0497,
      "step": 205
    },
    {
      "epoch": 18.727272727272727,
      "grad_norm": 2.90625,
      "learning_rate": 2.818843167645835e-07,
      "loss": 0.0466,
      "step": 206
    },
    {
      "epoch": 18.818181818181817,
      "grad_norm": 2.3125,
      "learning_rate": 2.4570213114592957e-07,
      "loss": 0.0415,
      "step": 207
    },
    {
      "epoch": 18.90909090909091,
      "grad_norm": 3.21875,
      "learning_rate": 2.119755378522137e-07,
      "loss": 0.0524,
      "step": 208
    },
    {
      "epoch": 19.0,
      "grad_norm": 2.84375,
      "learning_rate": 1.8071302737293294e-07,
      "loss": 0.0507,
      "step": 209
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 3.0,
      "learning_rate": 1.519224698779198e-07,
      "loss": 0.0477,
      "step": 210
    },
    {
      "epoch": 19.181818181818183,
      "grad_norm": 3.125,
      "learning_rate": 1.2561111323605714e-07,
      "loss": 0.0495,
      "step": 211
    },
    {
      "epoch": 19.272727272727273,
      "grad_norm": 2.90625,
      "learning_rate": 1.0178558119067316e-07,
      "loss": 0.0503,
      "step": 212
    },
    {
      "epoch": 19.363636363636363,
      "grad_norm": 2.75,
      "learning_rate": 8.04518716920466e-08,
      "loss": 0.0467,
      "step": 213
    },
    {
      "epoch": 19.454545454545453,
      "grad_norm": 2.375,
      "learning_rate": 6.161535538745877e-08,
      "loss": 0.053,
      "step": 214
    },
    {
      "epoch": 19.545454545454547,
      "grad_norm": 2.546875,
      "learning_rate": 4.528077426915412e-08,
      "loss": 0.0431,
      "step": 215
    },
    {
      "epoch": 19.636363636363637,
      "grad_norm": 3.203125,
      "learning_rate": 3.1452240480577265e-08,
      "loss": 0.0509,
      "step": 216
    },
    {
      "epoch": 19.727272727272727,
      "grad_norm": 2.453125,
      "learning_rate": 2.013323528115674e-08,
      "loss": 0.0471,
      "step": 217
    },
    {
      "epoch": 19.818181818181817,
      "grad_norm": 2.59375,
      "learning_rate": 1.1326608169920373e-08,
      "loss": 0.0481,
      "step": 218
    },
    {
      "epoch": 19.90909090909091,
      "grad_norm": 2.71875,
      "learning_rate": 5.034576168149175e-09,
      "loss": 0.0445,
      "step": 219
    },
    {
      "epoch": 20.0,
      "grad_norm": 2.890625,
      "learning_rate": 1.2587232612493172e-09,
      "loss": 0.0466,
      "step": 220
    }
  ],
  "logging_steps": 1,
  "max_steps": 220,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2077655102595072.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
