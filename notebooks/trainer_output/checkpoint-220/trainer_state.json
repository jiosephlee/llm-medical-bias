{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 220,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 19.25,
      "learning_rate": 0.0,
      "loss": 2.1329,
      "step": 1
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 18.5,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 2.1671,
      "step": 2
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 19.25,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.2441,
      "step": 3
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 17.0,
      "learning_rate": 6e-06,
      "loss": 2.1751,
      "step": 4
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 16.125,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.0858,
      "step": 5
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 17.375,
      "learning_rate": 1e-05,
      "loss": 1.8139,
      "step": 6
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 16.125,
      "learning_rate": 1.2e-05,
      "loss": 1.828,
      "step": 7
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 16.75,
      "learning_rate": 1.4e-05,
      "loss": 1.5706,
      "step": 8
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 18.25,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 1.6196,
      "step": 9
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 13.5,
      "learning_rate": 1.8e-05,
      "loss": 1.4341,
      "step": 10
    },
    {
      "epoch": 1.0,
      "grad_norm": 9.0,
      "learning_rate": 2e-05,
      "loss": 1.2789,
      "step": 11
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 7.75,
      "learning_rate": 1.9998881018102735e-05,
      "loss": 1.2258,
      "step": 12
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 6.375,
      "learning_rate": 1.9995524322835035e-05,
      "loss": 1.124,
      "step": 13
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 5.9375,
      "learning_rate": 1.9989930665413148e-05,
      "loss": 1.2915,
      "step": 14
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 6.1875,
      "learning_rate": 1.998210129767735e-05,
      "loss": 1.151,
      "step": 15
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 5.375,
      "learning_rate": 1.9972037971811802e-05,
      "loss": 0.9883,
      "step": 16
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 6.0625,
      "learning_rate": 1.9959742939952393e-05,
      "loss": 1.162,
      "step": 17
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 7.15625,
      "learning_rate": 1.9945218953682736e-05,
      "loss": 1.093,
      "step": 18
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 5.75,
      "learning_rate": 1.9928469263418376e-05,
      "loss": 1.078,
      "step": 19
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 6.21875,
      "learning_rate": 1.990949761767935e-05,
      "loss": 1.0227,
      "step": 20
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 5.25,
      "learning_rate": 1.9888308262251286e-05,
      "loss": 1.0243,
      "step": 21
    },
    {
      "epoch": 2.0,
      "grad_norm": 5.34375,
      "learning_rate": 1.9864905939235215e-05,
      "loss": 1.1063,
      "step": 22
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 5.25,
      "learning_rate": 1.98392958859863e-05,
      "loss": 1.0551,
      "step": 23
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 5.5,
      "learning_rate": 1.9811483833941726e-05,
      "loss": 0.9277,
      "step": 24
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 5.4375,
      "learning_rate": 1.9781476007338058e-05,
      "loss": 0.8238,
      "step": 25
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 5.15625,
      "learning_rate": 1.9749279121818235e-05,
      "loss": 0.8901,
      "step": 26
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 5.75,
      "learning_rate": 1.9714900382928674e-05,
      "loss": 0.9745,
      "step": 27
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 6.625,
      "learning_rate": 1.9678347484506667e-05,
      "loss": 0.8587,
      "step": 28
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 7.59375,
      "learning_rate": 1.9639628606958535e-05,
      "loss": 0.9193,
      "step": 29
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 6.0625,
      "learning_rate": 1.9598752415428893e-05,
      "loss": 0.8538,
      "step": 30
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 6.0625,
      "learning_rate": 1.955572805786141e-05,
      "loss": 0.9495,
      "step": 31
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 5.0,
      "learning_rate": 1.9510565162951538e-05,
      "loss": 0.8658,
      "step": 32
    },
    {
      "epoch": 3.0,
      "grad_norm": 5.65625,
      "learning_rate": 1.9463273837991643e-05,
      "loss": 0.839,
      "step": 33
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 5.53125,
      "learning_rate": 1.9413864666609036e-05,
      "loss": 0.8463,
      "step": 34
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 5.21875,
      "learning_rate": 1.9362348706397374e-05,
      "loss": 0.812,
      "step": 35
    },
    {
      "epoch": 3.2727272727272725,
      "grad_norm": 5.59375,
      "learning_rate": 1.9308737486442045e-05,
      "loss": 0.7159,
      "step": 36
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 6.0,
      "learning_rate": 1.9253043004739967e-05,
      "loss": 0.7459,
      "step": 37
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 6.46875,
      "learning_rate": 1.919527772551451e-05,
      "loss": 0.7318,
      "step": 38
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 9.0,
      "learning_rate": 1.913545457642601e-05,
      "loss": 0.6585,
      "step": 39
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 9.0,
      "learning_rate": 1.907358694567865e-05,
      "loss": 0.7629,
      "step": 40
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 5.90625,
      "learning_rate": 1.900968867902419e-05,
      "loss": 0.6981,
      "step": 41
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 6.28125,
      "learning_rate": 1.8943774076663372e-05,
      "loss": 0.6838,
      "step": 42
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 6.0625,
      "learning_rate": 1.8875857890045544e-05,
      "loss": 0.6163,
      "step": 43
    },
    {
      "epoch": 4.0,
      "grad_norm": 5.90625,
      "learning_rate": 1.880595531856738e-05,
      "loss": 0.6838,
      "step": 44
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 7.0625,
      "learning_rate": 1.87340820061713e-05,
      "loss": 0.5165,
      "step": 45
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 6.59375,
      "learning_rate": 1.866025403784439e-05,
      "loss": 0.5512,
      "step": 46
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 7.03125,
      "learning_rate": 1.8584487936018663e-05,
      "loss": 0.5783,
      "step": 47
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 8.25,
      "learning_rate": 1.8506800656873397e-05,
      "loss": 0.6111,
      "step": 48
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 11.9375,
      "learning_rate": 1.8427209586540392e-05,
      "loss": 0.5557,
      "step": 49
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 10.1875,
      "learning_rate": 1.834573253721303e-05,
      "loss": 0.558,
      "step": 50
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 6.875,
      "learning_rate": 1.826238774315995e-05,
      "loss": 0.5251,
      "step": 51
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 6.625,
      "learning_rate": 1.8177193856644315e-05,
      "loss": 0.6514,
      "step": 52
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 6.15625,
      "learning_rate": 1.8090169943749477e-05,
      "loss": 0.5235,
      "step": 53
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 5.8125,
      "learning_rate": 1.8001335480112067e-05,
      "loss": 0.4837,
      "step": 54
    },
    {
      "epoch": 5.0,
      "grad_norm": 6.53125,
      "learning_rate": 1.7910710346563417e-05,
      "loss": 0.5519,
      "step": 55
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 8.25,
      "learning_rate": 1.78183148246803e-05,
      "loss": 0.3969,
      "step": 56
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 7.4375,
      "learning_rate": 1.7724169592245996e-05,
      "loss": 0.4206,
      "step": 57
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 6.65625,
      "learning_rate": 1.7628295718622666e-05,
      "loss": 0.3952,
      "step": 58
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 7.375,
      "learning_rate": 1.7530714660036112e-05,
      "loss": 0.403,
      "step": 59
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 9.25,
      "learning_rate": 1.7431448254773943e-05,
      "loss": 0.4797,
      "step": 60
    },
    {
      "epoch": 5.545454545454545,
      "grad_norm": 10.875,
      "learning_rate": 1.7330518718298263e-05,
      "loss": 0.4478,
      "step": 61
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 11.3125,
      "learning_rate": 1.7227948638273918e-05,
      "loss": 0.4152,
      "step": 62
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 9.1875,
      "learning_rate": 1.712376096951345e-05,
      "loss": 0.3186,
      "step": 63
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 7.90625,
      "learning_rate": 1.7017979028839918e-05,
      "loss": 0.3516,
      "step": 64
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 6.375,
      "learning_rate": 1.691062648986865e-05,
      "loss": 0.3792,
      "step": 65
    },
    {
      "epoch": 6.0,
      "grad_norm": 6.5,
      "learning_rate": 1.6801727377709195e-05,
      "loss": 0.4102,
      "step": 66
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 7.40625,
      "learning_rate": 1.6691306063588583e-05,
      "loss": 0.2805,
      "step": 67
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 7.40625,
      "learning_rate": 1.657938725939713e-05,
      "loss": 0.2979,
      "step": 68
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 7.40625,
      "learning_rate": 1.6465996012157996e-05,
      "loss": 0.2842,
      "step": 69
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 6.5,
      "learning_rate": 1.635115769842179e-05,
      "loss": 0.2295,
      "step": 70
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 6.90625,
      "learning_rate": 1.6234898018587336e-05,
      "loss": 0.2618,
      "step": 71
    },
    {
      "epoch": 6.545454545454545,
      "grad_norm": 10.5625,
      "learning_rate": 1.6117242991150064e-05,
      "loss": 0.3091,
      "step": 72
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 11.6875,
      "learning_rate": 1.599821894687914e-05,
      "loss": 0.3298,
      "step": 73
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 10.5,
      "learning_rate": 1.5877852522924733e-05,
      "loss": 0.2872,
      "step": 74
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 11.9375,
      "learning_rate": 1.575617065685674e-05,
      "loss": 0.2869,
      "step": 75
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 11.4375,
      "learning_rate": 1.563320058063622e-05,
      "loss": 0.2776,
      "step": 76
    },
    {
      "epoch": 7.0,
      "grad_norm": 11.5,
      "learning_rate": 1.5508969814521026e-05,
      "loss": 0.244,
      "step": 77
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 7.1875,
      "learning_rate": 1.5383506160906826e-05,
      "loss": 0.2302,
      "step": 78
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 5.6875,
      "learning_rate": 1.5256837698105047e-05,
      "loss": 0.1891,
      "step": 79
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 6.8125,
      "learning_rate": 1.5128992774059063e-05,
      "loss": 0.2223,
      "step": 80
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 5.90625,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.1522,
      "step": 81
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 5.875,
      "learning_rate": 1.4869888244043674e-05,
      "loss": 0.1958,
      "step": 82
    },
    {
      "epoch": 7.545454545454545,
      "grad_norm": 5.40625,
      "learning_rate": 1.4738686624729987e-05,
      "loss": 0.1685,
      "step": 83
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 7.375,
      "learning_rate": 1.4606424504506325e-05,
      "loss": 0.1836,
      "step": 84
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 7.4375,
      "learning_rate": 1.4473131483156326e-05,
      "loss": 0.1532,
      "step": 85
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 10.3125,
      "learning_rate": 1.4338837391175582e-05,
      "loss": 0.1884,
      "step": 86
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 7.96875,
      "learning_rate": 1.4203572283095657e-05,
      "loss": 0.1509,
      "step": 87
    },
    {
      "epoch": 8.0,
      "grad_norm": 12.375,
      "learning_rate": 1.4067366430758004e-05,
      "loss": 0.2228,
      "step": 88
    },
    {
      "epoch": 8.090909090909092,
      "grad_norm": 6.0625,
      "learning_rate": 1.3930250316539237e-05,
      "loss": 0.1401,
      "step": 89
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 5.71875,
      "learning_rate": 1.3792254626529286e-05,
      "loss": 0.1146,
      "step": 90
    },
    {
      "epoch": 8.272727272727273,
      "grad_norm": 4.75,
      "learning_rate": 1.3653410243663953e-05,
      "loss": 0.1184,
      "step": 91
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 4.8125,
      "learning_rate": 1.3513748240813429e-05,
      "loss": 0.1266,
      "step": 92
    },
    {
      "epoch": 8.454545454545455,
      "grad_norm": 6.5625,
      "learning_rate": 1.3373299873828303e-05,
      "loss": 0.1737,
      "step": 93
    },
    {
      "epoch": 8.545454545454545,
      "grad_norm": 6.21875,
      "learning_rate": 1.3232096574544602e-05,
      "loss": 0.1291,
      "step": 94
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 5.5,
      "learning_rate": 1.3090169943749475e-05,
      "loss": 0.1283,
      "step": 95
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 6.15625,
      "learning_rate": 1.2947551744109044e-05,
      "loss": 0.1174,
      "step": 96
    },
    {
      "epoch": 8.818181818181818,
      "grad_norm": 4.3125,
      "learning_rate": 1.2804273893060028e-05,
      "loss": 0.1041,
      "step": 97
    },
    {
      "epoch": 8.909090909090908,
      "grad_norm": 5.1875,
      "learning_rate": 1.2660368455666752e-05,
      "loss": 0.1209,
      "step": 98
    },
    {
      "epoch": 9.0,
      "grad_norm": 4.875,
      "learning_rate": 1.2515867637445088e-05,
      "loss": 0.1145,
      "step": 99
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 5.96875,
      "learning_rate": 1.2370803777154976e-05,
      "loss": 0.1216,
      "step": 100
    },
    {
      "epoch": 9.181818181818182,
      "grad_norm": 3.890625,
      "learning_rate": 1.2225209339563144e-05,
      "loss": 0.086,
      "step": 101
    },
    {
      "epoch": 9.272727272727273,
      "grad_norm": 5.03125,
      "learning_rate": 1.2079116908177592e-05,
      "loss": 0.1034,
      "step": 102
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 4.75,
      "learning_rate": 1.1932559177955533e-05,
      "loss": 0.0922,
      "step": 103
    },
    {
      "epoch": 9.454545454545455,
      "grad_norm": 4.6875,
      "learning_rate": 1.1785568947986368e-05,
      "loss": 0.088,
      "step": 104
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 3.75,
      "learning_rate": 1.1638179114151378e-05,
      "loss": 0.0804,
      "step": 105
    },
    {
      "epoch": 9.636363636363637,
      "grad_norm": 4.09375,
      "learning_rate": 1.1490422661761744e-05,
      "loss": 0.0912,
      "step": 106
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 4.59375,
      "learning_rate": 1.1342332658176556e-05,
      "loss": 0.0834,
      "step": 107
    },
    {
      "epoch": 9.818181818181818,
      "grad_norm": 8.5625,
      "learning_rate": 1.1193942245402443e-05,
      "loss": 0.1002,
      "step": 108
    },
    {
      "epoch": 9.909090909090908,
      "grad_norm": 7.34375,
      "learning_rate": 1.1045284632676535e-05,
      "loss": 0.0878,
      "step": 109
    },
    {
      "epoch": 10.0,
      "grad_norm": 6.40625,
      "learning_rate": 1.0896393089034336e-05,
      "loss": 0.0947,
      "step": 110
    },
    {
      "epoch": 10.090909090909092,
      "grad_norm": 8.6875,
      "learning_rate": 1.0747300935864245e-05,
      "loss": 0.0789,
      "step": 111
    },
    {
      "epoch": 10.181818181818182,
      "grad_norm": 3.890625,
      "learning_rate": 1.0598041539450344e-05,
      "loss": 0.0739,
      "step": 112
    },
    {
      "epoch": 10.272727272727273,
      "grad_norm": 3.390625,
      "learning_rate": 1.044864830350515e-05,
      "loss": 0.0716,
      "step": 113
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 3.234375,
      "learning_rate": 1.0299154661693987e-05,
      "loss": 0.0649,
      "step": 114
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 5.125,
      "learning_rate": 1.0149594070152638e-05,
      "loss": 0.0828,
      "step": 115
    },
    {
      "epoch": 10.545454545454545,
      "grad_norm": 3.453125,
      "learning_rate": 1e-05,
      "loss": 0.0678,
      "step": 116
    },
    {
      "epoch": 10.636363636363637,
      "grad_norm": 6.0625,
      "learning_rate": 9.850405929847367e-06,
      "loss": 0.0778,
      "step": 117
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 3.828125,
      "learning_rate": 9.700845338306018e-06,
      "loss": 0.0796,
      "step": 118
    },
    {
      "epoch": 10.818181818181818,
      "grad_norm": 4.3125,
      "learning_rate": 9.551351696494854e-06,
      "loss": 0.0685,
      "step": 119
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 5.53125,
      "learning_rate": 9.401958460549658e-06,
      "loss": 0.0746,
      "step": 120
    },
    {
      "epoch": 11.0,
      "grad_norm": 4.53125,
      "learning_rate": 9.252699064135759e-06,
      "loss": 0.0744,
      "step": 121
    },
    {
      "epoch": 11.090909090909092,
      "grad_norm": 3.21875,
      "learning_rate": 9.103606910965666e-06,
      "loss": 0.0589,
      "step": 122
    },
    {
      "epoch": 11.181818181818182,
      "grad_norm": 2.671875,
      "learning_rate": 8.954715367323468e-06,
      "loss": 0.0529,
      "step": 123
    },
    {
      "epoch": 11.272727272727273,
      "grad_norm": 3.546875,
      "learning_rate": 8.806057754597559e-06,
      "loss": 0.0649,
      "step": 124
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 2.828125,
      "learning_rate": 8.657667341823449e-06,
      "loss": 0.0618,
      "step": 125
    },
    {
      "epoch": 11.454545454545455,
      "grad_norm": 4.71875,
      "learning_rate": 8.509577338238255e-06,
      "loss": 0.0775,
      "step": 126
    },
    {
      "epoch": 11.545454545454545,
      "grad_norm": 3.984375,
      "learning_rate": 8.361820885848623e-06,
      "loss": 0.0665,
      "step": 127
    },
    {
      "epoch": 11.636363636363637,
      "grad_norm": 3.21875,
      "learning_rate": 8.214431052013636e-06,
      "loss": 0.0675,
      "step": 128
    },
    {
      "epoch": 11.727272727272727,
      "grad_norm": 2.921875,
      "learning_rate": 8.06744082204447e-06,
      "loss": 0.0562,
      "step": 129
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 3.125,
      "learning_rate": 7.92088309182241e-06,
      "loss": 0.0648,
      "step": 130
    },
    {
      "epoch": 11.909090909090908,
      "grad_norm": 3.5,
      "learning_rate": 7.774790660436857e-06,
      "loss": 0.0577,
      "step": 131
    },
    {
      "epoch": 12.0,
      "grad_norm": 3.6875,
      "learning_rate": 7.629196222845027e-06,
      "loss": 0.0689,
      "step": 132
    },
    {
      "epoch": 12.090909090909092,
      "grad_norm": 2.84375,
      "learning_rate": 7.484132362554915e-06,
      "loss": 0.0538,
      "step": 133
    },
    {
      "epoch": 12.181818181818182,
      "grad_norm": 3.0625,
      "learning_rate": 7.33963154433325e-06,
      "loss": 0.0503,
      "step": 134
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 3.671875,
      "learning_rate": 7.1957261069399745e-06,
      "loss": 0.0715,
      "step": 135
    },
    {
      "epoch": 12.363636363636363,
      "grad_norm": 2.65625,
      "learning_rate": 7.052448255890958e-06,
      "loss": 0.0471,
      "step": 136
    },
    {
      "epoch": 12.454545454545455,
      "grad_norm": 3.203125,
      "learning_rate": 6.909830056250527e-06,
      "loss": 0.0635,
      "step": 137
    },
    {
      "epoch": 12.545454545454545,
      "grad_norm": 2.875,
      "learning_rate": 6.767903425455402e-06,
      "loss": 0.0583,
      "step": 138
    },
    {
      "epoch": 12.636363636363637,
      "grad_norm": 3.390625,
      "learning_rate": 6.6267001261717015e-06,
      "loss": 0.0642,
      "step": 139
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 3.5625,
      "learning_rate": 6.486251759186573e-06,
      "loss": 0.0616,
      "step": 140
    },
    {
      "epoch": 12.818181818181818,
      "grad_norm": 2.9375,
      "learning_rate": 6.34658975633605e-06,
      "loss": 0.0531,
      "step": 141
    },
    {
      "epoch": 12.909090909090908,
      "grad_norm": 3.3125,
      "learning_rate": 6.207745373470717e-06,
      "loss": 0.0556,
      "step": 142
    },
    {
      "epoch": 13.0,
      "grad_norm": 3.75,
      "learning_rate": 6.069749683460765e-06,
      "loss": 0.0547,
      "step": 143
    },
    {
      "epoch": 13.090909090909092,
      "grad_norm": 3.5,
      "learning_rate": 5.932633569242e-06,
      "loss": 0.0599,
      "step": 144
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 2.75,
      "learning_rate": 5.796427716904347e-06,
      "loss": 0.0578,
      "step": 145
    },
    {
      "epoch": 13.272727272727273,
      "grad_norm": 2.90625,
      "learning_rate": 5.66116260882442e-06,
      "loss": 0.0524,
      "step": 146
    },
    {
      "epoch": 13.363636363636363,
      "grad_norm": 2.984375,
      "learning_rate": 5.526868516843673e-06,
      "loss": 0.0523,
      "step": 147
    },
    {
      "epoch": 13.454545454545455,
      "grad_norm": 3.4375,
      "learning_rate": 5.393575495493679e-06,
      "loss": 0.0514,
      "step": 148
    },
    {
      "epoch": 13.545454545454545,
      "grad_norm": 3.640625,
      "learning_rate": 5.2613133752700145e-06,
      "loss": 0.0547,
      "step": 149
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 2.921875,
      "learning_rate": 5.130111755956327e-06,
      "loss": 0.0543,
      "step": 150
    },
    {
      "epoch": 13.727272727272727,
      "grad_norm": 3.15625,
      "learning_rate": 5.000000000000003e-06,
      "loss": 0.05,
      "step": 151
    },
    {
      "epoch": 13.818181818181818,
      "grad_norm": 3.1875,
      "learning_rate": 4.87100722594094e-06,
      "loss": 0.0486,
      "step": 152
    },
    {
      "epoch": 13.909090909090908,
      "grad_norm": 3.640625,
      "learning_rate": 4.743162301894952e-06,
      "loss": 0.0555,
      "step": 153
    },
    {
      "epoch": 14.0,
      "grad_norm": 2.984375,
      "learning_rate": 4.616493839093179e-06,
      "loss": 0.0515,
      "step": 154
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 3.25,
      "learning_rate": 4.491030185478976e-06,
      "loss": 0.0517,
      "step": 155
    },
    {
      "epoch": 14.181818181818182,
      "grad_norm": 3.15625,
      "learning_rate": 4.3667994193637794e-06,
      "loss": 0.052,
      "step": 156
    },
    {
      "epoch": 14.272727272727273,
      "grad_norm": 3.84375,
      "learning_rate": 4.2438293431432665e-06,
      "loss": 0.0544,
      "step": 157
    },
    {
      "epoch": 14.363636363636363,
      "grad_norm": 2.8125,
      "learning_rate": 4.12214747707527e-06,
      "loss": 0.0485,
      "step": 158
    },
    {
      "epoch": 14.454545454545455,
      "grad_norm": 2.8125,
      "learning_rate": 4.001781053120863e-06,
      "loss": 0.0479,
      "step": 159
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 2.90625,
      "learning_rate": 3.882757008849936e-06,
      "loss": 0.0581,
      "step": 160
    },
    {
      "epoch": 14.636363636363637,
      "grad_norm": 2.984375,
      "learning_rate": 3.7651019814126656e-06,
      "loss": 0.0536,
      "step": 161
    },
    {
      "epoch": 14.727272727272727,
      "grad_norm": 3.453125,
      "learning_rate": 3.6488423015782128e-06,
      "loss": 0.055,
      "step": 162
    },
    {
      "epoch": 14.818181818181818,
      "grad_norm": 2.6875,
      "learning_rate": 3.534003987842005e-06,
      "loss": 0.046,
      "step": 163
    },
    {
      "epoch": 14.909090909090908,
      "grad_norm": 3.796875,
      "learning_rate": 3.4206127406028744e-06,
      "loss": 0.0506,
      "step": 164
    },
    {
      "epoch": 15.0,
      "grad_norm": 3.484375,
      "learning_rate": 3.308693936411421e-06,
      "loss": 0.0509,
      "step": 165
    },
    {
      "epoch": 15.090909090909092,
      "grad_norm": 2.9375,
      "learning_rate": 3.1982726222908046e-06,
      "loss": 0.0514,
      "step": 166
    },
    {
      "epoch": 15.181818181818182,
      "grad_norm": 2.921875,
      "learning_rate": 3.089373510131354e-06,
      "loss": 0.0478,
      "step": 167
    },
    {
      "epoch": 15.272727272727273,
      "grad_norm": 3.28125,
      "learning_rate": 2.9820209711600858e-06,
      "loss": 0.0488,
      "step": 168
    },
    {
      "epoch": 15.363636363636363,
      "grad_norm": 2.609375,
      "learning_rate": 2.876239030486554e-06,
      "loss": 0.0513,
      "step": 169
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 3.515625,
      "learning_rate": 2.7720513617260857e-06,
      "loss": 0.0538,
      "step": 170
    },
    {
      "epoch": 15.545454545454545,
      "grad_norm": 3.421875,
      "learning_rate": 2.669481281701739e-06,
      "loss": 0.0522,
      "step": 171
    },
    {
      "epoch": 15.636363636363637,
      "grad_norm": 2.765625,
      "learning_rate": 2.5685517452260566e-06,
      "loss": 0.0442,
      "step": 172
    },
    {
      "epoch": 15.727272727272727,
      "grad_norm": 3.15625,
      "learning_rate": 2.469285339963892e-06,
      "loss": 0.0532,
      "step": 173
    },
    {
      "epoch": 15.818181818181818,
      "grad_norm": 2.640625,
      "learning_rate": 2.371704281377335e-06,
      "loss": 0.0461,
      "step": 174
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 3.265625,
      "learning_rate": 2.275830407754006e-06,
      "loss": 0.0522,
      "step": 175
    },
    {
      "epoch": 16.0,
      "grad_norm": 2.90625,
      "learning_rate": 2.1816851753197023e-06,
      "loss": 0.0487,
      "step": 176
    },
    {
      "epoch": 16.09090909090909,
      "grad_norm": 2.953125,
      "learning_rate": 2.08928965343659e-06,
      "loss": 0.0514,
      "step": 177
    },
    {
      "epoch": 16.181818181818183,
      "grad_norm": 2.546875,
      "learning_rate": 1.9986645198879385e-06,
      "loss": 0.0438,
      "step": 178
    },
    {
      "epoch": 16.272727272727273,
      "grad_norm": 3.5,
      "learning_rate": 1.9098300562505266e-06,
      "loss": 0.0524,
      "step": 179
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 3.046875,
      "learning_rate": 1.8228061433556866e-06,
      "loss": 0.0484,
      "step": 180
    },
    {
      "epoch": 16.454545454545453,
      "grad_norm": 3.171875,
      "learning_rate": 1.7376122568400533e-06,
      "loss": 0.052,
      "step": 181
    },
    {
      "epoch": 16.545454545454547,
      "grad_norm": 3.265625,
      "learning_rate": 1.6542674627869738e-06,
      "loss": 0.0522,
      "step": 182
    },
    {
      "epoch": 16.636363636363637,
      "grad_norm": 3.671875,
      "learning_rate": 1.5727904134596084e-06,
      "loss": 0.0502,
      "step": 183
    },
    {
      "epoch": 16.727272727272727,
      "grad_norm": 2.953125,
      "learning_rate": 1.4931993431266056e-06,
      "loss": 0.0483,
      "step": 184
    },
    {
      "epoch": 16.818181818181817,
      "grad_norm": 3.0625,
      "learning_rate": 1.4155120639813392e-06,
      "loss": 0.0496,
      "step": 185
    },
    {
      "epoch": 16.90909090909091,
      "grad_norm": 2.921875,
      "learning_rate": 1.339745962155613e-06,
      "loss": 0.0537,
      "step": 186
    },
    {
      "epoch": 17.0,
      "grad_norm": 2.84375,
      "learning_rate": 1.2659179938287035e-06,
      "loss": 0.0447,
      "step": 187
    },
    {
      "epoch": 17.09090909090909,
      "grad_norm": 3.1875,
      "learning_rate": 1.19404468143262e-06,
      "loss": 0.0491,
      "step": 188
    },
    {
      "epoch": 17.181818181818183,
      "grad_norm": 3.3125,
      "learning_rate": 1.124142109954459e-06,
      "loss": 0.0586,
      "step": 189
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 2.96875,
      "learning_rate": 1.0562259233366334e-06,
      "loss": 0.0464,
      "step": 190
    },
    {
      "epoch": 17.363636363636363,
      "grad_norm": 3.390625,
      "learning_rate": 9.903113209758098e-07,
      "loss": 0.0453,
      "step": 191
    },
    {
      "epoch": 17.454545454545453,
      "grad_norm": 3.125,
      "learning_rate": 9.264130543213512e-07,
      "loss": 0.0456,
      "step": 192
    },
    {
      "epoch": 17.545454545454547,
      "grad_norm": 2.875,
      "learning_rate": 8.645454235739903e-07,
      "loss": 0.0425,
      "step": 193
    },
    {
      "epoch": 17.636363636363637,
      "grad_norm": 3.046875,
      "learning_rate": 8.047222744854943e-07,
      "loss": 0.0496,
      "step": 194
    },
    {
      "epoch": 17.727272727272727,
      "grad_norm": 3.15625,
      "learning_rate": 7.46956995260033e-07,
      "loss": 0.0514,
      "step": 195
    },
    {
      "epoch": 17.818181818181817,
      "grad_norm": 2.828125,
      "learning_rate": 6.912625135579587e-07,
      "loss": 0.0598,
      "step": 196
    },
    {
      "epoch": 17.90909090909091,
      "grad_norm": 3.140625,
      "learning_rate": 6.37651293602628e-07,
      "loss": 0.0459,
      "step": 197
    },
    {
      "epoch": 18.0,
      "grad_norm": 3.703125,
      "learning_rate": 5.861353333909692e-07,
      "loss": 0.0525,
      "step": 198
    },
    {
      "epoch": 18.09090909090909,
      "grad_norm": 2.953125,
      "learning_rate": 5.367261620083575e-07,
      "loss": 0.0538,
      "step": 199
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 2.953125,
      "learning_rate": 4.894348370484648e-07,
      "loss": 0.0493,
      "step": 200
    },
    {
      "epoch": 18.272727272727273,
      "grad_norm": 2.734375,
      "learning_rate": 4.4427194213859216e-07,
      "loss": 0.0488,
      "step": 201
    },
    {
      "epoch": 18.363636363636363,
      "grad_norm": 2.828125,
      "learning_rate": 4.012475845711106e-07,
      "loss": 0.0463,
      "step": 202
    },
    {
      "epoch": 18.454545454545453,
      "grad_norm": 3.046875,
      "learning_rate": 3.603713930414676e-07,
      "loss": 0.0473,
      "step": 203
    },
    {
      "epoch": 18.545454545454547,
      "grad_norm": 3.46875,
      "learning_rate": 3.2165251549333585e-07,
      "loss": 0.0517,
      "step": 204
    },
    {
      "epoch": 18.636363636363637,
      "grad_norm": 2.90625,
      "learning_rate": 2.8509961707132496e-07,
      "loss": 0.0524,
      "step": 205
    },
    {
      "epoch": 18.727272727272727,
      "grad_norm": 3.3125,
      "learning_rate": 2.507208781817638e-07,
      "loss": 0.046,
      "step": 206
    },
    {
      "epoch": 18.818181818181817,
      "grad_norm": 2.953125,
      "learning_rate": 2.1852399266194312e-07,
      "loss": 0.0442,
      "step": 207
    },
    {
      "epoch": 18.90909090909091,
      "grad_norm": 3.6875,
      "learning_rate": 1.885161660582746e-07,
      "loss": 0.0517,
      "step": 208
    },
    {
      "epoch": 19.0,
      "grad_norm": 3.03125,
      "learning_rate": 1.6070411401370335e-07,
      "loss": 0.0512,
      "step": 209
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 3.390625,
      "learning_rate": 1.350940607647866e-07,
      "loss": 0.0522,
      "step": 210
    },
    {
      "epoch": 19.181818181818183,
      "grad_norm": 3.515625,
      "learning_rate": 1.1169173774871478e-07,
      "loss": 0.0529,
      "step": 211
    },
    {
      "epoch": 19.272727272727273,
      "grad_norm": 2.9375,
      "learning_rate": 9.0502382320653e-08,
      "loss": 0.0514,
      "step": 212
    },
    {
      "epoch": 19.363636363636363,
      "grad_norm": 2.921875,
      "learning_rate": 7.153073658162646e-08,
      "loss": 0.0474,
      "step": 213
    },
    {
      "epoch": 19.454545454545453,
      "grad_norm": 2.703125,
      "learning_rate": 5.4781046317267103e-08,
      "loss": 0.0539,
      "step": 214
    },
    {
      "epoch": 19.545454545454547,
      "grad_norm": 2.90625,
      "learning_rate": 4.025706004760932e-08,
      "loss": 0.0439,
      "step": 215
    },
    {
      "epoch": 19.636363636363637,
      "grad_norm": 3.546875,
      "learning_rate": 2.796202818819871e-08,
      "loss": 0.0532,
      "step": 216
    },
    {
      "epoch": 19.727272727272727,
      "grad_norm": 3.0625,
      "learning_rate": 1.7898702322648453e-08,
      "loss": 0.044,
      "step": 217
    },
    {
      "epoch": 19.818181818181817,
      "grad_norm": 2.875,
      "learning_rate": 1.0069334586854106e-08,
      "loss": 0.0499,
      "step": 218
    },
    {
      "epoch": 19.90909090909091,
      "grad_norm": 2.65625,
      "learning_rate": 4.475677164966774e-09,
      "loss": 0.0461,
      "step": 219
    },
    {
      "epoch": 20.0,
      "grad_norm": 3.421875,
      "learning_rate": 1.1189818972656697e-09,
      "loss": 0.0472,
      "step": 220
    }
  ],
  "logging_steps": 1,
  "max_steps": 220,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2077655102595072.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
