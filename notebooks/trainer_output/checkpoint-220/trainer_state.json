{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 220,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 19.125,
      "learning_rate": 0.0,
      "loss": 2.2107,
      "step": 1
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 18.375,
      "learning_rate": 9.090909090909091e-07,
      "loss": 2.2374,
      "step": 2
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 19.5,
      "learning_rate": 1.8181818181818183e-06,
      "loss": 2.3422,
      "step": 3
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 17.625,
      "learning_rate": 2.7272727272727272e-06,
      "loss": 2.3125,
      "step": 4
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 16.625,
      "learning_rate": 3.6363636363636366e-06,
      "loss": 2.2427,
      "step": 5
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 18.5,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 2.0761,
      "step": 6
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 18.25,
      "learning_rate": 5.4545454545454545e-06,
      "loss": 2.1736,
      "step": 7
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 16.875,
      "learning_rate": 6.363636363636364e-06,
      "loss": 1.9784,
      "step": 8
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 18.5,
      "learning_rate": 7.272727272727273e-06,
      "loss": 2.1472,
      "step": 9
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 16.0,
      "learning_rate": 8.181818181818183e-06,
      "loss": 1.994,
      "step": 10
    },
    {
      "epoch": 1.0,
      "grad_norm": 15.625,
      "learning_rate": 9.090909090909091e-06,
      "loss": 1.8522,
      "step": 11
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 16.25,
      "learning_rate": 1e-05,
      "loss": 1.8286,
      "step": 12
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 15.5625,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 1.5917,
      "step": 13
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 16.625,
      "learning_rate": 1.181818181818182e-05,
      "loss": 1.6795,
      "step": 14
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 15.3125,
      "learning_rate": 1.2727272727272728e-05,
      "loss": 1.5314,
      "step": 15
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 11.4375,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 1.2747,
      "step": 16
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 9.5,
      "learning_rate": 1.4545454545454546e-05,
      "loss": 1.4128,
      "step": 17
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 8.75,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 1.2719,
      "step": 18
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 6.75,
      "learning_rate": 1.6363636363636366e-05,
      "loss": 1.2118,
      "step": 19
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 6.90625,
      "learning_rate": 1.7272727272727274e-05,
      "loss": 1.1622,
      "step": 20
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 6.21875,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.1599,
      "step": 21
    },
    {
      "epoch": 2.0,
      "grad_norm": 9.125,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 1.2505,
      "step": 22
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 5.9375,
      "learning_rate": 2e-05,
      "loss": 1.2222,
      "step": 23
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 6.0,
      "learning_rate": 1.9998741276738753e-05,
      "loss": 1.0419,
      "step": 24
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 6.5,
      "learning_rate": 1.9994965423831853e-05,
      "loss": 0.9544,
      "step": 25
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 4.78125,
      "learning_rate": 1.9988673391830082e-05,
      "loss": 0.9815,
      "step": 26
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 6.6875,
      "learning_rate": 1.9979866764718846e-05,
      "loss": 1.1323,
      "step": 27
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 6.5,
      "learning_rate": 1.9968547759519426e-05,
      "loss": 1.0113,
      "step": 28
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 6.40625,
      "learning_rate": 1.9954719225730847e-05,
      "loss": 1.0536,
      "step": 29
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 5.5625,
      "learning_rate": 1.9938384644612542e-05,
      "loss": 1.0177,
      "step": 30
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 6.03125,
      "learning_rate": 1.9919548128307954e-05,
      "loss": 1.0679,
      "step": 31
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 5.3125,
      "learning_rate": 1.989821441880933e-05,
      "loss": 1.0148,
      "step": 32
    },
    {
      "epoch": 3.0,
      "grad_norm": 5.59375,
      "learning_rate": 1.9874388886763944e-05,
      "loss": 1.0084,
      "step": 33
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 5.46875,
      "learning_rate": 1.9848077530122083e-05,
      "loss": 0.9721,
      "step": 34
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 5.1875,
      "learning_rate": 1.9819286972627066e-05,
      "loss": 0.8997,
      "step": 35
    },
    {
      "epoch": 3.2727272727272725,
      "grad_norm": 5.5,
      "learning_rate": 1.978802446214779e-05,
      "loss": 0.8212,
      "step": 36
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 6.0625,
      "learning_rate": 1.9754297868854075e-05,
      "loss": 0.8162,
      "step": 37
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 14.25,
      "learning_rate": 1.9718115683235418e-05,
      "loss": 0.9117,
      "step": 38
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 8.25,
      "learning_rate": 1.9679487013963566e-05,
      "loss": 0.7406,
      "step": 39
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 5.96875,
      "learning_rate": 1.9638421585599422e-05,
      "loss": 0.8828,
      "step": 40
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 5.40625,
      "learning_rate": 1.9594929736144978e-05,
      "loss": 0.8168,
      "step": 41
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 5.84375,
      "learning_rate": 1.9549022414440738e-05,
      "loss": 0.8294,
      "step": 42
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 5.25,
      "learning_rate": 1.9500711177409456e-05,
      "loss": 0.716,
      "step": 43
    },
    {
      "epoch": 4.0,
      "grad_norm": 5.15625,
      "learning_rate": 1.9450008187146685e-05,
      "loss": 0.8002,
      "step": 44
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 5.8125,
      "learning_rate": 1.9396926207859085e-05,
      "loss": 0.6388,
      "step": 45
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 5.5625,
      "learning_rate": 1.9341478602651068e-05,
      "loss": 0.6914,
      "step": 46
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 6.21875,
      "learning_rate": 1.9283679330160726e-05,
      "loss": 0.6777,
      "step": 47
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 9.625,
      "learning_rate": 1.9223542941045817e-05,
      "loss": 0.7049,
      "step": 48
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 11.4375,
      "learning_rate": 1.9161084574320696e-05,
      "loss": 0.6298,
      "step": 49
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 10.3125,
      "learning_rate": 1.9096319953545186e-05,
      "loss": 0.6502,
      "step": 50
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 6.625,
      "learning_rate": 1.9029265382866216e-05,
      "loss": 0.6287,
      "step": 51
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 6.25,
      "learning_rate": 1.895993774291336e-05,
      "loss": 0.7417,
      "step": 52
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 5.84375,
      "learning_rate": 1.8888354486549238e-05,
      "loss": 0.637,
      "step": 53
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 5.46875,
      "learning_rate": 1.881453363447582e-05,
      "loss": 0.5423,
      "step": 54
    },
    {
      "epoch": 5.0,
      "grad_norm": 6.21875,
      "learning_rate": 1.873849377069785e-05,
      "loss": 0.6419,
      "step": 55
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 7.125,
      "learning_rate": 1.866025403784439e-05,
      "loss": 0.4887,
      "step": 56
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 6.0625,
      "learning_rate": 1.8579834132349773e-05,
      "loss": 0.5021,
      "step": 57
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 6.53125,
      "learning_rate": 1.8497254299495147e-05,
      "loss": 0.4728,
      "step": 58
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 7.21875,
      "learning_rate": 1.8412535328311813e-05,
      "loss": 0.4931,
      "step": 59
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 7.0,
      "learning_rate": 1.8325698546347714e-05,
      "loss": 0.5442,
      "step": 60
    },
    {
      "epoch": 5.545454545454545,
      "grad_norm": 14.0625,
      "learning_rate": 1.8236765814298328e-05,
      "loss": 0.5476,
      "step": 61
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 11.6875,
      "learning_rate": 1.814575952050336e-05,
      "loss": 0.4956,
      "step": 62
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 7.53125,
      "learning_rate": 1.8052702575310588e-05,
      "loss": 0.3884,
      "step": 63
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 6.40625,
      "learning_rate": 1.7957618405308323e-05,
      "loss": 0.4068,
      "step": 64
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 6.25,
      "learning_rate": 1.7860530947427878e-05,
      "loss": 0.4772,
      "step": 65
    },
    {
      "epoch": 6.0,
      "grad_norm": 6.375,
      "learning_rate": 1.776146464291757e-05,
      "loss": 0.4745,
      "step": 66
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 6.5625,
      "learning_rate": 1.766044443118978e-05,
      "loss": 0.321,
      "step": 67
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 7.21875,
      "learning_rate": 1.7557495743542586e-05,
      "loss": 0.3272,
      "step": 68
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 6.625,
      "learning_rate": 1.745264449675755e-05,
      "loss": 0.3212,
      "step": 69
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 6.4375,
      "learning_rate": 1.734591708657533e-05,
      "loss": 0.2682,
      "step": 70
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 10.375,
      "learning_rate": 1.72373403810507e-05,
      "loss": 0.3428,
      "step": 71
    },
    {
      "epoch": 6.545454545454545,
      "grad_norm": 10.0,
      "learning_rate": 1.7126941713788633e-05,
      "loss": 0.3384,
      "step": 72
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 10.75,
      "learning_rate": 1.7014748877063212e-05,
      "loss": 0.3632,
      "step": 73
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 11.6875,
      "learning_rate": 1.6900790114821122e-05,
      "loss": 0.3576,
      "step": 74
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 7.8125,
      "learning_rate": 1.6785094115571323e-05,
      "loss": 0.3157,
      "step": 75
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 9.0625,
      "learning_rate": 1.666769000516292e-05,
      "loss": 0.3259,
      "step": 76
    },
    {
      "epoch": 7.0,
      "grad_norm": 6.375,
      "learning_rate": 1.6548607339452853e-05,
      "loss": 0.2695,
      "step": 77
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 8.25,
      "learning_rate": 1.6427876096865394e-05,
      "loss": 0.2621,
      "step": 78
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 6.3125,
      "learning_rate": 1.6305526670845225e-05,
      "loss": 0.2213,
      "step": 79
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 6.96875,
      "learning_rate": 1.6181589862206053e-05,
      "loss": 0.2636,
      "step": 80
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 5.8125,
      "learning_rate": 1.6056096871376667e-05,
      "loss": 0.1793,
      "step": 81
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 6.28125,
      "learning_rate": 1.5929079290546408e-05,
      "loss": 0.2015,
      "step": 82
    },
    {
      "epoch": 7.545454545454545,
      "grad_norm": 7.75,
      "learning_rate": 1.5800569095711983e-05,
      "loss": 0.1857,
      "step": 83
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 8.3125,
      "learning_rate": 1.5670598638627707e-05,
      "loss": 0.2135,
      "step": 84
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 12.25,
      "learning_rate": 1.5539200638661106e-05,
      "loss": 0.1883,
      "step": 85
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 13.375,
      "learning_rate": 1.5406408174555978e-05,
      "loss": 0.22,
      "step": 86
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 11.6875,
      "learning_rate": 1.5272254676105026e-05,
      "loss": 0.2162,
      "step": 87
    },
    {
      "epoch": 8.0,
      "grad_norm": 13.875,
      "learning_rate": 1.5136773915734067e-05,
      "loss": 0.237,
      "step": 88
    },
    {
      "epoch": 8.090909090909092,
      "grad_norm": 6.53125,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.169,
      "step": 89
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 5.75,
      "learning_rate": 1.4861967361004687e-05,
      "loss": 0.1247,
      "step": 90
    },
    {
      "epoch": 8.272727272727273,
      "grad_norm": 6.03125,
      "learning_rate": 1.472271074772683e-05,
      "loss": 0.1497,
      "step": 91
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 5.40625,
      "learning_rate": 1.4582265217274105e-05,
      "loss": 0.1435,
      "step": 92
    },
    {
      "epoch": 8.454545454545455,
      "grad_norm": 6.1875,
      "learning_rate": 1.4440666126057743e-05,
      "loss": 0.1808,
      "step": 93
    },
    {
      "epoch": 8.545454545454545,
      "grad_norm": 5.25,
      "learning_rate": 1.4297949120891718e-05,
      "loss": 0.1308,
      "step": 94
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 5.875,
      "learning_rate": 1.4154150130018867e-05,
      "loss": 0.1402,
      "step": 95
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 6.78125,
      "learning_rate": 1.4009305354066138e-05,
      "loss": 0.1338,
      "step": 96
    },
    {
      "epoch": 8.818181818181818,
      "grad_norm": 5.125,
      "learning_rate": 1.3863451256931286e-05,
      "loss": 0.1062,
      "step": 97
    },
    {
      "epoch": 8.909090909090908,
      "grad_norm": 4.3125,
      "learning_rate": 1.3716624556603275e-05,
      "loss": 0.1142,
      "step": 98
    },
    {
      "epoch": 9.0,
      "grad_norm": 10.125,
      "learning_rate": 1.356886221591872e-05,
      "loss": 0.1341,
      "step": 99
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 5.25,
      "learning_rate": 1.342020143325669e-05,
      "loss": 0.1187,
      "step": 100
    },
    {
      "epoch": 9.181818181818182,
      "grad_norm": 4.375,
      "learning_rate": 1.3270679633174219e-05,
      "loss": 0.0935,
      "step": 101
    },
    {
      "epoch": 9.272727272727273,
      "grad_norm": 4.125,
      "learning_rate": 1.3120334456984871e-05,
      "loss": 0.1021,
      "step": 102
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 5.0,
      "learning_rate": 1.296920375328275e-05,
      "loss": 0.0943,
      "step": 103
    },
    {
      "epoch": 9.454545454545455,
      "grad_norm": 4.375,
      "learning_rate": 1.2817325568414299e-05,
      "loss": 0.0976,
      "step": 104
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 3.984375,
      "learning_rate": 1.266473813690035e-05,
      "loss": 0.0859,
      "step": 105
    },
    {
      "epoch": 9.636363636363637,
      "grad_norm": 5.65625,
      "learning_rate": 1.2511479871810792e-05,
      "loss": 0.0957,
      "step": 106
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 8.875,
      "learning_rate": 1.2357589355094275e-05,
      "loss": 0.1057,
      "step": 107
    },
    {
      "epoch": 9.818181818181818,
      "grad_norm": 4.5,
      "learning_rate": 1.2203105327865407e-05,
      "loss": 0.0888,
      "step": 108
    },
    {
      "epoch": 9.909090909090908,
      "grad_norm": 4.84375,
      "learning_rate": 1.2048066680651908e-05,
      "loss": 0.0864,
      "step": 109
    },
    {
      "epoch": 10.0,
      "grad_norm": 4.25,
      "learning_rate": 1.1892512443604103e-05,
      "loss": 0.0861,
      "step": 110
    },
    {
      "epoch": 10.090909090909092,
      "grad_norm": 3.640625,
      "learning_rate": 1.1736481776669307e-05,
      "loss": 0.0639,
      "step": 111
    },
    {
      "epoch": 10.181818181818182,
      "grad_norm": 3.40625,
      "learning_rate": 1.15800139597335e-05,
      "loss": 0.0757,
      "step": 112
    },
    {
      "epoch": 10.272727272727273,
      "grad_norm": 3.3125,
      "learning_rate": 1.1423148382732854e-05,
      "loss": 0.0719,
      "step": 113
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 4.09375,
      "learning_rate": 1.1265924535737494e-05,
      "loss": 0.0656,
      "step": 114
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 3.921875,
      "learning_rate": 1.1108381999010111e-05,
      "loss": 0.0778,
      "step": 115
    },
    {
      "epoch": 10.545454545454545,
      "grad_norm": 3.578125,
      "learning_rate": 1.0950560433041825e-05,
      "loss": 0.0723,
      "step": 116
    },
    {
      "epoch": 10.636363636363637,
      "grad_norm": 5.0,
      "learning_rate": 1.0792499568567885e-05,
      "loss": 0.0834,
      "step": 117
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 4.25,
      "learning_rate": 1.0634239196565646e-05,
      "loss": 0.086,
      "step": 118
    },
    {
      "epoch": 10.818181818181818,
      "grad_norm": 7.5,
      "learning_rate": 1.0475819158237426e-05,
      "loss": 0.0731,
      "step": 119
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 3.03125,
      "learning_rate": 1.031727933498068e-05,
      "loss": 0.0709,
      "step": 120
    },
    {
      "epoch": 11.0,
      "grad_norm": 6.375,
      "learning_rate": 1.015865963834808e-05,
      "loss": 0.0752,
      "step": 121
    },
    {
      "epoch": 11.090909090909092,
      "grad_norm": 2.5,
      "learning_rate": 1e-05,
      "loss": 0.0568,
      "step": 122
    },
    {
      "epoch": 11.181818181818182,
      "grad_norm": 2.34375,
      "learning_rate": 9.841340361651921e-06,
      "loss": 0.051,
      "step": 123
    },
    {
      "epoch": 11.272727272727273,
      "grad_norm": 2.625,
      "learning_rate": 9.682720665019325e-06,
      "loss": 0.0604,
      "step": 124
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 2.328125,
      "learning_rate": 9.524180841762577e-06,
      "loss": 0.061,
      "step": 125
    },
    {
      "epoch": 11.454545454545455,
      "grad_norm": 3.703125,
      "learning_rate": 9.365760803434356e-06,
      "loss": 0.0713,
      "step": 126
    },
    {
      "epoch": 11.545454545454545,
      "grad_norm": 3.0,
      "learning_rate": 9.207500431432115e-06,
      "loss": 0.0669,
      "step": 127
    },
    {
      "epoch": 11.636363636363637,
      "grad_norm": 3.28125,
      "learning_rate": 9.049439566958176e-06,
      "loss": 0.0725,
      "step": 128
    },
    {
      "epoch": 11.727272727272727,
      "grad_norm": 2.78125,
      "learning_rate": 8.89161800098989e-06,
      "loss": 0.0565,
      "step": 129
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 3.546875,
      "learning_rate": 8.734075464262507e-06,
      "loss": 0.0604,
      "step": 130
    },
    {
      "epoch": 11.909090909090908,
      "grad_norm": 3.296875,
      "learning_rate": 8.576851617267151e-06,
      "loss": 0.0591,
      "step": 131
    },
    {
      "epoch": 12.0,
      "grad_norm": 3.78125,
      "learning_rate": 8.419986040266502e-06,
      "loss": 0.0731,
      "step": 132
    },
    {
      "epoch": 12.090909090909092,
      "grad_norm": 2.078125,
      "learning_rate": 8.263518223330698e-06,
      "loss": 0.0488,
      "step": 133
    },
    {
      "epoch": 12.181818181818182,
      "grad_norm": 2.65625,
      "learning_rate": 8.107487556395902e-06,
      "loss": 0.0485,
      "step": 134
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 2.984375,
      "learning_rate": 7.951933319348095e-06,
      "loss": 0.0681,
      "step": 135
    },
    {
      "epoch": 12.363636363636363,
      "grad_norm": 2.3125,
      "learning_rate": 7.796894672134594e-06,
      "loss": 0.0454,
      "step": 136
    },
    {
      "epoch": 12.454545454545455,
      "grad_norm": 3.046875,
      "learning_rate": 7.642410644905726e-06,
      "loss": 0.0628,
      "step": 137
    },
    {
      "epoch": 12.545454545454545,
      "grad_norm": 2.53125,
      "learning_rate": 7.488520128189209e-06,
      "loss": 0.0553,
      "step": 138
    },
    {
      "epoch": 12.636363636363637,
      "grad_norm": 3.078125,
      "learning_rate": 7.335261863099652e-06,
      "loss": 0.0596,
      "step": 139
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 2.734375,
      "learning_rate": 7.182674431585703e-06,
      "loss": 0.058,
      "step": 140
    },
    {
      "epoch": 12.818181818181818,
      "grad_norm": 4.375,
      "learning_rate": 7.0307962467172555e-06,
      "loss": 0.0533,
      "step": 141
    },
    {
      "epoch": 12.909090909090908,
      "grad_norm": 3.015625,
      "learning_rate": 6.87966554301513e-06,
      "loss": 0.0553,
      "step": 142
    },
    {
      "epoch": 13.0,
      "grad_norm": 3.34375,
      "learning_rate": 6.729320366825785e-06,
      "loss": 0.0536,
      "step": 143
    },
    {
      "epoch": 13.090909090909092,
      "grad_norm": 2.859375,
      "learning_rate": 6.579798566743314e-06,
      "loss": 0.0561,
      "step": 144
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 2.5625,
      "learning_rate": 6.431137784081283e-06,
      "loss": 0.058,
      "step": 145
    },
    {
      "epoch": 13.272727272727273,
      "grad_norm": 2.53125,
      "learning_rate": 6.283375443396726e-06,
      "loss": 0.0492,
      "step": 146
    },
    {
      "epoch": 13.363636363636363,
      "grad_norm": 2.59375,
      "learning_rate": 6.136548743068713e-06,
      "loss": 0.0502,
      "step": 147
    },
    {
      "epoch": 13.454545454545455,
      "grad_norm": 3.15625,
      "learning_rate": 5.990694645933866e-06,
      "loss": 0.0522,
      "step": 148
    },
    {
      "epoch": 13.545454545454545,
      "grad_norm": 3.0,
      "learning_rate": 5.845849869981137e-06,
      "loss": 0.0521,
      "step": 149
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 2.875,
      "learning_rate": 5.702050879108284e-06,
      "loss": 0.0554,
      "step": 150
    },
    {
      "epoch": 13.727272727272727,
      "grad_norm": 2.765625,
      "learning_rate": 5.559333873942259e-06,
      "loss": 0.0476,
      "step": 151
    },
    {
      "epoch": 13.818181818181818,
      "grad_norm": 2.734375,
      "learning_rate": 5.417734782725896e-06,
      "loss": 0.045,
      "step": 152
    },
    {
      "epoch": 13.909090909090908,
      "grad_norm": 3.09375,
      "learning_rate": 5.277289252273175e-06,
      "loss": 0.0527,
      "step": 153
    },
    {
      "epoch": 14.0,
      "grad_norm": 2.375,
      "learning_rate": 5.138032638995315e-06,
      "loss": 0.0476,
      "step": 154
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 2.515625,
      "learning_rate": 5.000000000000003e-06,
      "loss": 0.0493,
      "step": 155
    },
    {
      "epoch": 14.181818181818182,
      "grad_norm": 2.671875,
      "learning_rate": 4.863226084265939e-06,
      "loss": 0.0499,
      "step": 156
    },
    {
      "epoch": 14.272727272727273,
      "grad_norm": 2.9375,
      "learning_rate": 4.727745323894976e-06,
      "loss": 0.0478,
      "step": 157
    },
    {
      "epoch": 14.363636363636363,
      "grad_norm": 2.46875,
      "learning_rate": 4.593591825444028e-06,
      "loss": 0.046,
      "step": 158
    },
    {
      "epoch": 14.454545454545455,
      "grad_norm": 2.6875,
      "learning_rate": 4.460799361338898e-06,
      "loss": 0.0459,
      "step": 159
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 2.484375,
      "learning_rate": 4.3294013613722944e-06,
      "loss": 0.0538,
      "step": 160
    },
    {
      "epoch": 14.636363636363637,
      "grad_norm": 2.59375,
      "learning_rate": 4.19943090428802e-06,
      "loss": 0.0499,
      "step": 161
    },
    {
      "epoch": 14.727272727272727,
      "grad_norm": 2.734375,
      "learning_rate": 4.070920709453597e-06,
      "loss": 0.0535,
      "step": 162
    },
    {
      "epoch": 14.818181818181818,
      "grad_norm": 2.25,
      "learning_rate": 3.943903128623336e-06,
      "loss": 0.0454,
      "step": 163
    },
    {
      "epoch": 14.909090909090908,
      "grad_norm": 3.359375,
      "learning_rate": 3.818410137793947e-06,
      "loss": 0.0484,
      "step": 164
    },
    {
      "epoch": 15.0,
      "grad_norm": 3.203125,
      "learning_rate": 3.6944733291547784e-06,
      "loss": 0.0501,
      "step": 165
    },
    {
      "epoch": 15.090909090909092,
      "grad_norm": 2.625,
      "learning_rate": 3.5721239031346067e-06,
      "loss": 0.0515,
      "step": 166
    },
    {
      "epoch": 15.181818181818182,
      "grad_norm": 2.453125,
      "learning_rate": 3.4513926605471504e-06,
      "loss": 0.0451,
      "step": 167
    },
    {
      "epoch": 15.272727272727273,
      "grad_norm": 2.828125,
      "learning_rate": 3.3323099948370853e-06,
      "loss": 0.0471,
      "step": 168
    },
    {
      "epoch": 15.363636363636363,
      "grad_norm": 2.171875,
      "learning_rate": 3.2149058844286796e-06,
      "loss": 0.045,
      "step": 169
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 3.0625,
      "learning_rate": 3.099209885178882e-06,
      "loss": 0.0544,
      "step": 170
    },
    {
      "epoch": 15.545454545454545,
      "grad_norm": 3.09375,
      "learning_rate": 2.9852511229367862e-06,
      "loss": 0.0512,
      "step": 171
    },
    {
      "epoch": 15.636363636363637,
      "grad_norm": 2.296875,
      "learning_rate": 2.8730582862113743e-06,
      "loss": 0.0416,
      "step": 172
    },
    {
      "epoch": 15.727272727272727,
      "grad_norm": 2.65625,
      "learning_rate": 2.7626596189492983e-06,
      "loss": 0.0498,
      "step": 173
    },
    {
      "epoch": 15.818181818181818,
      "grad_norm": 2.296875,
      "learning_rate": 2.6540829134246683e-06,
      "loss": 0.0468,
      "step": 174
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 2.546875,
      "learning_rate": 2.5473555032424534e-06,
      "loss": 0.0485,
      "step": 175
    },
    {
      "epoch": 16.0,
      "grad_norm": 2.484375,
      "learning_rate": 2.4425042564574186e-06,
      "loss": 0.0469,
      "step": 176
    },
    {
      "epoch": 16.09090909090909,
      "grad_norm": 2.515625,
      "learning_rate": 2.339555568810221e-06,
      "loss": 0.0487,
      "step": 177
    },
    {
      "epoch": 16.181818181818183,
      "grad_norm": 2.15625,
      "learning_rate": 2.2385353570824308e-06,
      "loss": 0.0411,
      "step": 178
    },
    {
      "epoch": 16.272727272727273,
      "grad_norm": 2.890625,
      "learning_rate": 2.1394690525721275e-06,
      "loss": 0.0505,
      "step": 179
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 2.515625,
      "learning_rate": 2.0423815946916783e-06,
      "loss": 0.0456,
      "step": 180
    },
    {
      "epoch": 16.454545454545453,
      "grad_norm": 2.609375,
      "learning_rate": 1.947297424689414e-06,
      "loss": 0.0475,
      "step": 181
    },
    {
      "epoch": 16.545454545454547,
      "grad_norm": 2.953125,
      "learning_rate": 1.854240479496643e-06,
      "loss": 0.0485,
      "step": 182
    },
    {
      "epoch": 16.636363636363637,
      "grad_norm": 2.8125,
      "learning_rate": 1.7632341857016733e-06,
      "loss": 0.0455,
      "step": 183
    },
    {
      "epoch": 16.727272727272727,
      "grad_norm": 2.640625,
      "learning_rate": 1.6743014536522872e-06,
      "loss": 0.0483,
      "step": 184
    },
    {
      "epoch": 16.818181818181817,
      "grad_norm": 2.46875,
      "learning_rate": 1.587464671688187e-06,
      "loss": 0.0484,
      "step": 185
    },
    {
      "epoch": 16.90909090909091,
      "grad_norm": 2.53125,
      "learning_rate": 1.5027457005048573e-06,
      "loss": 0.0518,
      "step": 186
    },
    {
      "epoch": 17.0,
      "grad_norm": 2.4375,
      "learning_rate": 1.4201658676502294e-06,
      "loss": 0.0442,
      "step": 187
    },
    {
      "epoch": 17.09090909090909,
      "grad_norm": 2.46875,
      "learning_rate": 1.339745962155613e-06,
      "loss": 0.0481,
      "step": 188
    },
    {
      "epoch": 17.181818181818183,
      "grad_norm": 2.59375,
      "learning_rate": 1.2615062293021508e-06,
      "loss": 0.0506,
      "step": 189
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 2.65625,
      "learning_rate": 1.1854663655241804e-06,
      "loss": 0.0429,
      "step": 190
    },
    {
      "epoch": 17.363636363636363,
      "grad_norm": 2.484375,
      "learning_rate": 1.1116455134507665e-06,
      "loss": 0.042,
      "step": 191
    },
    {
      "epoch": 17.454545454545453,
      "grad_norm": 2.828125,
      "learning_rate": 1.0400622570866426e-06,
      "loss": 0.043,
      "step": 192
    },
    {
      "epoch": 17.545454545454547,
      "grad_norm": 2.296875,
      "learning_rate": 9.707346171337895e-07,
      "loss": 0.0423,
      "step": 193
    },
    {
      "epoch": 17.636363636363637,
      "grad_norm": 2.546875,
      "learning_rate": 9.036800464548157e-07,
      "loss": 0.0486,
      "step": 194
    },
    {
      "epoch": 17.727272727272727,
      "grad_norm": 2.703125,
      "learning_rate": 8.389154256793042e-07,
      "loss": 0.0489,
      "step": 195
    },
    {
      "epoch": 17.818181818181817,
      "grad_norm": 2.46875,
      "learning_rate": 7.764570589541876e-07,
      "loss": 0.0547,
      "step": 196
    },
    {
      "epoch": 17.90909090909091,
      "grad_norm": 2.703125,
      "learning_rate": 7.163206698392744e-07,
      "loss": 0.0448,
      "step": 197
    },
    {
      "epoch": 18.0,
      "grad_norm": 3.046875,
      "learning_rate": 6.585213973489335e-07,
      "loss": 0.0512,
      "step": 198
    },
    {
      "epoch": 18.09090909090909,
      "grad_norm": 2.453125,
      "learning_rate": 6.030737921409169e-07,
      "loss": 0.0531,
      "step": 199
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 2.5,
      "learning_rate": 5.499918128533155e-07,
      "loss": 0.045,
      "step": 200
    },
    {
      "epoch": 18.272727272727273,
      "grad_norm": 2.359375,
      "learning_rate": 4.992888225905467e-07,
      "loss": 0.046,
      "step": 201
    },
    {
      "epoch": 18.363636363636363,
      "grad_norm": 2.203125,
      "learning_rate": 4.509775855592613e-07,
      "loss": 0.0442,
      "step": 202
    },
    {
      "epoch": 18.454545454545453,
      "grad_norm": 2.640625,
      "learning_rate": 4.0507026385502747e-07,
      "loss": 0.0448,
      "step": 203
    },
    {
      "epoch": 18.545454545454547,
      "grad_norm": 2.78125,
      "learning_rate": 3.615784144005796e-07,
      "loss": 0.0463,
      "step": 204
    },
    {
      "epoch": 18.636363636363637,
      "grad_norm": 2.3125,
      "learning_rate": 3.2051298603643754e-07,
      "loss": 0.0486,
      "step": 205
    },
    {
      "epoch": 18.727272727272727,
      "grad_norm": 2.46875,
      "learning_rate": 2.818843167645835e-07,
      "loss": 0.0457,
      "step": 206
    },
    {
      "epoch": 18.818181818181817,
      "grad_norm": 2.390625,
      "learning_rate": 2.4570213114592957e-07,
      "loss": 0.0411,
      "step": 207
    },
    {
      "epoch": 18.90909090909091,
      "grad_norm": 3.03125,
      "learning_rate": 2.119755378522137e-07,
      "loss": 0.0507,
      "step": 208
    },
    {
      "epoch": 19.0,
      "grad_norm": 2.53125,
      "learning_rate": 1.8071302737293294e-07,
      "loss": 0.0505,
      "step": 209
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 2.53125,
      "learning_rate": 1.519224698779198e-07,
      "loss": 0.0477,
      "step": 210
    },
    {
      "epoch": 19.181818181818183,
      "grad_norm": 3.015625,
      "learning_rate": 1.2561111323605714e-07,
      "loss": 0.0512,
      "step": 211
    },
    {
      "epoch": 19.272727272727273,
      "grad_norm": 2.53125,
      "learning_rate": 1.0178558119067316e-07,
      "loss": 0.0487,
      "step": 212
    },
    {
      "epoch": 19.363636363636363,
      "grad_norm": 2.484375,
      "learning_rate": 8.04518716920466e-08,
      "loss": 0.0463,
      "step": 213
    },
    {
      "epoch": 19.454545454545453,
      "grad_norm": 2.171875,
      "learning_rate": 6.161535538745877e-08,
      "loss": 0.0504,
      "step": 214
    },
    {
      "epoch": 19.545454545454547,
      "grad_norm": 2.484375,
      "learning_rate": 4.528077426915412e-08,
      "loss": 0.0439,
      "step": 215
    },
    {
      "epoch": 19.636363636363637,
      "grad_norm": 2.953125,
      "learning_rate": 3.1452240480577265e-08,
      "loss": 0.0487,
      "step": 216
    },
    {
      "epoch": 19.727272727272727,
      "grad_norm": 2.515625,
      "learning_rate": 2.013323528115674e-08,
      "loss": 0.0422,
      "step": 217
    },
    {
      "epoch": 19.818181818181817,
      "grad_norm": 2.359375,
      "learning_rate": 1.1326608169920373e-08,
      "loss": 0.0453,
      "step": 218
    },
    {
      "epoch": 19.90909090909091,
      "grad_norm": 2.3125,
      "learning_rate": 5.034576168149175e-09,
      "loss": 0.0458,
      "step": 219
    },
    {
      "epoch": 20.0,
      "grad_norm": 2.984375,
      "learning_rate": 1.2587232612493172e-09,
      "loss": 0.0449,
      "step": 220
    }
  ],
  "logging_steps": 1,
  "max_steps": 220,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1969363313387520.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
