{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 20.0,
  "eval_steps": 500,
  "global_step": 220,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.09090909090909091,
      "grad_norm": 22.0,
      "learning_rate": 0.0,
      "loss": 1.9698,
      "step": 1
    },
    {
      "epoch": 0.18181818181818182,
      "grad_norm": 20.375,
      "learning_rate": 9.090909090909091e-07,
      "loss": 2.0558,
      "step": 2
    },
    {
      "epoch": 0.2727272727272727,
      "grad_norm": 22.375,
      "learning_rate": 1.8181818181818183e-06,
      "loss": 2.1222,
      "step": 3
    },
    {
      "epoch": 0.36363636363636365,
      "grad_norm": 19.5,
      "learning_rate": 2.7272727272727272e-06,
      "loss": 2.1064,
      "step": 4
    },
    {
      "epoch": 0.45454545454545453,
      "grad_norm": 18.875,
      "learning_rate": 3.6363636363636366e-06,
      "loss": 2.0328,
      "step": 5
    },
    {
      "epoch": 0.5454545454545454,
      "grad_norm": 20.375,
      "learning_rate": 4.5454545454545455e-06,
      "loss": 1.8396,
      "step": 6
    },
    {
      "epoch": 0.6363636363636364,
      "grad_norm": 19.75,
      "learning_rate": 5.4545454545454545e-06,
      "loss": 1.9353,
      "step": 7
    },
    {
      "epoch": 0.7272727272727273,
      "grad_norm": 17.625,
      "learning_rate": 6.363636363636364e-06,
      "loss": 1.7308,
      "step": 8
    },
    {
      "epoch": 0.8181818181818182,
      "grad_norm": 19.625,
      "learning_rate": 7.272727272727273e-06,
      "loss": 1.8313,
      "step": 9
    },
    {
      "epoch": 0.9090909090909091,
      "grad_norm": 15.625,
      "learning_rate": 8.181818181818183e-06,
      "loss": 1.6943,
      "step": 10
    },
    {
      "epoch": 1.0,
      "grad_norm": 15.9375,
      "learning_rate": 9.090909090909091e-06,
      "loss": 1.5571,
      "step": 11
    },
    {
      "epoch": 1.0909090909090908,
      "grad_norm": 15.25,
      "learning_rate": 1e-05,
      "loss": 1.5151,
      "step": 12
    },
    {
      "epoch": 1.1818181818181819,
      "grad_norm": 14.9375,
      "learning_rate": 1.0909090909090909e-05,
      "loss": 1.3749,
      "step": 13
    },
    {
      "epoch": 1.2727272727272727,
      "grad_norm": 14.4375,
      "learning_rate": 1.181818181818182e-05,
      "loss": 1.4767,
      "step": 14
    },
    {
      "epoch": 1.3636363636363638,
      "grad_norm": 16.125,
      "learning_rate": 1.2727272727272728e-05,
      "loss": 1.2704,
      "step": 15
    },
    {
      "epoch": 1.4545454545454546,
      "grad_norm": 13.625,
      "learning_rate": 1.3636363636363637e-05,
      "loss": 1.0916,
      "step": 16
    },
    {
      "epoch": 1.5454545454545454,
      "grad_norm": 11.3125,
      "learning_rate": 1.4545454545454546e-05,
      "loss": 1.2324,
      "step": 17
    },
    {
      "epoch": 1.6363636363636362,
      "grad_norm": 11.875,
      "learning_rate": 1.5454545454545454e-05,
      "loss": 1.1475,
      "step": 18
    },
    {
      "epoch": 1.7272727272727273,
      "grad_norm": 7.1875,
      "learning_rate": 1.6363636363636366e-05,
      "loss": 1.1223,
      "step": 19
    },
    {
      "epoch": 1.8181818181818183,
      "grad_norm": 5.78125,
      "learning_rate": 1.7272727272727274e-05,
      "loss": 1.0276,
      "step": 20
    },
    {
      "epoch": 1.9090909090909092,
      "grad_norm": 6.15625,
      "learning_rate": 1.8181818181818182e-05,
      "loss": 1.0616,
      "step": 21
    },
    {
      "epoch": 2.0,
      "grad_norm": 5.65625,
      "learning_rate": 1.9090909090909094e-05,
      "loss": 1.1501,
      "step": 22
    },
    {
      "epoch": 2.090909090909091,
      "grad_norm": 5.5,
      "learning_rate": 2e-05,
      "loss": 1.0992,
      "step": 23
    },
    {
      "epoch": 2.1818181818181817,
      "grad_norm": 5.875,
      "learning_rate": 1.9998741276738753e-05,
      "loss": 0.9509,
      "step": 24
    },
    {
      "epoch": 2.2727272727272725,
      "grad_norm": 5.46875,
      "learning_rate": 1.9994965423831853e-05,
      "loss": 0.8461,
      "step": 25
    },
    {
      "epoch": 2.3636363636363638,
      "grad_norm": 5.4375,
      "learning_rate": 1.9988673391830082e-05,
      "loss": 0.8678,
      "step": 26
    },
    {
      "epoch": 2.4545454545454546,
      "grad_norm": 10.1875,
      "learning_rate": 1.9979866764718846e-05,
      "loss": 1.034,
      "step": 27
    },
    {
      "epoch": 2.5454545454545454,
      "grad_norm": 9.6875,
      "learning_rate": 1.9968547759519426e-05,
      "loss": 0.911,
      "step": 28
    },
    {
      "epoch": 2.6363636363636362,
      "grad_norm": 6.75,
      "learning_rate": 1.9954719225730847e-05,
      "loss": 0.9491,
      "step": 29
    },
    {
      "epoch": 2.7272727272727275,
      "grad_norm": 5.375,
      "learning_rate": 1.9938384644612542e-05,
      "loss": 0.8934,
      "step": 30
    },
    {
      "epoch": 2.8181818181818183,
      "grad_norm": 5.9375,
      "learning_rate": 1.9919548128307954e-05,
      "loss": 0.9118,
      "step": 31
    },
    {
      "epoch": 2.909090909090909,
      "grad_norm": 5.46875,
      "learning_rate": 1.989821441880933e-05,
      "loss": 0.893,
      "step": 32
    },
    {
      "epoch": 3.0,
      "grad_norm": 5.5,
      "learning_rate": 1.9874388886763944e-05,
      "loss": 0.8841,
      "step": 33
    },
    {
      "epoch": 3.090909090909091,
      "grad_norm": 5.75,
      "learning_rate": 1.9848077530122083e-05,
      "loss": 0.8024,
      "step": 34
    },
    {
      "epoch": 3.1818181818181817,
      "grad_norm": 5.65625,
      "learning_rate": 1.9819286972627066e-05,
      "loss": 0.7402,
      "step": 35
    },
    {
      "epoch": 3.2727272727272725,
      "grad_norm": 10.9375,
      "learning_rate": 1.978802446214779e-05,
      "loss": 0.7161,
      "step": 36
    },
    {
      "epoch": 3.3636363636363638,
      "grad_norm": 6.75,
      "learning_rate": 1.9754297868854075e-05,
      "loss": 0.6664,
      "step": 37
    },
    {
      "epoch": 3.4545454545454546,
      "grad_norm": 8.1875,
      "learning_rate": 1.9718115683235418e-05,
      "loss": 0.6699,
      "step": 38
    },
    {
      "epoch": 3.5454545454545454,
      "grad_norm": 7.0625,
      "learning_rate": 1.9679487013963566e-05,
      "loss": 0.6137,
      "step": 39
    },
    {
      "epoch": 3.6363636363636362,
      "grad_norm": 6.0625,
      "learning_rate": 1.9638421585599422e-05,
      "loss": 0.7378,
      "step": 40
    },
    {
      "epoch": 3.7272727272727275,
      "grad_norm": 5.875,
      "learning_rate": 1.9594929736144978e-05,
      "loss": 0.6699,
      "step": 41
    },
    {
      "epoch": 3.8181818181818183,
      "grad_norm": 6.4375,
      "learning_rate": 1.9549022414440738e-05,
      "loss": 0.6976,
      "step": 42
    },
    {
      "epoch": 3.909090909090909,
      "grad_norm": 5.5625,
      "learning_rate": 1.9500711177409456e-05,
      "loss": 0.6115,
      "step": 43
    },
    {
      "epoch": 4.0,
      "grad_norm": 6.0625,
      "learning_rate": 1.9450008187146685e-05,
      "loss": 0.6167,
      "step": 44
    },
    {
      "epoch": 4.090909090909091,
      "grad_norm": 7.28125,
      "learning_rate": 1.9396926207859085e-05,
      "loss": 0.4735,
      "step": 45
    },
    {
      "epoch": 4.181818181818182,
      "grad_norm": 7.625,
      "learning_rate": 1.9341478602651068e-05,
      "loss": 0.4488,
      "step": 46
    },
    {
      "epoch": 4.2727272727272725,
      "grad_norm": 7.875,
      "learning_rate": 1.9283679330160726e-05,
      "loss": 0.5844,
      "step": 47
    },
    {
      "epoch": 4.363636363636363,
      "grad_norm": 12.75,
      "learning_rate": 1.9223542941045817e-05,
      "loss": 0.5115,
      "step": 48
    },
    {
      "epoch": 4.454545454545454,
      "grad_norm": 19.375,
      "learning_rate": 1.9161084574320696e-05,
      "loss": 0.448,
      "step": 49
    },
    {
      "epoch": 4.545454545454545,
      "grad_norm": 14.3125,
      "learning_rate": 1.9096319953545186e-05,
      "loss": 0.4786,
      "step": 50
    },
    {
      "epoch": 4.636363636363637,
      "grad_norm": 8.0625,
      "learning_rate": 1.9029265382866216e-05,
      "loss": 0.4531,
      "step": 51
    },
    {
      "epoch": 4.7272727272727275,
      "grad_norm": 6.71875,
      "learning_rate": 1.895993774291336e-05,
      "loss": 0.4678,
      "step": 52
    },
    {
      "epoch": 4.818181818181818,
      "grad_norm": 10.3125,
      "learning_rate": 1.8888354486549238e-05,
      "loss": 0.4712,
      "step": 53
    },
    {
      "epoch": 4.909090909090909,
      "grad_norm": 6.3125,
      "learning_rate": 1.881453363447582e-05,
      "loss": 0.4014,
      "step": 54
    },
    {
      "epoch": 5.0,
      "grad_norm": 8.5625,
      "learning_rate": 1.873849377069785e-05,
      "loss": 0.4527,
      "step": 55
    },
    {
      "epoch": 5.090909090909091,
      "grad_norm": 8.4375,
      "learning_rate": 1.866025403784439e-05,
      "loss": 0.3625,
      "step": 56
    },
    {
      "epoch": 5.181818181818182,
      "grad_norm": 6.9375,
      "learning_rate": 1.8579834132349773e-05,
      "loss": 0.3227,
      "step": 57
    },
    {
      "epoch": 5.2727272727272725,
      "grad_norm": 6.53125,
      "learning_rate": 1.8497254299495147e-05,
      "loss": 0.2694,
      "step": 58
    },
    {
      "epoch": 5.363636363636363,
      "grad_norm": 9.0625,
      "learning_rate": 1.8412535328311813e-05,
      "loss": 0.2281,
      "step": 59
    },
    {
      "epoch": 5.454545454545454,
      "grad_norm": 11.5,
      "learning_rate": 1.8325698546347714e-05,
      "loss": 0.3173,
      "step": 60
    },
    {
      "epoch": 5.545454545454545,
      "grad_norm": 16.375,
      "learning_rate": 1.8236765814298328e-05,
      "loss": 0.3762,
      "step": 61
    },
    {
      "epoch": 5.636363636363637,
      "grad_norm": 18.375,
      "learning_rate": 1.814575952050336e-05,
      "loss": 0.3491,
      "step": 62
    },
    {
      "epoch": 5.7272727272727275,
      "grad_norm": 11.5625,
      "learning_rate": 1.8052702575310588e-05,
      "loss": 0.242,
      "step": 63
    },
    {
      "epoch": 5.818181818181818,
      "grad_norm": 10.0625,
      "learning_rate": 1.7957618405308323e-05,
      "loss": 0.2499,
      "step": 64
    },
    {
      "epoch": 5.909090909090909,
      "grad_norm": 8.5625,
      "learning_rate": 1.7860530947427878e-05,
      "loss": 0.2892,
      "step": 65
    },
    {
      "epoch": 6.0,
      "grad_norm": 7.78125,
      "learning_rate": 1.776146464291757e-05,
      "loss": 0.273,
      "step": 66
    },
    {
      "epoch": 6.090909090909091,
      "grad_norm": 6.34375,
      "learning_rate": 1.766044443118978e-05,
      "loss": 0.1836,
      "step": 67
    },
    {
      "epoch": 6.181818181818182,
      "grad_norm": 6.40625,
      "learning_rate": 1.7557495743542586e-05,
      "loss": 0.1471,
      "step": 68
    },
    {
      "epoch": 6.2727272727272725,
      "grad_norm": 6.5625,
      "learning_rate": 1.745264449675755e-05,
      "loss": 0.2068,
      "step": 69
    },
    {
      "epoch": 6.363636363636363,
      "grad_norm": 6.21875,
      "learning_rate": 1.734591708657533e-05,
      "loss": 0.1719,
      "step": 70
    },
    {
      "epoch": 6.454545454545454,
      "grad_norm": 6.125,
      "learning_rate": 1.72373403810507e-05,
      "loss": 0.148,
      "step": 71
    },
    {
      "epoch": 6.545454545454545,
      "grad_norm": 7.5,
      "learning_rate": 1.7126941713788633e-05,
      "loss": 0.1734,
      "step": 72
    },
    {
      "epoch": 6.636363636363637,
      "grad_norm": 6.96875,
      "learning_rate": 1.7014748877063212e-05,
      "loss": 0.159,
      "step": 73
    },
    {
      "epoch": 6.7272727272727275,
      "grad_norm": 8.0625,
      "learning_rate": 1.6900790114821122e-05,
      "loss": 0.1413,
      "step": 74
    },
    {
      "epoch": 6.818181818181818,
      "grad_norm": 9.1875,
      "learning_rate": 1.6785094115571323e-05,
      "loss": 0.1619,
      "step": 75
    },
    {
      "epoch": 6.909090909090909,
      "grad_norm": 13.4375,
      "learning_rate": 1.666769000516292e-05,
      "loss": 0.1952,
      "step": 76
    },
    {
      "epoch": 7.0,
      "grad_norm": 17.375,
      "learning_rate": 1.6548607339452853e-05,
      "loss": 0.1827,
      "step": 77
    },
    {
      "epoch": 7.090909090909091,
      "grad_norm": 11.1875,
      "learning_rate": 1.6427876096865394e-05,
      "loss": 0.1031,
      "step": 78
    },
    {
      "epoch": 7.181818181818182,
      "grad_norm": 7.15625,
      "learning_rate": 1.6305526670845225e-05,
      "loss": 0.113,
      "step": 79
    },
    {
      "epoch": 7.2727272727272725,
      "grad_norm": 7.0,
      "learning_rate": 1.6181589862206053e-05,
      "loss": 0.1046,
      "step": 80
    },
    {
      "epoch": 7.363636363636363,
      "grad_norm": 6.53125,
      "learning_rate": 1.6056096871376667e-05,
      "loss": 0.0889,
      "step": 81
    },
    {
      "epoch": 7.454545454545454,
      "grad_norm": 8.5,
      "learning_rate": 1.5929079290546408e-05,
      "loss": 0.1305,
      "step": 82
    },
    {
      "epoch": 7.545454545454545,
      "grad_norm": 5.125,
      "learning_rate": 1.5800569095711983e-05,
      "loss": 0.0959,
      "step": 83
    },
    {
      "epoch": 7.636363636363637,
      "grad_norm": 5.6875,
      "learning_rate": 1.5670598638627707e-05,
      "loss": 0.1086,
      "step": 84
    },
    {
      "epoch": 7.7272727272727275,
      "grad_norm": 5.21875,
      "learning_rate": 1.5539200638661106e-05,
      "loss": 0.0917,
      "step": 85
    },
    {
      "epoch": 7.818181818181818,
      "grad_norm": 4.65625,
      "learning_rate": 1.5406408174555978e-05,
      "loss": 0.0814,
      "step": 86
    },
    {
      "epoch": 7.909090909090909,
      "grad_norm": 4.875,
      "learning_rate": 1.5272254676105026e-05,
      "loss": 0.0948,
      "step": 87
    },
    {
      "epoch": 8.0,
      "grad_norm": 5.15625,
      "learning_rate": 1.5136773915734067e-05,
      "loss": 0.0986,
      "step": 88
    },
    {
      "epoch": 8.090909090909092,
      "grad_norm": 3.203125,
      "learning_rate": 1.5000000000000002e-05,
      "loss": 0.0687,
      "step": 89
    },
    {
      "epoch": 8.181818181818182,
      "grad_norm": 3.34375,
      "learning_rate": 1.4861967361004687e-05,
      "loss": 0.0632,
      "step": 90
    },
    {
      "epoch": 8.272727272727273,
      "grad_norm": 4.46875,
      "learning_rate": 1.472271074772683e-05,
      "loss": 0.0638,
      "step": 91
    },
    {
      "epoch": 8.363636363636363,
      "grad_norm": 5.71875,
      "learning_rate": 1.4582265217274105e-05,
      "loss": 0.0796,
      "step": 92
    },
    {
      "epoch": 8.454545454545455,
      "grad_norm": 7.28125,
      "learning_rate": 1.4440666126057743e-05,
      "loss": 0.0972,
      "step": 93
    },
    {
      "epoch": 8.545454545454545,
      "grad_norm": 6.3125,
      "learning_rate": 1.4297949120891718e-05,
      "loss": 0.0729,
      "step": 94
    },
    {
      "epoch": 8.636363636363637,
      "grad_norm": 5.25,
      "learning_rate": 1.4154150130018867e-05,
      "loss": 0.0918,
      "step": 95
    },
    {
      "epoch": 8.727272727272727,
      "grad_norm": 3.65625,
      "learning_rate": 1.4009305354066138e-05,
      "loss": 0.0674,
      "step": 96
    },
    {
      "epoch": 8.818181818181818,
      "grad_norm": 3.90625,
      "learning_rate": 1.3863451256931286e-05,
      "loss": 0.0742,
      "step": 97
    },
    {
      "epoch": 8.909090909090908,
      "grad_norm": 4.59375,
      "learning_rate": 1.3716624556603275e-05,
      "loss": 0.0726,
      "step": 98
    },
    {
      "epoch": 9.0,
      "grad_norm": 4.46875,
      "learning_rate": 1.356886221591872e-05,
      "loss": 0.088,
      "step": 99
    },
    {
      "epoch": 9.090909090909092,
      "grad_norm": 2.9375,
      "learning_rate": 1.342020143325669e-05,
      "loss": 0.0601,
      "step": 100
    },
    {
      "epoch": 9.181818181818182,
      "grad_norm": 3.5625,
      "learning_rate": 1.3270679633174219e-05,
      "loss": 0.055,
      "step": 101
    },
    {
      "epoch": 9.272727272727273,
      "grad_norm": 2.828125,
      "learning_rate": 1.3120334456984871e-05,
      "loss": 0.0549,
      "step": 102
    },
    {
      "epoch": 9.363636363636363,
      "grad_norm": 5.8125,
      "learning_rate": 1.296920375328275e-05,
      "loss": 0.0711,
      "step": 103
    },
    {
      "epoch": 9.454545454545455,
      "grad_norm": 8.875,
      "learning_rate": 1.2817325568414299e-05,
      "loss": 0.0624,
      "step": 104
    },
    {
      "epoch": 9.545454545454545,
      "grad_norm": 3.640625,
      "learning_rate": 1.266473813690035e-05,
      "loss": 0.0571,
      "step": 105
    },
    {
      "epoch": 9.636363636363637,
      "grad_norm": 4.25,
      "learning_rate": 1.2511479871810792e-05,
      "loss": 0.0717,
      "step": 106
    },
    {
      "epoch": 9.727272727272727,
      "grad_norm": 2.96875,
      "learning_rate": 1.2357589355094275e-05,
      "loss": 0.0553,
      "step": 107
    },
    {
      "epoch": 9.818181818181818,
      "grad_norm": 4.09375,
      "learning_rate": 1.2203105327865407e-05,
      "loss": 0.0627,
      "step": 108
    },
    {
      "epoch": 9.909090909090908,
      "grad_norm": 3.703125,
      "learning_rate": 1.2048066680651908e-05,
      "loss": 0.0613,
      "step": 109
    },
    {
      "epoch": 10.0,
      "grad_norm": 2.734375,
      "learning_rate": 1.1892512443604103e-05,
      "loss": 0.0542,
      "step": 110
    },
    {
      "epoch": 10.090909090909092,
      "grad_norm": 2.9375,
      "learning_rate": 1.1736481776669307e-05,
      "loss": 0.0556,
      "step": 111
    },
    {
      "epoch": 10.181818181818182,
      "grad_norm": 2.53125,
      "learning_rate": 1.15800139597335e-05,
      "loss": 0.0512,
      "step": 112
    },
    {
      "epoch": 10.272727272727273,
      "grad_norm": 2.515625,
      "learning_rate": 1.1423148382732854e-05,
      "loss": 0.0442,
      "step": 113
    },
    {
      "epoch": 10.363636363636363,
      "grad_norm": 2.296875,
      "learning_rate": 1.1265924535737494e-05,
      "loss": 0.0469,
      "step": 114
    },
    {
      "epoch": 10.454545454545455,
      "grad_norm": 2.21875,
      "learning_rate": 1.1108381999010111e-05,
      "loss": 0.0501,
      "step": 115
    },
    {
      "epoch": 10.545454545454545,
      "grad_norm": 2.578125,
      "learning_rate": 1.0950560433041825e-05,
      "loss": 0.0509,
      "step": 116
    },
    {
      "epoch": 10.636363636363637,
      "grad_norm": 3.09375,
      "learning_rate": 1.0792499568567885e-05,
      "loss": 0.0543,
      "step": 117
    },
    {
      "epoch": 10.727272727272727,
      "grad_norm": 3.203125,
      "learning_rate": 1.0634239196565646e-05,
      "loss": 0.055,
      "step": 118
    },
    {
      "epoch": 10.818181818181818,
      "grad_norm": 2.6875,
      "learning_rate": 1.0475819158237426e-05,
      "loss": 0.0505,
      "step": 119
    },
    {
      "epoch": 10.909090909090908,
      "grad_norm": 2.75,
      "learning_rate": 1.031727933498068e-05,
      "loss": 0.0539,
      "step": 120
    },
    {
      "epoch": 11.0,
      "grad_norm": 3.0,
      "learning_rate": 1.015865963834808e-05,
      "loss": 0.0595,
      "step": 121
    },
    {
      "epoch": 11.090909090909092,
      "grad_norm": 2.140625,
      "learning_rate": 1e-05,
      "loss": 0.0479,
      "step": 122
    },
    {
      "epoch": 11.181818181818182,
      "grad_norm": 2.0625,
      "learning_rate": 9.841340361651921e-06,
      "loss": 0.0385,
      "step": 123
    },
    {
      "epoch": 11.272727272727273,
      "grad_norm": 2.453125,
      "learning_rate": 9.682720665019325e-06,
      "loss": 0.0425,
      "step": 124
    },
    {
      "epoch": 11.363636363636363,
      "grad_norm": 1.8203125,
      "learning_rate": 9.524180841762577e-06,
      "loss": 0.0461,
      "step": 125
    },
    {
      "epoch": 11.454545454545455,
      "grad_norm": 2.671875,
      "learning_rate": 9.365760803434356e-06,
      "loss": 0.0521,
      "step": 126
    },
    {
      "epoch": 11.545454545454545,
      "grad_norm": 3.671875,
      "learning_rate": 9.207500431432115e-06,
      "loss": 0.0532,
      "step": 127
    },
    {
      "epoch": 11.636363636363637,
      "grad_norm": 3.0,
      "learning_rate": 9.049439566958176e-06,
      "loss": 0.0491,
      "step": 128
    },
    {
      "epoch": 11.727272727272727,
      "grad_norm": 2.328125,
      "learning_rate": 8.89161800098989e-06,
      "loss": 0.0438,
      "step": 129
    },
    {
      "epoch": 11.818181818181818,
      "grad_norm": 2.28125,
      "learning_rate": 8.734075464262507e-06,
      "loss": 0.0443,
      "step": 130
    },
    {
      "epoch": 11.909090909090908,
      "grad_norm": 2.4375,
      "learning_rate": 8.576851617267151e-06,
      "loss": 0.0435,
      "step": 131
    },
    {
      "epoch": 12.0,
      "grad_norm": 2.4375,
      "learning_rate": 8.419986040266502e-06,
      "loss": 0.0458,
      "step": 132
    },
    {
      "epoch": 12.090909090909092,
      "grad_norm": 2.265625,
      "learning_rate": 8.263518223330698e-06,
      "loss": 0.0401,
      "step": 133
    },
    {
      "epoch": 12.181818181818182,
      "grad_norm": 2.34375,
      "learning_rate": 8.107487556395902e-06,
      "loss": 0.0425,
      "step": 134
    },
    {
      "epoch": 12.272727272727273,
      "grad_norm": 2.640625,
      "learning_rate": 7.951933319348095e-06,
      "loss": 0.0492,
      "step": 135
    },
    {
      "epoch": 12.363636363636363,
      "grad_norm": 1.8984375,
      "learning_rate": 7.796894672134594e-06,
      "loss": 0.0385,
      "step": 136
    },
    {
      "epoch": 12.454545454545455,
      "grad_norm": 2.75,
      "learning_rate": 7.642410644905726e-06,
      "loss": 0.0473,
      "step": 137
    },
    {
      "epoch": 12.545454545454545,
      "grad_norm": 2.125,
      "learning_rate": 7.488520128189209e-06,
      "loss": 0.0431,
      "step": 138
    },
    {
      "epoch": 12.636363636363637,
      "grad_norm": 2.28125,
      "learning_rate": 7.335261863099652e-06,
      "loss": 0.044,
      "step": 139
    },
    {
      "epoch": 12.727272727272727,
      "grad_norm": 2.34375,
      "learning_rate": 7.182674431585703e-06,
      "loss": 0.0443,
      "step": 140
    },
    {
      "epoch": 12.818181818181818,
      "grad_norm": 2.546875,
      "learning_rate": 7.0307962467172555e-06,
      "loss": 0.0411,
      "step": 141
    },
    {
      "epoch": 12.909090909090908,
      "grad_norm": 2.640625,
      "learning_rate": 6.87966554301513e-06,
      "loss": 0.0429,
      "step": 142
    },
    {
      "epoch": 13.0,
      "grad_norm": 2.265625,
      "learning_rate": 6.729320366825785e-06,
      "loss": 0.04,
      "step": 143
    },
    {
      "epoch": 13.090909090909092,
      "grad_norm": 2.328125,
      "learning_rate": 6.579798566743314e-06,
      "loss": 0.0446,
      "step": 144
    },
    {
      "epoch": 13.181818181818182,
      "grad_norm": 1.8203125,
      "learning_rate": 6.431137784081283e-06,
      "loss": 0.0386,
      "step": 145
    },
    {
      "epoch": 13.272727272727273,
      "grad_norm": 2.109375,
      "learning_rate": 6.283375443396726e-06,
      "loss": 0.0425,
      "step": 146
    },
    {
      "epoch": 13.363636363636363,
      "grad_norm": 2.359375,
      "learning_rate": 6.136548743068713e-06,
      "loss": 0.0403,
      "step": 147
    },
    {
      "epoch": 13.454545454545455,
      "grad_norm": 2.25,
      "learning_rate": 5.990694645933866e-06,
      "loss": 0.0393,
      "step": 148
    },
    {
      "epoch": 13.545454545454545,
      "grad_norm": 3.859375,
      "learning_rate": 5.845849869981137e-06,
      "loss": 0.0435,
      "step": 149
    },
    {
      "epoch": 13.636363636363637,
      "grad_norm": 2.21875,
      "learning_rate": 5.702050879108284e-06,
      "loss": 0.0415,
      "step": 150
    },
    {
      "epoch": 13.727272727272727,
      "grad_norm": 2.703125,
      "learning_rate": 5.559333873942259e-06,
      "loss": 0.0422,
      "step": 151
    },
    {
      "epoch": 13.818181818181818,
      "grad_norm": 2.21875,
      "learning_rate": 5.417734782725896e-06,
      "loss": 0.0373,
      "step": 152
    },
    {
      "epoch": 13.909090909090908,
      "grad_norm": 2.640625,
      "learning_rate": 5.277289252273175e-06,
      "loss": 0.0433,
      "step": 153
    },
    {
      "epoch": 14.0,
      "grad_norm": 2.0625,
      "learning_rate": 5.138032638995315e-06,
      "loss": 0.0398,
      "step": 154
    },
    {
      "epoch": 14.090909090909092,
      "grad_norm": 1.84375,
      "learning_rate": 5.000000000000003e-06,
      "loss": 0.0378,
      "step": 155
    },
    {
      "epoch": 14.181818181818182,
      "grad_norm": 2.3125,
      "learning_rate": 4.863226084265939e-06,
      "loss": 0.0413,
      "step": 156
    },
    {
      "epoch": 14.272727272727273,
      "grad_norm": 2.296875,
      "learning_rate": 4.727745323894976e-06,
      "loss": 0.0404,
      "step": 157
    },
    {
      "epoch": 14.363636363636363,
      "grad_norm": 1.7734375,
      "learning_rate": 4.593591825444028e-06,
      "loss": 0.0371,
      "step": 158
    },
    {
      "epoch": 14.454545454545455,
      "grad_norm": 2.0625,
      "learning_rate": 4.460799361338898e-06,
      "loss": 0.0368,
      "step": 159
    },
    {
      "epoch": 14.545454545454545,
      "grad_norm": 2.5,
      "learning_rate": 4.3294013613722944e-06,
      "loss": 0.0415,
      "step": 160
    },
    {
      "epoch": 14.636363636363637,
      "grad_norm": 3.09375,
      "learning_rate": 4.19943090428802e-06,
      "loss": 0.0445,
      "step": 161
    },
    {
      "epoch": 14.727272727272727,
      "grad_norm": 2.078125,
      "learning_rate": 4.070920709453597e-06,
      "loss": 0.0409,
      "step": 162
    },
    {
      "epoch": 14.818181818181818,
      "grad_norm": 2.171875,
      "learning_rate": 3.943903128623336e-06,
      "loss": 0.038,
      "step": 163
    },
    {
      "epoch": 14.909090909090908,
      "grad_norm": 2.53125,
      "learning_rate": 3.818410137793947e-06,
      "loss": 0.0408,
      "step": 164
    },
    {
      "epoch": 15.0,
      "grad_norm": 2.15625,
      "learning_rate": 3.6944733291547784e-06,
      "loss": 0.0389,
      "step": 165
    },
    {
      "epoch": 15.090909090909092,
      "grad_norm": 2.015625,
      "learning_rate": 3.5721239031346067e-06,
      "loss": 0.0399,
      "step": 166
    },
    {
      "epoch": 15.181818181818182,
      "grad_norm": 1.96875,
      "learning_rate": 3.4513926605471504e-06,
      "loss": 0.036,
      "step": 167
    },
    {
      "epoch": 15.272727272727273,
      "grad_norm": 2.328125,
      "learning_rate": 3.3323099948370853e-06,
      "loss": 0.0418,
      "step": 168
    },
    {
      "epoch": 15.363636363636363,
      "grad_norm": 1.78125,
      "learning_rate": 3.2149058844286796e-06,
      "loss": 0.0379,
      "step": 169
    },
    {
      "epoch": 15.454545454545455,
      "grad_norm": 2.328125,
      "learning_rate": 3.099209885178882e-06,
      "loss": 0.0407,
      "step": 170
    },
    {
      "epoch": 15.545454545454545,
      "grad_norm": 2.1875,
      "learning_rate": 2.9852511229367862e-06,
      "loss": 0.0391,
      "step": 171
    },
    {
      "epoch": 15.636363636363637,
      "grad_norm": 2.140625,
      "learning_rate": 2.8730582862113743e-06,
      "loss": 0.0356,
      "step": 172
    },
    {
      "epoch": 15.727272727272727,
      "grad_norm": 2.453125,
      "learning_rate": 2.7626596189492983e-06,
      "loss": 0.0427,
      "step": 173
    },
    {
      "epoch": 15.818181818181818,
      "grad_norm": 1.625,
      "learning_rate": 2.6540829134246683e-06,
      "loss": 0.0376,
      "step": 174
    },
    {
      "epoch": 15.909090909090908,
      "grad_norm": 2.453125,
      "learning_rate": 2.5473555032424534e-06,
      "loss": 0.041,
      "step": 175
    },
    {
      "epoch": 16.0,
      "grad_norm": 1.90625,
      "learning_rate": 2.4425042564574186e-06,
      "loss": 0.0387,
      "step": 176
    },
    {
      "epoch": 16.09090909090909,
      "grad_norm": 2.171875,
      "learning_rate": 2.339555568810221e-06,
      "loss": 0.041,
      "step": 177
    },
    {
      "epoch": 16.181818181818183,
      "grad_norm": 1.875,
      "learning_rate": 2.2385353570824308e-06,
      "loss": 0.0351,
      "step": 178
    },
    {
      "epoch": 16.272727272727273,
      "grad_norm": 2.234375,
      "learning_rate": 2.1394690525721275e-06,
      "loss": 0.0406,
      "step": 179
    },
    {
      "epoch": 16.363636363636363,
      "grad_norm": 2.1875,
      "learning_rate": 2.0423815946916783e-06,
      "loss": 0.04,
      "step": 180
    },
    {
      "epoch": 16.454545454545453,
      "grad_norm": 1.984375,
      "learning_rate": 1.947297424689414e-06,
      "loss": 0.0387,
      "step": 181
    },
    {
      "epoch": 16.545454545454547,
      "grad_norm": 2.46875,
      "learning_rate": 1.854240479496643e-06,
      "loss": 0.0417,
      "step": 182
    },
    {
      "epoch": 16.636363636363637,
      "grad_norm": 1.96875,
      "learning_rate": 1.7632341857016733e-06,
      "loss": 0.0372,
      "step": 183
    },
    {
      "epoch": 16.727272727272727,
      "grad_norm": 2.015625,
      "learning_rate": 1.6743014536522872e-06,
      "loss": 0.0369,
      "step": 184
    },
    {
      "epoch": 16.818181818181817,
      "grad_norm": 2.15625,
      "learning_rate": 1.587464671688187e-06,
      "loss": 0.0377,
      "step": 185
    },
    {
      "epoch": 16.90909090909091,
      "grad_norm": 2.46875,
      "learning_rate": 1.5027457005048573e-06,
      "loss": 0.0412,
      "step": 186
    },
    {
      "epoch": 17.0,
      "grad_norm": 1.8671875,
      "learning_rate": 1.4201658676502294e-06,
      "loss": 0.0365,
      "step": 187
    },
    {
      "epoch": 17.09090909090909,
      "grad_norm": 2.015625,
      "learning_rate": 1.339745962155613e-06,
      "loss": 0.0385,
      "step": 188
    },
    {
      "epoch": 17.181818181818183,
      "grad_norm": 2.578125,
      "learning_rate": 1.2615062293021508e-06,
      "loss": 0.0455,
      "step": 189
    },
    {
      "epoch": 17.272727272727273,
      "grad_norm": 2.03125,
      "learning_rate": 1.1854663655241804e-06,
      "loss": 0.0365,
      "step": 190
    },
    {
      "epoch": 17.363636363636363,
      "grad_norm": 2.0,
      "learning_rate": 1.1116455134507665e-06,
      "loss": 0.0363,
      "step": 191
    },
    {
      "epoch": 17.454545454545453,
      "grad_norm": 1.8984375,
      "learning_rate": 1.0400622570866426e-06,
      "loss": 0.0378,
      "step": 192
    },
    {
      "epoch": 17.545454545454547,
      "grad_norm": 1.8203125,
      "learning_rate": 9.707346171337895e-07,
      "loss": 0.0364,
      "step": 193
    },
    {
      "epoch": 17.636363636363637,
      "grad_norm": 1.9921875,
      "learning_rate": 9.036800464548157e-07,
      "loss": 0.0358,
      "step": 194
    },
    {
      "epoch": 17.727272727272727,
      "grad_norm": 2.359375,
      "learning_rate": 8.389154256793042e-07,
      "loss": 0.0377,
      "step": 195
    },
    {
      "epoch": 17.818181818181817,
      "grad_norm": 1.9765625,
      "learning_rate": 7.764570589541876e-07,
      "loss": 0.0409,
      "step": 196
    },
    {
      "epoch": 17.90909090909091,
      "grad_norm": 1.921875,
      "learning_rate": 7.163206698392744e-07,
      "loss": 0.0377,
      "step": 197
    },
    {
      "epoch": 18.0,
      "grad_norm": 2.625,
      "learning_rate": 6.585213973489335e-07,
      "loss": 0.0418,
      "step": 198
    },
    {
      "epoch": 18.09090909090909,
      "grad_norm": 2.140625,
      "learning_rate": 6.030737921409169e-07,
      "loss": 0.0434,
      "step": 199
    },
    {
      "epoch": 18.181818181818183,
      "grad_norm": 2.0,
      "learning_rate": 5.499918128533155e-07,
      "loss": 0.0369,
      "step": 200
    },
    {
      "epoch": 18.272727272727273,
      "grad_norm": 1.921875,
      "learning_rate": 4.992888225905467e-07,
      "loss": 0.0377,
      "step": 201
    },
    {
      "epoch": 18.363636363636363,
      "grad_norm": 1.96875,
      "learning_rate": 4.509775855592613e-07,
      "loss": 0.0387,
      "step": 202
    },
    {
      "epoch": 18.454545454545453,
      "grad_norm": 2.21875,
      "learning_rate": 4.0507026385502747e-07,
      "loss": 0.0376,
      "step": 203
    },
    {
      "epoch": 18.545454545454547,
      "grad_norm": 2.359375,
      "learning_rate": 3.615784144005796e-07,
      "loss": 0.039,
      "step": 204
    },
    {
      "epoch": 18.636363636363637,
      "grad_norm": 1.7421875,
      "learning_rate": 3.2051298603643754e-07,
      "loss": 0.0387,
      "step": 205
    },
    {
      "epoch": 18.727272727272727,
      "grad_norm": 1.8984375,
      "learning_rate": 2.818843167645835e-07,
      "loss": 0.0366,
      "step": 206
    },
    {
      "epoch": 18.818181818181817,
      "grad_norm": 2.03125,
      "learning_rate": 2.4570213114592957e-07,
      "loss": 0.036,
      "step": 207
    },
    {
      "epoch": 18.90909090909091,
      "grad_norm": 2.25,
      "learning_rate": 2.119755378522137e-07,
      "loss": 0.0392,
      "step": 208
    },
    {
      "epoch": 19.0,
      "grad_norm": 2.359375,
      "learning_rate": 1.8071302737293294e-07,
      "loss": 0.0411,
      "step": 209
    },
    {
      "epoch": 19.09090909090909,
      "grad_norm": 1.953125,
      "learning_rate": 1.519224698779198e-07,
      "loss": 0.0403,
      "step": 210
    },
    {
      "epoch": 19.181818181818183,
      "grad_norm": 2.421875,
      "learning_rate": 1.2561111323605714e-07,
      "loss": 0.0432,
      "step": 211
    },
    {
      "epoch": 19.272727272727273,
      "grad_norm": 1.8203125,
      "learning_rate": 1.0178558119067316e-07,
      "loss": 0.0366,
      "step": 212
    },
    {
      "epoch": 19.363636363636363,
      "grad_norm": 2.140625,
      "learning_rate": 8.04518716920466e-08,
      "loss": 0.0365,
      "step": 213
    },
    {
      "epoch": 19.454545454545453,
      "grad_norm": 1.6796875,
      "learning_rate": 6.161535538745877e-08,
      "loss": 0.0384,
      "step": 214
    },
    {
      "epoch": 19.545454545454547,
      "grad_norm": 2.328125,
      "learning_rate": 4.528077426915412e-08,
      "loss": 0.04,
      "step": 215
    },
    {
      "epoch": 19.636363636363637,
      "grad_norm": 2.171875,
      "learning_rate": 3.1452240480577265e-08,
      "loss": 0.0409,
      "step": 216
    },
    {
      "epoch": 19.727272727272727,
      "grad_norm": 2.21875,
      "learning_rate": 2.013323528115674e-08,
      "loss": 0.0364,
      "step": 217
    },
    {
      "epoch": 19.818181818181817,
      "grad_norm": 1.9765625,
      "learning_rate": 1.1326608169920373e-08,
      "loss": 0.0369,
      "step": 218
    },
    {
      "epoch": 19.90909090909091,
      "grad_norm": 2.078125,
      "learning_rate": 5.034576168149175e-09,
      "loss": 0.0379,
      "step": 219
    },
    {
      "epoch": 20.0,
      "grad_norm": 2.3125,
      "learning_rate": 1.2587232612493172e-09,
      "loss": 0.0373,
      "step": 220
    }
  ],
  "logging_steps": 1,
  "max_steps": 220,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4399636877475840.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
