{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 204,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.03007518796992481,
      "grad_norm": 0.23024515807628632,
      "learning_rate": 0.0,
      "loss": 2.1261,
      "step": 1
    },
    {
      "epoch": 0.06015037593984962,
      "grad_norm": 0.22935515642166138,
      "learning_rate": 1.904761904761905e-06,
      "loss": 1.9786,
      "step": 2
    },
    {
      "epoch": 0.09022556390977443,
      "grad_norm": 0.24641039967536926,
      "learning_rate": 3.80952380952381e-06,
      "loss": 2.098,
      "step": 3
    },
    {
      "epoch": 0.12030075187969924,
      "grad_norm": 0.24074986577033997,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 2.051,
      "step": 4
    },
    {
      "epoch": 0.15037593984962405,
      "grad_norm": 0.27779898047447205,
      "learning_rate": 7.61904761904762e-06,
      "loss": 2.0486,
      "step": 5
    },
    {
      "epoch": 0.18045112781954886,
      "grad_norm": 0.26643824577331543,
      "learning_rate": 9.523809523809525e-06,
      "loss": 1.9813,
      "step": 6
    },
    {
      "epoch": 0.21052631578947367,
      "grad_norm": 0.24267487227916718,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 2.0965,
      "step": 7
    },
    {
      "epoch": 0.24060150375939848,
      "grad_norm": 0.24407711625099182,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 2.0854,
      "step": 8
    },
    {
      "epoch": 0.2706766917293233,
      "grad_norm": 0.24492506682872772,
      "learning_rate": 1.523809523809524e-05,
      "loss": 2.0988,
      "step": 9
    },
    {
      "epoch": 0.3007518796992481,
      "grad_norm": 0.245197594165802,
      "learning_rate": 1.7142857142857142e-05,
      "loss": 2.0335,
      "step": 10
    },
    {
      "epoch": 0.3308270676691729,
      "grad_norm": 0.24696935713291168,
      "learning_rate": 1.904761904761905e-05,
      "loss": 2.0826,
      "step": 11
    },
    {
      "epoch": 0.3609022556390977,
      "grad_norm": 0.27941375970840454,
      "learning_rate": 2.0952380952380954e-05,
      "loss": 2.0673,
      "step": 12
    },
    {
      "epoch": 0.39097744360902253,
      "grad_norm": 0.24493752419948578,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 2.0144,
      "step": 13
    },
    {
      "epoch": 0.42105263157894735,
      "grad_norm": 0.28627315163612366,
      "learning_rate": 2.4761904761904766e-05,
      "loss": 2.027,
      "step": 14
    },
    {
      "epoch": 0.45112781954887216,
      "grad_norm": 29.614683151245117,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 2.047,
      "step": 15
    },
    {
      "epoch": 0.48120300751879697,
      "grad_norm": 0.2889651656150818,
      "learning_rate": 2.8571428571428574e-05,
      "loss": 2.0964,
      "step": 16
    },
    {
      "epoch": 0.5112781954887218,
      "grad_norm": 0.2865060865879059,
      "learning_rate": 3.047619047619048e-05,
      "loss": 2.0625,
      "step": 17
    },
    {
      "epoch": 0.5413533834586466,
      "grad_norm": 0.28928306698799133,
      "learning_rate": 3.2380952380952386e-05,
      "loss": 2.0893,
      "step": 18
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.2901804745197296,
      "learning_rate": 3.4285714285714284e-05,
      "loss": 2.0279,
      "step": 19
    },
    {
      "epoch": 0.6015037593984962,
      "grad_norm": 0.27256226539611816,
      "learning_rate": 3.6190476190476195e-05,
      "loss": 2.0034,
      "step": 20
    },
    {
      "epoch": 0.631578947368421,
      "grad_norm": 1.4073575735092163,
      "learning_rate": 3.80952380952381e-05,
      "loss": 1.9732,
      "step": 21
    },
    {
      "epoch": 0.6616541353383458,
      "grad_norm": 0.2452545166015625,
      "learning_rate": 4e-05,
      "loss": 1.9668,
      "step": 22
    },
    {
      "epoch": 0.6917293233082706,
      "grad_norm": 0.22362020611763,
      "learning_rate": 3.999705295410054e-05,
      "loss": 1.9273,
      "step": 23
    },
    {
      "epoch": 0.7218045112781954,
      "grad_norm": 0.21568191051483154,
      "learning_rate": 3.9988212684910107e-05,
      "loss": 1.9356,
      "step": 24
    },
    {
      "epoch": 0.7518796992481203,
      "grad_norm": 0.19010300934314728,
      "learning_rate": 3.997348179769661e-05,
      "loss": 1.9165,
      "step": 25
    },
    {
      "epoch": 0.7819548872180451,
      "grad_norm": 0.17370393872261047,
      "learning_rate": 3.995286463372013e-05,
      "loss": 1.8356,
      "step": 26
    },
    {
      "epoch": 0.8120300751879699,
      "grad_norm": 0.7359831929206848,
      "learning_rate": 3.9926367268953514e-05,
      "loss": 1.9556,
      "step": 27
    },
    {
      "epoch": 0.8421052631578947,
      "grad_norm": 0.17103920876979828,
      "learning_rate": 3.989399751229179e-05,
      "loss": 1.8567,
      "step": 28
    },
    {
      "epoch": 0.8721804511278195,
      "grad_norm": 0.15046925842761993,
      "learning_rate": 3.98557649032508e-05,
      "loss": 1.8767,
      "step": 29
    },
    {
      "epoch": 0.9022556390977443,
      "grad_norm": 0.15657883882522583,
      "learning_rate": 3.981168070915594e-05,
      "loss": 1.9377,
      "step": 30
    },
    {
      "epoch": 0.9323308270676691,
      "grad_norm": 0.16813614964485168,
      "learning_rate": 3.9761757921821544e-05,
      "loss": 1.896,
      "step": 31
    },
    {
      "epoch": 0.9624060150375939,
      "grad_norm": 0.19126668572425842,
      "learning_rate": 3.970601125372218e-05,
      "loss": 1.9272,
      "step": 32
    },
    {
      "epoch": 0.9924812030075187,
      "grad_norm": 0.17670752108097076,
      "learning_rate": 3.964445713365682e-05,
      "loss": 1.8589,
      "step": 33
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.2763274013996124,
      "learning_rate": 3.957711370190716e-05,
      "loss": 1.9981,
      "step": 34
    },
    {
      "epoch": 1.0300751879699248,
      "grad_norm": 0.17309126257896423,
      "learning_rate": 3.950400080489165e-05,
      "loss": 1.8569,
      "step": 35
    },
    {
      "epoch": 1.0601503759398496,
      "grad_norm": 0.18310560286045074,
      "learning_rate": 3.942513998931663e-05,
      "loss": 1.8125,
      "step": 36
    },
    {
      "epoch": 1.0902255639097744,
      "grad_norm": 0.16071730852127075,
      "learning_rate": 3.934055449582641e-05,
      "loss": 1.8002,
      "step": 37
    },
    {
      "epoch": 1.1203007518796992,
      "grad_norm": 0.17591558396816254,
      "learning_rate": 3.925026925215417e-05,
      "loss": 1.863,
      "step": 38
    },
    {
      "epoch": 1.150375939849624,
      "grad_norm": 0.16546405851840973,
      "learning_rate": 3.915431086577561e-05,
      "loss": 1.8379,
      "step": 39
    },
    {
      "epoch": 1.1804511278195489,
      "grad_norm": 0.17860829830169678,
      "learning_rate": 3.9052707616067654e-05,
      "loss": 1.8171,
      "step": 40
    },
    {
      "epoch": 1.2105263157894737,
      "grad_norm": 0.17189505696296692,
      "learning_rate": 3.894548944597434e-05,
      "loss": 1.8619,
      "step": 41
    },
    {
      "epoch": 1.2406015037593985,
      "grad_norm": 0.1797834187746048,
      "learning_rate": 3.883268795318252e-05,
      "loss": 1.7917,
      "step": 42
    },
    {
      "epoch": 1.2706766917293233,
      "grad_norm": 0.17264071106910706,
      "learning_rate": 3.8714336380809874e-05,
      "loss": 1.786,
      "step": 43
    },
    {
      "epoch": 1.300751879699248,
      "grad_norm": 0.17937740683555603,
      "learning_rate": 3.859046960760801e-05,
      "loss": 1.7456,
      "step": 44
    },
    {
      "epoch": 1.330827067669173,
      "grad_norm": 0.16974470019340515,
      "learning_rate": 3.846112413768353e-05,
      "loss": 1.7939,
      "step": 45
    },
    {
      "epoch": 1.3609022556390977,
      "grad_norm": 0.195951908826828,
      "learning_rate": 3.83263380897401e-05,
      "loss": 1.7904,
      "step": 46
    },
    {
      "epoch": 1.3909774436090225,
      "grad_norm": 0.16179977357387543,
      "learning_rate": 3.818615118584472e-05,
      "loss": 1.7277,
      "step": 47
    },
    {
      "epoch": 1.4210526315789473,
      "grad_norm": 0.1767236441373825,
      "learning_rate": 3.8040604739721415e-05,
      "loss": 1.7696,
      "step": 48
    },
    {
      "epoch": 1.4511278195488722,
      "grad_norm": 0.17641586065292358,
      "learning_rate": 3.7889741644575914e-05,
      "loss": 1.7085,
      "step": 49
    },
    {
      "epoch": 1.481203007518797,
      "grad_norm": 0.18313083052635193,
      "learning_rate": 3.773360636045481e-05,
      "loss": 1.7153,
      "step": 50
    },
    {
      "epoch": 1.5112781954887218,
      "grad_norm": 0.20530681312084198,
      "learning_rate": 3.757224490114297e-05,
      "loss": 1.7115,
      "step": 51
    },
    {
      "epoch": 1.5413533834586466,
      "grad_norm": 0.1822083741426468,
      "learning_rate": 3.740570482060311e-05,
      "loss": 1.7616,
      "step": 52
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.2039535790681839,
      "learning_rate": 3.723403519896136e-05,
      "loss": 1.7721,
      "step": 53
    },
    {
      "epoch": 1.6015037593984962,
      "grad_norm": 0.17683778703212738,
      "learning_rate": 3.70572866280432e-05,
      "loss": 1.7264,
      "step": 54
    },
    {
      "epoch": 1.631578947368421,
      "grad_norm": 0.1847587376832962,
      "learning_rate": 3.6875511196463715e-05,
      "loss": 1.7501,
      "step": 55
    },
    {
      "epoch": 1.6616541353383458,
      "grad_norm": 0.2009415179491043,
      "learning_rate": 3.6688762474276945e-05,
      "loss": 1.6093,
      "step": 56
    },
    {
      "epoch": 1.6917293233082706,
      "grad_norm": 0.18907029926776886,
      "learning_rate": 3.649709549718849e-05,
      "loss": 1.7104,
      "step": 57
    },
    {
      "epoch": 1.7218045112781954,
      "grad_norm": 0.20673207938671112,
      "learning_rate": 3.6300566750336225e-05,
      "loss": 1.7126,
      "step": 58
    },
    {
      "epoch": 1.7518796992481203,
      "grad_norm": 0.1975688636302948,
      "learning_rate": 3.6099234151643924e-05,
      "loss": 1.7354,
      "step": 59
    },
    {
      "epoch": 1.781954887218045,
      "grad_norm": 0.20944947004318237,
      "learning_rate": 3.58931570347525e-05,
      "loss": 1.6012,
      "step": 60
    },
    {
      "epoch": 1.8120300751879699,
      "grad_norm": 0.21804441511631012,
      "learning_rate": 3.568239613153421e-05,
      "loss": 1.671,
      "step": 61
    },
    {
      "epoch": 1.8421052631578947,
      "grad_norm": 0.200750470161438,
      "learning_rate": 3.54670135541946e-05,
      "loss": 1.6678,
      "step": 62
    },
    {
      "epoch": 1.8721804511278195,
      "grad_norm": 0.22711892426013947,
      "learning_rate": 3.5247072776967805e-05,
      "loss": 1.6741,
      "step": 63
    },
    {
      "epoch": 1.9022556390977443,
      "grad_norm": 0.22562743723392487,
      "learning_rate": 3.5022638617410396e-05,
      "loss": 1.5921,
      "step": 64
    },
    {
      "epoch": 1.9323308270676691,
      "grad_norm": 0.21921533346176147,
      "learning_rate": 3.4793777217299346e-05,
      "loss": 1.672,
      "step": 65
    },
    {
      "epoch": 1.962406015037594,
      "grad_norm": 0.26713162660598755,
      "learning_rate": 3.4560556023139695e-05,
      "loss": 1.7664,
      "step": 66
    },
    {
      "epoch": 1.9924812030075187,
      "grad_norm": 0.22409798204898834,
      "learning_rate": 3.432304376628787e-05,
      "loss": 1.6884,
      "step": 67
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.4433588683605194,
      "learning_rate": 3.4081310442696114e-05,
      "loss": 1.5945,
      "step": 68
    },
    {
      "epoch": 2.030075187969925,
      "grad_norm": 0.23757079243659973,
      "learning_rate": 3.3835427292284445e-05,
      "loss": 1.578,
      "step": 69
    },
    {
      "epoch": 2.0601503759398496,
      "grad_norm": 0.23832441866397858,
      "learning_rate": 3.358546677794586e-05,
      "loss": 1.5991,
      "step": 70
    },
    {
      "epoch": 2.090225563909774,
      "grad_norm": 0.25009238719940186,
      "learning_rate": 3.333150256419127e-05,
      "loss": 1.489,
      "step": 71
    },
    {
      "epoch": 2.1203007518796992,
      "grad_norm": 0.2602270841598511,
      "learning_rate": 3.307360949544012e-05,
      "loss": 1.5562,
      "step": 72
    },
    {
      "epoch": 2.1503759398496243,
      "grad_norm": 0.25766247510910034,
      "learning_rate": 3.281186357396351e-05,
      "loss": 1.465,
      "step": 73
    },
    {
      "epoch": 2.180451127819549,
      "grad_norm": 0.24606026709079742,
      "learning_rate": 3.2546341937485884e-05,
      "loss": 1.5488,
      "step": 74
    },
    {
      "epoch": 2.2105263157894735,
      "grad_norm": 0.2696640193462372,
      "learning_rate": 3.227712283645224e-05,
      "loss": 1.6102,
      "step": 75
    },
    {
      "epoch": 2.2406015037593985,
      "grad_norm": 0.26335659623146057,
      "learning_rate": 3.200428561096737e-05,
      "loss": 1.4817,
      "step": 76
    },
    {
      "epoch": 2.2706766917293235,
      "grad_norm": 0.27291667461395264,
      "learning_rate": 3.172791066741392e-05,
      "loss": 1.5672,
      "step": 77
    },
    {
      "epoch": 2.300751879699248,
      "grad_norm": 0.29788729548454285,
      "learning_rate": 3.14480794547563e-05,
      "loss": 1.5629,
      "step": 78
    },
    {
      "epoch": 2.3308270676691727,
      "grad_norm": 0.27992692589759827,
      "learning_rate": 3.1164874440537295e-05,
      "loss": 1.4308,
      "step": 79
    },
    {
      "epoch": 2.3609022556390977,
      "grad_norm": 0.28521066904067993,
      "learning_rate": 3.0878379086574494e-05,
      "loss": 1.5381,
      "step": 80
    },
    {
      "epoch": 2.3909774436090228,
      "grad_norm": 0.29366210103034973,
      "learning_rate": 3.05886778243637e-05,
      "loss": 1.5106,
      "step": 81
    },
    {
      "epoch": 2.4210526315789473,
      "grad_norm": 0.3425496518611908,
      "learning_rate": 3.0295856030196618e-05,
      "loss": 1.488,
      "step": 82
    },
    {
      "epoch": 2.451127819548872,
      "grad_norm": 0.28813666105270386,
      "learning_rate": 3.0000000000000004e-05,
      "loss": 1.5057,
      "step": 83
    },
    {
      "epoch": 2.481203007518797,
      "grad_norm": 0.4015097916126251,
      "learning_rate": 2.9701196923903927e-05,
      "loss": 1.4047,
      "step": 84
    },
    {
      "epoch": 2.511278195488722,
      "grad_norm": 0.34074661135673523,
      "learning_rate": 2.9399534860546404e-05,
      "loss": 1.4468,
      "step": 85
    },
    {
      "epoch": 2.5413533834586466,
      "grad_norm": 0.3219844102859497,
      "learning_rate": 2.909510271112212e-05,
      "loss": 1.4744,
      "step": 86
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 6.3723907470703125,
      "learning_rate": 2.878799019318283e-05,
      "loss": 1.4539,
      "step": 87
    },
    {
      "epoch": 2.601503759398496,
      "grad_norm": 0.38432741165161133,
      "learning_rate": 2.847828781419722e-05,
      "loss": 1.5171,
      "step": 88
    },
    {
      "epoch": 2.6315789473684212,
      "grad_norm": 0.3461434841156006,
      "learning_rate": 2.816608684487787e-05,
      "loss": 1.4718,
      "step": 89
    },
    {
      "epoch": 2.661654135338346,
      "grad_norm": 0.3877663016319275,
      "learning_rate": 2.7851479292283442e-05,
      "loss": 1.2694,
      "step": 90
    },
    {
      "epoch": 2.6917293233082704,
      "grad_norm": 0.37383413314819336,
      "learning_rate": 2.7534557872703705e-05,
      "loss": 1.4269,
      "step": 91
    },
    {
      "epoch": 2.7218045112781954,
      "grad_norm": 0.3399207890033722,
      "learning_rate": 2.721541598433567e-05,
      "loss": 1.3803,
      "step": 92
    },
    {
      "epoch": 2.7518796992481205,
      "grad_norm": 0.35617363452911377,
      "learning_rate": 2.6894147679758678e-05,
      "loss": 1.3021,
      "step": 93
    },
    {
      "epoch": 2.781954887218045,
      "grad_norm": 0.35070860385894775,
      "learning_rate": 2.6570847638216698e-05,
      "loss": 1.3278,
      "step": 94
    },
    {
      "epoch": 2.8120300751879697,
      "grad_norm": 0.4430675208568573,
      "learning_rate": 2.6245611137715897e-05,
      "loss": 1.3461,
      "step": 95
    },
    {
      "epoch": 2.8421052631578947,
      "grad_norm": 0.511020302772522,
      "learning_rate": 2.5918534026945787e-05,
      "loss": 1.195,
      "step": 96
    },
    {
      "epoch": 2.8721804511278197,
      "grad_norm": 0.4618777632713318,
      "learning_rate": 2.558971269703219e-05,
      "loss": 1.455,
      "step": 97
    },
    {
      "epoch": 2.9022556390977443,
      "grad_norm": 0.5225962996482849,
      "learning_rate": 2.5259244053130295e-05,
      "loss": 1.3495,
      "step": 98
    },
    {
      "epoch": 2.932330827067669,
      "grad_norm": 0.5128374099731445,
      "learning_rate": 2.4927225485866297e-05,
      "loss": 1.3579,
      "step": 99
    },
    {
      "epoch": 2.962406015037594,
      "grad_norm": 0.4274022877216339,
      "learning_rate": 2.4593754842635917e-05,
      "loss": 1.2443,
      "step": 100
    },
    {
      "epoch": 2.992481203007519,
      "grad_norm": 0.45486995577812195,
      "learning_rate": 2.4258930398768317e-05,
      "loss": 1.4695,
      "step": 101
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.7558174729347229,
      "learning_rate": 2.392285082856394e-05,
      "loss": 1.2024,
      "step": 102
    },
    {
      "epoch": 3.030075187969925,
      "grad_norm": 0.47704750299453735,
      "learning_rate": 2.3585615176214716e-05,
      "loss": 1.2468,
      "step": 103
    },
    {
      "epoch": 3.0601503759398496,
      "grad_norm": 0.4901094436645508,
      "learning_rate": 2.3247322826615276e-05,
      "loss": 1.1186,
      "step": 104
    },
    {
      "epoch": 3.090225563909774,
      "grad_norm": 0.48490896821022034,
      "learning_rate": 2.29080734760738e-05,
      "loss": 1.2531,
      "step": 105
    },
    {
      "epoch": 3.1203007518796992,
      "grad_norm": 0.7305996417999268,
      "learning_rate": 2.2567967102931025e-05,
      "loss": 1.079,
      "step": 106
    },
    {
      "epoch": 3.1503759398496243,
      "grad_norm": 0.4562229514122009,
      "learning_rate": 2.2227103938096176e-05,
      "loss": 1.1721,
      "step": 107
    },
    {
      "epoch": 3.180451127819549,
      "grad_norm": 0.5878682732582092,
      "learning_rate": 2.188558443550849e-05,
      "loss": 1.1663,
      "step": 108
    },
    {
      "epoch": 3.2105263157894735,
      "grad_norm": 0.665214478969574,
      "learning_rate": 2.1543509242532932e-05,
      "loss": 1.2787,
      "step": 109
    },
    {
      "epoch": 3.2406015037593985,
      "grad_norm": 0.6100378036499023,
      "learning_rate": 2.120097917029897e-05,
      "loss": 1.3514,
      "step": 110
    },
    {
      "epoch": 3.2706766917293235,
      "grad_norm": 0.5832256078720093,
      "learning_rate": 2.0858095163991094e-05,
      "loss": 1.2836,
      "step": 111
    },
    {
      "epoch": 3.300751879699248,
      "grad_norm": 0.5527430176734924,
      "learning_rate": 2.0514958273099778e-05,
      "loss": 0.9463,
      "step": 112
    },
    {
      "epoch": 3.3308270676691727,
      "grad_norm": 0.5037208795547485,
      "learning_rate": 2.0171669621641743e-05,
      "loss": 1.2953,
      "step": 113
    },
    {
      "epoch": 3.3609022556390977,
      "grad_norm": 0.49634280800819397,
      "learning_rate": 1.9828330378358264e-05,
      "loss": 1.1189,
      "step": 114
    },
    {
      "epoch": 3.3909774436090228,
      "grad_norm": 0.5217916965484619,
      "learning_rate": 1.9485041726900232e-05,
      "loss": 1.2426,
      "step": 115
    },
    {
      "epoch": 3.4210526315789473,
      "grad_norm": 0.4887547194957733,
      "learning_rate": 1.914190483600891e-05,
      "loss": 1.2249,
      "step": 116
    },
    {
      "epoch": 3.451127819548872,
      "grad_norm": 0.5625555515289307,
      "learning_rate": 1.8799020829701036e-05,
      "loss": 1.0883,
      "step": 117
    },
    {
      "epoch": 3.481203007518797,
      "grad_norm": 0.6052926778793335,
      "learning_rate": 1.8456490757467075e-05,
      "loss": 1.1607,
      "step": 118
    },
    {
      "epoch": 3.511278195488722,
      "grad_norm": 0.5754396915435791,
      "learning_rate": 1.8114415564491513e-05,
      "loss": 1.0081,
      "step": 119
    },
    {
      "epoch": 3.5413533834586466,
      "grad_norm": 0.5823931097984314,
      "learning_rate": 1.7772896061903824e-05,
      "loss": 1.0815,
      "step": 120
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 0.5422841310501099,
      "learning_rate": 1.743203289706898e-05,
      "loss": 1.2131,
      "step": 121
    },
    {
      "epoch": 3.601503759398496,
      "grad_norm": 0.7090235948562622,
      "learning_rate": 1.7091926523926205e-05,
      "loss": 1.2663,
      "step": 122
    },
    {
      "epoch": 3.6315789473684212,
      "grad_norm": 0.7250402569770813,
      "learning_rate": 1.6752677173384734e-05,
      "loss": 1.1378,
      "step": 123
    },
    {
      "epoch": 3.661654135338346,
      "grad_norm": 0.6691647171974182,
      "learning_rate": 1.641438482378529e-05,
      "loss": 1.1309,
      "step": 124
    },
    {
      "epoch": 3.6917293233082704,
      "grad_norm": 0.6205020546913147,
      "learning_rate": 1.6077149171436063e-05,
      "loss": 1.0189,
      "step": 125
    },
    {
      "epoch": 3.7218045112781954,
      "grad_norm": 0.6518645882606506,
      "learning_rate": 1.574106960123169e-05,
      "loss": 1.0281,
      "step": 126
    },
    {
      "epoch": 3.7518796992481205,
      "grad_norm": 0.5995050668716431,
      "learning_rate": 1.5406245157364093e-05,
      "loss": 1.0518,
      "step": 127
    },
    {
      "epoch": 3.781954887218045,
      "grad_norm": 0.62736976146698,
      "learning_rate": 1.5072774514133708e-05,
      "loss": 1.0577,
      "step": 128
    },
    {
      "epoch": 3.8120300751879697,
      "grad_norm": 0.5828613638877869,
      "learning_rate": 1.4740755946869708e-05,
      "loss": 1.0202,
      "step": 129
    },
    {
      "epoch": 3.8421052631578947,
      "grad_norm": 0.6569452881813049,
      "learning_rate": 1.4410287302967813e-05,
      "loss": 1.1437,
      "step": 130
    },
    {
      "epoch": 3.8721804511278197,
      "grad_norm": 0.6809309124946594,
      "learning_rate": 1.4081465973054216e-05,
      "loss": 1.0661,
      "step": 131
    },
    {
      "epoch": 3.9022556390977443,
      "grad_norm": 0.6816602349281311,
      "learning_rate": 1.375438886228411e-05,
      "loss": 0.9038,
      "step": 132
    },
    {
      "epoch": 3.932330827067669,
      "grad_norm": 0.7054687142372131,
      "learning_rate": 1.3429152361783307e-05,
      "loss": 1.1276,
      "step": 133
    },
    {
      "epoch": 3.962406015037594,
      "grad_norm": 0.6493988037109375,
      "learning_rate": 1.3105852320241326e-05,
      "loss": 0.9507,
      "step": 134
    },
    {
      "epoch": 3.992481203007519,
      "grad_norm": 31028.556640625,
      "learning_rate": 1.2784584015664337e-05,
      "loss": 1.0118,
      "step": 135
    },
    {
      "epoch": 4.0,
      "grad_norm": 1.1575344800949097,
      "learning_rate": 1.2465442127296297e-05,
      "loss": 1.135,
      "step": 136
    },
    {
      "epoch": 4.030075187969925,
      "grad_norm": 0.7147042751312256,
      "learning_rate": 1.2148520707716567e-05,
      "loss": 0.899,
      "step": 137
    },
    {
      "epoch": 4.06015037593985,
      "grad_norm": 0.664226233959198,
      "learning_rate": 1.1833913155122132e-05,
      "loss": 0.9625,
      "step": 138
    },
    {
      "epoch": 4.090225563909774,
      "grad_norm": 0.6920687556266785,
      "learning_rate": 1.1521712185802789e-05,
      "loss": 0.9981,
      "step": 139
    },
    {
      "epoch": 4.120300751879699,
      "grad_norm": 0.7415268421173096,
      "learning_rate": 1.1212009806817163e-05,
      "loss": 0.8717,
      "step": 140
    },
    {
      "epoch": 4.150375939849624,
      "grad_norm": 0.7083598971366882,
      "learning_rate": 1.0904897288877891e-05,
      "loss": 1.075,
      "step": 141
    },
    {
      "epoch": 4.180451127819548,
      "grad_norm": 0.6882477402687073,
      "learning_rate": 1.060046513945361e-05,
      "loss": 0.8811,
      "step": 142
    },
    {
      "epoch": 4.2105263157894735,
      "grad_norm": 0.7212458252906799,
      "learning_rate": 1.029880307609608e-05,
      "loss": 0.8068,
      "step": 143
    },
    {
      "epoch": 4.2406015037593985,
      "grad_norm": 0.7541677951812744,
      "learning_rate": 1.0000000000000006e-05,
      "loss": 0.8738,
      "step": 144
    },
    {
      "epoch": 4.2706766917293235,
      "grad_norm": 0.7313429117202759,
      "learning_rate": 9.704143969803392e-06,
      "loss": 0.8444,
      "step": 145
    },
    {
      "epoch": 4.3007518796992485,
      "grad_norm": 0.8523767590522766,
      "learning_rate": 9.411322175636298e-06,
      "loss": 1.0045,
      "step": 146
    },
    {
      "epoch": 4.330827067669173,
      "grad_norm": 0.746304988861084,
      "learning_rate": 9.121620913425508e-06,
      "loss": 0.7183,
      "step": 147
    },
    {
      "epoch": 4.360902255639098,
      "grad_norm": 0.7015994787216187,
      "learning_rate": 8.83512555946271e-06,
      "loss": 0.9322,
      "step": 148
    },
    {
      "epoch": 4.390977443609023,
      "grad_norm": 0.7493524551391602,
      "learning_rate": 8.551920545243704e-06,
      "loss": 0.8225,
      "step": 149
    },
    {
      "epoch": 4.421052631578947,
      "grad_norm": 0.8095531463623047,
      "learning_rate": 8.272089332586089e-06,
      "loss": 0.822,
      "step": 150
    },
    {
      "epoch": 4.451127819548872,
      "grad_norm": 0.6827863454818726,
      "learning_rate": 7.995714389032638e-06,
      "loss": 0.9187,
      "step": 151
    },
    {
      "epoch": 4.481203007518797,
      "grad_norm": 0.984308123588562,
      "learning_rate": 7.72287716354776e-06,
      "loss": 0.9278,
      "step": 152
    },
    {
      "epoch": 4.511278195488722,
      "grad_norm": 0.7921786904335022,
      "learning_rate": 7.4536580625141244e-06,
      "loss": 0.908,
      "step": 153
    },
    {
      "epoch": 4.541353383458647,
      "grad_norm": 0.9149224162101746,
      "learning_rate": 7.188136426036498e-06,
      "loss": 0.7481,
      "step": 154
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 0.8064824938774109,
      "learning_rate": 6.926390504559879e-06,
      "loss": 0.831,
      "step": 155
    },
    {
      "epoch": 4.601503759398496,
      "grad_norm": 0.8960024118423462,
      "learning_rate": 6.668497435808736e-06,
      "loss": 0.9631,
      "step": 156
    },
    {
      "epoch": 4.631578947368421,
      "grad_norm": 0.7341713905334473,
      "learning_rate": 6.414533222054138e-06,
      "loss": 0.9825,
      "step": 157
    },
    {
      "epoch": 4.661654135338345,
      "grad_norm": 0.7203328609466553,
      "learning_rate": 6.164572707715564e-06,
      "loss": 0.9864,
      "step": 158
    },
    {
      "epoch": 4.69172932330827,
      "grad_norm": 0.7450446486473083,
      "learning_rate": 5.918689557303885e-06,
      "loss": 0.8927,
      "step": 159
    },
    {
      "epoch": 4.7218045112781954,
      "grad_norm": 0.7680813074111938,
      "learning_rate": 5.676956233712139e-06,
      "loss": 0.7971,
      "step": 160
    },
    {
      "epoch": 4.7518796992481205,
      "grad_norm": 0.8773247599601746,
      "learning_rate": 5.439443976860306e-06,
      "loss": 0.848,
      "step": 161
    },
    {
      "epoch": 4.7819548872180455,
      "grad_norm": 0.8050733804702759,
      "learning_rate": 5.206222782700667e-06,
      "loss": 0.7329,
      "step": 162
    },
    {
      "epoch": 4.81203007518797,
      "grad_norm": 0.8220205903053284,
      "learning_rate": 4.977361382589607e-06,
      "loss": 1.1128,
      "step": 163
    },
    {
      "epoch": 4.842105263157895,
      "grad_norm": 0.7838015556335449,
      "learning_rate": 4.752927223032196e-06,
      "loss": 0.9387,
      "step": 164
    },
    {
      "epoch": 4.87218045112782,
      "grad_norm": 0.7622078657150269,
      "learning_rate": 4.532986445805405e-06,
      "loss": 0.715,
      "step": 165
    },
    {
      "epoch": 4.902255639097744,
      "grad_norm": 0.8675037026405334,
      "learning_rate": 4.317603868465794e-06,
      "loss": 0.8184,
      "step": 166
    },
    {
      "epoch": 4.932330827067669,
      "grad_norm": 0.9312249422073364,
      "learning_rate": 4.106842965247497e-06,
      "loss": 0.8153,
      "step": 167
    },
    {
      "epoch": 4.962406015037594,
      "grad_norm": 0.9473294019699097,
      "learning_rate": 3.900765848356083e-06,
      "loss": 0.808,
      "step": 168
    },
    {
      "epoch": 4.992481203007519,
      "grad_norm": 0.8373932838439941,
      "learning_rate": 3.699433249663775e-06,
      "loss": 1.0729,
      "step": 169
    },
    {
      "epoch": 5.0,
      "grad_norm": 1.9762171506881714,
      "learning_rate": 3.5029045028115105e-06,
      "loss": 1.2172,
      "step": 170
    },
    {
      "epoch": 5.030075187969925,
      "grad_norm": 0.7809961438179016,
      "learning_rate": 3.3112375257230547e-06,
      "loss": 0.6588,
      "step": 171
    },
    {
      "epoch": 5.06015037593985,
      "grad_norm": 0.7792893648147583,
      "learning_rate": 3.1244888035362875e-06,
      "loss": 0.7569,
      "step": 172
    },
    {
      "epoch": 5.090225563909774,
      "grad_norm": 0.8018438816070557,
      "learning_rate": 2.942713371956809e-06,
      "loss": 0.8599,
      "step": 173
    },
    {
      "epoch": 5.120300751879699,
      "grad_norm": 0.8261454105377197,
      "learning_rate": 2.7659648010386365e-06,
      "loss": 0.8318,
      "step": 174
    },
    {
      "epoch": 5.150375939849624,
      "grad_norm": 13709134.0,
      "learning_rate": 2.594295179396895e-06,
      "loss": 0.6625,
      "step": 175
    },
    {
      "epoch": 5.180451127819548,
      "grad_norm": 0.8715937733650208,
      "learning_rate": 2.4277550988570362e-06,
      "loss": 0.9783,
      "step": 176
    },
    {
      "epoch": 5.2105263157894735,
      "grad_norm": 0.9480197429656982,
      "learning_rate": 2.266393639545197e-06,
      "loss": 0.5929,
      "step": 177
    },
    {
      "epoch": 5.2406015037593985,
      "grad_norm": 0.757579505443573,
      "learning_rate": 2.110258355424093e-06,
      "loss": 0.7485,
      "step": 178
    },
    {
      "epoch": 5.2706766917293235,
      "grad_norm": 0.8263081908226013,
      "learning_rate": 1.959395260278587e-06,
      "loss": 0.7486,
      "step": 179
    },
    {
      "epoch": 5.3007518796992485,
      "grad_norm": 0.7079708576202393,
      "learning_rate": 1.8138488141552856e-06,
      "loss": 0.8322,
      "step": 180
    },
    {
      "epoch": 5.330827067669173,
      "grad_norm": 0.8971163630485535,
      "learning_rate": 1.6736619102599073e-06,
      "loss": 0.9039,
      "step": 181
    },
    {
      "epoch": 5.360902255639098,
      "grad_norm": 0.8186072111129761,
      "learning_rate": 1.5388758623164802e-06,
      "loss": 0.6865,
      "step": 182
    },
    {
      "epoch": 5.390977443609023,
      "grad_norm": 0.9238935708999634,
      "learning_rate": 1.4095303923919956e-06,
      "loss": 0.8387,
      "step": 183
    },
    {
      "epoch": 5.421052631578947,
      "grad_norm": 0.8123499751091003,
      "learning_rate": 1.2856636191901296e-06,
      "loss": 0.8051,
      "step": 184
    },
    {
      "epoch": 5.451127819548872,
      "grad_norm": 0.7885828018188477,
      "learning_rate": 1.1673120468174837e-06,
      "loss": 0.7797,
      "step": 185
    },
    {
      "epoch": 5.481203007518797,
      "grad_norm": 1612420736.0,
      "learning_rate": 1.0545105540256628e-06,
      "loss": 0.7176,
      "step": 186
    },
    {
      "epoch": 5.511278195488722,
      "grad_norm": 0.7323185205459595,
      "learning_rate": 9.4729238393235e-07,
      "loss": 0.734,
      "step": 187
    },
    {
      "epoch": 5.541353383458647,
      "grad_norm": 0.9873113632202148,
      "learning_rate": 8.456891342243945e-07,
      "loss": 0.7763,
      "step": 188
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 0.8716198205947876,
      "learning_rate": 7.497307478458382e-07,
      "loss": 0.8738,
      "step": 189
    },
    {
      "epoch": 5.601503759398496,
      "grad_norm": 0.780148983001709,
      "learning_rate": 6.594455041735925e-07,
      "loss": 0.9157,
      "step": 190
    },
    {
      "epoch": 5.631578947368421,
      "grad_norm": 3.6460886001586914,
      "learning_rate": 5.748600106833735e-07,
      "loss": 0.8161,
      "step": 191
    },
    {
      "epoch": 5.661654135338345,
      "grad_norm": 0.7689335346221924,
      "learning_rate": 4.959991951083498e-07,
      "loss": 0.7985,
      "step": 192
    },
    {
      "epoch": 5.69172932330827,
      "grad_norm": 0.8584907054901123,
      "learning_rate": 4.228862980928439e-07,
      "loss": 0.9453,
      "step": 193
    },
    {
      "epoch": 5.7218045112781954,
      "grad_norm": 0.7982515096664429,
      "learning_rate": 3.5554286634318814e-07,
      "loss": 0.7745,
      "step": 194
    },
    {
      "epoch": 5.7518796992481205,
      "grad_norm": 0.9169142246246338,
      "learning_rate": 2.9398874627782014e-07,
      "loss": 0.8355,
      "step": 195
    },
    {
      "epoch": 5.7819548872180455,
      "grad_norm": 0.805745005607605,
      "learning_rate": 2.382420781784589e-07,
      "loss": 0.703,
      "step": 196
    },
    {
      "epoch": 5.81203007518797,
      "grad_norm": 0.8508309125900269,
      "learning_rate": 1.8831929084406119e-07,
      "loss": 0.758,
      "step": 197
    },
    {
      "epoch": 5.842105263157895,
      "grad_norm": 0.7641580700874329,
      "learning_rate": 1.44235096749199e-07,
      "loss": 0.7897,
      "step": 198
    },
    {
      "epoch": 5.87218045112782,
      "grad_norm": 2.7669153213500977,
      "learning_rate": 1.0600248770821886e-07,
      "loss": 0.8463,
      "step": 199
    },
    {
      "epoch": 5.902255639097744,
      "grad_norm": 1.101312518119812,
      "learning_rate": 7.363273104648904e-08,
      "loss": 0.9146,
      "step": 200
    },
    {
      "epoch": 5.932330827067669,
      "grad_norm": 0.8066525459289551,
      "learning_rate": 4.713536627987347e-08,
      "loss": 0.8187,
      "step": 201
    },
    {
      "epoch": 5.962406015037594,
      "grad_norm": 0.8795727491378784,
      "learning_rate": 2.651820230338942e-08,
      "loss": 0.9285,
      "step": 202
    },
    {
      "epoch": 5.992481203007519,
      "grad_norm": 0.8723462820053101,
      "learning_rate": 1.1787315089895057e-08,
      "loss": 0.7438,
      "step": 203
    },
    {
      "epoch": 6.0,
      "grad_norm": 1.4992603063583374,
      "learning_rate": 2.94704589946182e-09,
      "loss": 0.8194,
      "step": 204
    }
  ],
  "logging_steps": 1,
  "max_steps": 204,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.407670105024512e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
