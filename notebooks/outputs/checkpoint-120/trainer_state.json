{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 30.0,
  "eval_steps": 500,
  "global_step": 120,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.2722186744213104,
      "learning_rate": 0.0,
      "loss": 2.0905,
      "step": 1
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 0.24350979924201965,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 2.1668,
      "step": 2
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 0.2139352411031723,
      "learning_rate": 6.666666666666667e-06,
      "loss": 2.1515,
      "step": 3
    },
    {
      "epoch": 1.0,
      "grad_norm": 0.24478594958782196,
      "learning_rate": 1e-05,
      "loss": 2.1313,
      "step": 4
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 0.22520826756954193,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 2.1476,
      "step": 5
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 0.25610023736953735,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 2.1056,
      "step": 6
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 0.24585360288619995,
      "learning_rate": 2e-05,
      "loss": 2.1803,
      "step": 7
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.2605369985103607,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 2.0368,
      "step": 8
    },
    {
      "epoch": 2.2857142857142856,
      "grad_norm": 0.24045169353485107,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 2.1192,
      "step": 9
    },
    {
      "epoch": 2.571428571428571,
      "grad_norm": 0.25805801153182983,
      "learning_rate": 3.0000000000000004e-05,
      "loss": 2.0874,
      "step": 10
    },
    {
      "epoch": 2.857142857142857,
      "grad_norm": 0.2569262385368347,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.1392,
      "step": 11
    },
    {
      "epoch": 3.0,
      "grad_norm": 0.36310291290283203,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 2.1394,
      "step": 12
    },
    {
      "epoch": 3.2857142857142856,
      "grad_norm": 0.2693454921245575,
      "learning_rate": 4e-05,
      "loss": 2.0747,
      "step": 13
    },
    {
      "epoch": 3.571428571428571,
      "grad_norm": 0.2601358890533447,
      "learning_rate": 3.9991539001644015e-05,
      "loss": 2.0847,
      "step": 14
    },
    {
      "epoch": 3.857142857142857,
      "grad_norm": 0.28435030579566956,
      "learning_rate": 3.996616316542537e-05,
      "loss": 2.1146,
      "step": 15
    },
    {
      "epoch": 4.0,
      "grad_norm": 0.2822946608066559,
      "learning_rate": 3.9923893961834914e-05,
      "loss": 1.95,
      "step": 16
    },
    {
      "epoch": 4.285714285714286,
      "grad_norm": 0.2284625768661499,
      "learning_rate": 3.9864767154838864e-05,
      "loss": 2.0022,
      "step": 17
    },
    {
      "epoch": 4.571428571428571,
      "grad_norm": 0.22021852433681488,
      "learning_rate": 3.978883277161889e-05,
      "loss": 2.0016,
      "step": 18
    },
    {
      "epoch": 4.857142857142857,
      "grad_norm": 0.2152148187160492,
      "learning_rate": 3.9696155060244166e-05,
      "loss": 2.0337,
      "step": 19
    },
    {
      "epoch": 5.0,
      "grad_norm": 0.24354241788387299,
      "learning_rate": 3.958681243531103e-05,
      "loss": 2.033,
      "step": 20
    },
    {
      "epoch": 5.285714285714286,
      "grad_norm": 0.22730909287929535,
      "learning_rate": 3.946089741159648e-05,
      "loss": 2.0027,
      "step": 21
    },
    {
      "epoch": 5.571428571428571,
      "grad_norm": 0.159841388463974,
      "learning_rate": 3.931851652578137e-05,
      "loss": 1.9817,
      "step": 22
    },
    {
      "epoch": 5.857142857142857,
      "grad_norm": 0.15008823573589325,
      "learning_rate": 3.915979024630978e-05,
      "loss": 1.9334,
      "step": 23
    },
    {
      "epoch": 6.0,
      "grad_norm": 0.1913265585899353,
      "learning_rate": 3.898485287146068e-05,
      "loss": 1.9076,
      "step": 24
    },
    {
      "epoch": 6.285714285714286,
      "grad_norm": 0.16748373210430145,
      "learning_rate": 3.879385241571817e-05,
      "loss": 1.966,
      "step": 25
    },
    {
      "epoch": 6.571428571428571,
      "grad_norm": 0.1454886943101883,
      "learning_rate": 3.858695048453645e-05,
      "loss": 1.9011,
      "step": 26
    },
    {
      "epoch": 6.857142857142857,
      "grad_norm": 0.14523260295391083,
      "learning_rate": 3.8364322137605484e-05,
      "loss": 1.912,
      "step": 27
    },
    {
      "epoch": 7.0,
      "grad_norm": 0.22568923234939575,
      "learning_rate": 3.812615574073301e-05,
      "loss": 1.9582,
      "step": 28
    },
    {
      "epoch": 7.285714285714286,
      "grad_norm": 0.15547852218151093,
      "learning_rate": 3.787265280646825e-05,
      "loss": 1.8747,
      "step": 29
    },
    {
      "epoch": 7.571428571428571,
      "grad_norm": 0.19364196062088013,
      "learning_rate": 3.760402782360222e-05,
      "loss": 1.9131,
      "step": 30
    },
    {
      "epoch": 7.857142857142857,
      "grad_norm": 0.17836031317710876,
      "learning_rate": 3.732050807568878e-05,
      "loss": 1.9474,
      "step": 31
    },
    {
      "epoch": 8.0,
      "grad_norm": 0.21324102580547333,
      "learning_rate": 3.702233344873999e-05,
      "loss": 1.7738,
      "step": 32
    },
    {
      "epoch": 8.285714285714286,
      "grad_norm": 1.7953053712844849,
      "learning_rate": 3.6709756228258735e-05,
      "loss": 1.8226,
      "step": 33
    },
    {
      "epoch": 8.571428571428571,
      "grad_norm": 0.1726701706647873,
      "learning_rate": 3.638304088577984e-05,
      "loss": 1.9512,
      "step": 34
    },
    {
      "epoch": 8.857142857142858,
      "grad_norm": 0.20987875759601593,
      "learning_rate": 3.604246385510088e-05,
      "loss": 1.8157,
      "step": 35
    },
    {
      "epoch": 9.0,
      "grad_norm": 0.2642809748649597,
      "learning_rate": 3.568831329839152e-05,
      "loss": 1.8664,
      "step": 36
    },
    {
      "epoch": 9.285714285714286,
      "grad_norm": 0.1825598180294037,
      "learning_rate": 3.532088886237956e-05,
      "loss": 1.8893,
      "step": 37
    },
    {
      "epoch": 9.571428571428571,
      "grad_norm": 0.17758558690547943,
      "learning_rate": 3.4940501424819927e-05,
      "loss": 1.8399,
      "step": 38
    },
    {
      "epoch": 9.857142857142858,
      "grad_norm": 0.2524544298648834,
      "learning_rate": 3.4547472831460976e-05,
      "loss": 1.786,
      "step": 39
    },
    {
      "epoch": 10.0,
      "grad_norm": 0.24239744246006012,
      "learning_rate": 3.4142135623730954e-05,
      "loss": 1.8029,
      "step": 40
    },
    {
      "epoch": 10.285714285714286,
      "grad_norm": 0.17356736958026886,
      "learning_rate": 3.372483275737468e-05,
      "loss": 1.803,
      "step": 41
    },
    {
      "epoch": 10.571428571428571,
      "grad_norm": 0.1632257103919983,
      "learning_rate": 3.3295917312278754e-05,
      "loss": 1.8066,
      "step": 42
    },
    {
      "epoch": 10.857142857142858,
      "grad_norm": 0.17215800285339355,
      "learning_rate": 3.285575219373079e-05,
      "loss": 1.7767,
      "step": 43
    },
    {
      "epoch": 11.0,
      "grad_norm": 0.2530139982700348,
      "learning_rate": 3.2404709825365204e-05,
      "loss": 1.86,
      "step": 44
    },
    {
      "epoch": 11.285714285714286,
      "grad_norm": 0.18683508038520813,
      "learning_rate": 3.194317183405573e-05,
      "loss": 1.7493,
      "step": 45
    },
    {
      "epoch": 11.571428571428571,
      "grad_norm": 0.16911326348781586,
      "learning_rate": 3.147152872702092e-05,
      "loss": 1.7375,
      "step": 46
    },
    {
      "epoch": 11.857142857142858,
      "grad_norm": 0.1726166158914566,
      "learning_rate": 3.0990179561416124e-05,
      "loss": 1.8214,
      "step": 47
    },
    {
      "epoch": 12.0,
      "grad_norm": 0.2696176767349243,
      "learning_rate": 3.0499531606691204e-05,
      "loss": 1.7848,
      "step": 48
    },
    {
      "epoch": 12.285714285714286,
      "grad_norm": 0.18334731459617615,
      "learning_rate": 3.0000000000000004e-05,
      "loss": 1.8457,
      "step": 49
    },
    {
      "epoch": 12.571428571428571,
      "grad_norm": 0.1817849576473236,
      "learning_rate": 2.9492007394952812e-05,
      "loss": 1.6704,
      "step": 50
    },
    {
      "epoch": 12.857142857142858,
      "grad_norm": 0.18873949348926544,
      "learning_rate": 2.897598360400925e-05,
      "loss": 1.7295,
      "step": 51
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.27682310342788696,
      "learning_rate": 2.8452365234813992e-05,
      "loss": 1.6685,
      "step": 52
    },
    {
      "epoch": 13.285714285714286,
      "grad_norm": 0.18503113090991974,
      "learning_rate": 2.792159532078314e-05,
      "loss": 1.7901,
      "step": 53
    },
    {
      "epoch": 13.571428571428571,
      "grad_norm": 0.20243984460830688,
      "learning_rate": 2.738412294625369e-05,
      "loss": 1.6,
      "step": 54
    },
    {
      "epoch": 13.857142857142858,
      "grad_norm": 0.19544723629951477,
      "learning_rate": 2.684040286651338e-05,
      "loss": 1.7087,
      "step": 55
    },
    {
      "epoch": 14.0,
      "grad_norm": 0.31785309314727783,
      "learning_rate": 2.6290895123032277e-05,
      "loss": 1.7532,
      "step": 56
    },
    {
      "epoch": 14.285714285714286,
      "grad_norm": 0.19610697031021118,
      "learning_rate": 2.5736064654221808e-05,
      "loss": 1.6583,
      "step": 57
    },
    {
      "epoch": 14.571428571428571,
      "grad_norm": 0.22724249958992004,
      "learning_rate": 2.5176380902050418e-05,
      "loss": 1.6173,
      "step": 58
    },
    {
      "epoch": 14.857142857142858,
      "grad_norm": 0.2131870687007904,
      "learning_rate": 2.4612317414848804e-05,
      "loss": 1.738,
      "step": 59
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.35334184765815735,
      "learning_rate": 2.4044351446640763e-05,
      "loss": 1.6963,
      "step": 60
    },
    {
      "epoch": 15.285714285714286,
      "grad_norm": 0.21726953983306885,
      "learning_rate": 2.3472963553338614e-05,
      "loss": 1.6135,
      "step": 61
    },
    {
      "epoch": 15.571428571428571,
      "grad_norm": 0.2134408801794052,
      "learning_rate": 2.2898637186144935e-05,
      "loss": 1.69,
      "step": 62
    },
    {
      "epoch": 15.857142857142858,
      "grad_norm": 0.22032959759235382,
      "learning_rate": 2.2321858282504606e-05,
      "loss": 1.6271,
      "step": 63
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.33996185660362244,
      "learning_rate": 2.174311485495317e-05,
      "loss": 1.6162,
      "step": 64
    },
    {
      "epoch": 16.285714285714285,
      "grad_norm": 0.23913495242595673,
      "learning_rate": 2.1162896578209517e-05,
      "loss": 1.5635,
      "step": 65
    },
    {
      "epoch": 16.571428571428573,
      "grad_norm": 0.22686775028705597,
      "learning_rate": 2.058169437486223e-05,
      "loss": 1.6615,
      "step": 66
    },
    {
      "epoch": 16.857142857142858,
      "grad_norm": 0.23010967671871185,
      "learning_rate": 2e-05,
      "loss": 1.6008,
      "step": 67
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.3815014064311981,
      "learning_rate": 1.9418305625137773e-05,
      "loss": 1.6054,
      "step": 68
    },
    {
      "epoch": 17.285714285714285,
      "grad_norm": 0.24735064804553986,
      "learning_rate": 1.8837103421790486e-05,
      "loss": 1.538,
      "step": 69
    },
    {
      "epoch": 17.571428571428573,
      "grad_norm": 0.25458893179893494,
      "learning_rate": 1.8256885145046837e-05,
      "loss": 1.5087,
      "step": 70
    },
    {
      "epoch": 17.857142857142858,
      "grad_norm": 0.2469550520181656,
      "learning_rate": 1.7678141717495394e-05,
      "loss": 1.6745,
      "step": 71
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.44688934087753296,
      "learning_rate": 1.7101362813855068e-05,
      "loss": 1.5931,
      "step": 72
    },
    {
      "epoch": 18.285714285714285,
      "grad_norm": 0.25702500343322754,
      "learning_rate": 1.6527036446661396e-05,
      "loss": 1.5702,
      "step": 73
    },
    {
      "epoch": 18.571428571428573,
      "grad_norm": 0.27046993374824524,
      "learning_rate": 1.5955648553359247e-05,
      "loss": 1.5323,
      "step": 74
    },
    {
      "epoch": 18.857142857142858,
      "grad_norm": 0.2680087089538574,
      "learning_rate": 1.53876825851512e-05,
      "loss": 1.5363,
      "step": 75
    },
    {
      "epoch": 19.0,
      "grad_norm": 0.4412308633327484,
      "learning_rate": 1.4823619097949584e-05,
      "loss": 1.5417,
      "step": 76
    },
    {
      "epoch": 19.285714285714285,
      "grad_norm": 0.27527427673339844,
      "learning_rate": 1.4263935345778202e-05,
      "loss": 1.5202,
      "step": 77
    },
    {
      "epoch": 19.571428571428573,
      "grad_norm": 0.2859900891780853,
      "learning_rate": 1.3709104876967732e-05,
      "loss": 1.5177,
      "step": 78
    },
    {
      "epoch": 19.857142857142858,
      "grad_norm": 0.29097989201545715,
      "learning_rate": 1.3159597133486628e-05,
      "loss": 1.5128,
      "step": 79
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.41428956389427185,
      "learning_rate": 1.2615877053746315e-05,
      "loss": 1.5419,
      "step": 80
    },
    {
      "epoch": 20.285714285714285,
      "grad_norm": 0.29226741194725037,
      "learning_rate": 1.2078404679216864e-05,
      "loss": 1.53,
      "step": 81
    },
    {
      "epoch": 20.571428571428573,
      "grad_norm": 0.3081391751766205,
      "learning_rate": 1.1547634765186016e-05,
      "loss": 1.4147,
      "step": 82
    },
    {
      "epoch": 20.857142857142858,
      "grad_norm": 0.3006667494773865,
      "learning_rate": 1.1024016395990758e-05,
      "loss": 1.55,
      "step": 83
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.4453766644001007,
      "learning_rate": 1.0507992605047193e-05,
      "loss": 1.455,
      "step": 84
    },
    {
      "epoch": 21.285714285714285,
      "grad_norm": 0.3151605725288391,
      "learning_rate": 1.0000000000000006e-05,
      "loss": 1.4644,
      "step": 85
    },
    {
      "epoch": 21.571428571428573,
      "grad_norm": 0.30127525329589844,
      "learning_rate": 9.5004683933088e-06,
      "loss": 1.4196,
      "step": 86
    },
    {
      "epoch": 21.857142857142858,
      "grad_norm": 0.33408525586128235,
      "learning_rate": 9.009820438583881e-06,
      "loss": 1.5402,
      "step": 87
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.5425130128860474,
      "learning_rate": 8.528471272979083e-06,
      "loss": 1.4433,
      "step": 88
    },
    {
      "epoch": 22.285714285714285,
      "grad_norm": 0.31517449021339417,
      "learning_rate": 8.056828165944282e-06,
      "loss": 1.4682,
      "step": 89
    },
    {
      "epoch": 22.571428571428573,
      "grad_norm": 0.32753220200538635,
      "learning_rate": 7.595290174634802e-06,
      "loss": 1.5285,
      "step": 90
    },
    {
      "epoch": 22.857142857142858,
      "grad_norm": 0.3326451778411865,
      "learning_rate": 7.1442478062692135e-06,
      "loss": 1.3446,
      "step": 91
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.5536580085754395,
      "learning_rate": 6.704082687721243e-06,
      "loss": 1.4832,
      "step": 92
    },
    {
      "epoch": 23.285714285714285,
      "grad_norm": 0.34203383326530457,
      "learning_rate": 6.275167242625331e-06,
      "loss": 1.3906,
      "step": 93
    },
    {
      "epoch": 23.571428571428573,
      "grad_norm": 0.43412521481513977,
      "learning_rate": 5.857864376269051e-06,
      "loss": 1.4283,
      "step": 94
    },
    {
      "epoch": 23.857142857142858,
      "grad_norm": 0.3274471163749695,
      "learning_rate": 5.452527168539026e-06,
      "loss": 1.4552,
      "step": 95
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.5360777974128723,
      "learning_rate": 5.059498575180084e-06,
      "loss": 1.5164,
      "step": 96
    },
    {
      "epoch": 24.285714285714285,
      "grad_norm": 0.34923192858695984,
      "learning_rate": 4.679111137620442e-06,
      "loss": 1.3942,
      "step": 97
    },
    {
      "epoch": 24.571428571428573,
      "grad_norm": 0.34301698207855225,
      "learning_rate": 4.311686701608486e-06,
      "loss": 1.4054,
      "step": 98
    },
    {
      "epoch": 24.857142857142858,
      "grad_norm": 0.35068824887275696,
      "learning_rate": 3.957536144899123e-06,
      "loss": 1.4602,
      "step": 99
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.5569413304328918,
      "learning_rate": 3.616959114220162e-06,
      "loss": 1.4422,
      "step": 100
    },
    {
      "epoch": 25.285714285714285,
      "grad_norm": 0.35546624660491943,
      "learning_rate": 3.290243771741275e-06,
      "loss": 1.375,
      "step": 101
    },
    {
      "epoch": 25.571428571428573,
      "grad_norm": 0.33432501554489136,
      "learning_rate": 2.9776665512600054e-06,
      "loss": 1.4949,
      "step": 102
    },
    {
      "epoch": 25.857142857142858,
      "grad_norm": 0.3561772108078003,
      "learning_rate": 2.679491924311226e-06,
      "loss": 1.4246,
      "step": 103
    },
    {
      "epoch": 26.0,
      "grad_norm": 0.6296412348747253,
      "learning_rate": 2.3959721763977805e-06,
      "loss": 1.2628,
      "step": 104
    },
    {
      "epoch": 26.285714285714285,
      "grad_norm": 0.3578568696975708,
      "learning_rate": 2.127347193531757e-06,
      "loss": 1.3105,
      "step": 105
    },
    {
      "epoch": 26.571428571428573,
      "grad_norm": 0.3822251260280609,
      "learning_rate": 1.8738442592670014e-06,
      "loss": 1.5463,
      "step": 106
    },
    {
      "epoch": 26.857142857142858,
      "grad_norm": 0.3727916479110718,
      "learning_rate": 1.6356778623945223e-06,
      "loss": 1.3507,
      "step": 107
    },
    {
      "epoch": 27.0,
      "grad_norm": 0.5030006766319275,
      "learning_rate": 1.4130495154635494e-06,
      "loss": 1.4385,
      "step": 108
    },
    {
      "epoch": 27.285714285714285,
      "grad_norm": 0.342174768447876,
      "learning_rate": 1.2061475842818337e-06,
      "loss": 1.3727,
      "step": 109
    },
    {
      "epoch": 27.571428571428573,
      "grad_norm": 0.37733379006385803,
      "learning_rate": 1.0151471285393223e-06,
      "loss": 1.3331,
      "step": 110
    },
    {
      "epoch": 27.857142857142858,
      "grad_norm": 0.3786414563655853,
      "learning_rate": 8.402097536902221e-07,
      "loss": 1.4481,
      "step": 111
    },
    {
      "epoch": 28.0,
      "grad_norm": 0.6220476627349854,
      "learning_rate": 6.814834742186361e-07,
      "loss": 1.5477,
      "step": 112
    },
    {
      "epoch": 28.285714285714285,
      "grad_norm": 0.3596637547016144,
      "learning_rate": 5.391025884035239e-07,
      "loss": 1.3878,
      "step": 113
    },
    {
      "epoch": 28.571428571428573,
      "grad_norm": 0.3870263993740082,
      "learning_rate": 4.1318756468897047e-07,
      "loss": 1.372,
      "step": 114
    },
    {
      "epoch": 28.857142857142858,
      "grad_norm": 0.38118433952331543,
      "learning_rate": 3.038449397558396e-07,
      "loss": 1.4189,
      "step": 115
    },
    {
      "epoch": 29.0,
      "grad_norm": 0.5353668928146362,
      "learning_rate": 2.111672283811106e-07,
      "loss": 1.4565,
      "step": 116
    },
    {
      "epoch": 29.285714285714285,
      "grad_norm": 0.4068208634853363,
      "learning_rate": 1.3523284516113955e-07,
      "loss": 1.2883,
      "step": 117
    },
    {
      "epoch": 29.571428571428573,
      "grad_norm": 0.37339162826538086,
      "learning_rate": 7.61060381650891e-08,
      "loss": 1.4634,
      "step": 118
    },
    {
      "epoch": 29.857142857142858,
      "grad_norm": 0.360376238822937,
      "learning_rate": 3.383683457463649e-08,
      "loss": 1.4305,
      "step": 119
    },
    {
      "epoch": 30.0,
      "grad_norm": 0.5176244378089905,
      "learning_rate": 8.460998355988014e-09,
      "loss": 1.4472,
      "step": 120
    }
  ],
  "logging_steps": 1,
  "max_steps": 120,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 30,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.77526732185641e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
