{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/envs/llm-medical-bias/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/homebrew/anaconda3/envs/llm-medical-bias/lib/python3.8/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/opt/homebrew/anaconda3/envs/llm-medical-bias/lib/python3.8/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <CAF361F5-1CAC-3EBE-9FC4-4B823D275CAA> /opt/homebrew/anaconda3/envs/llm-medical-bias/lib/python3.8/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/homebrew/anaconda3/envs/llm-medical-bias/lib/python3.8/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/homebrew/anaconda3/envs/llm-medical-bias/lib/python3.8/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/opt/homebrew/anaconda3/envs/llm-medical-bias/lib/python3.8/lib-dynload/../../libjpeg.9.dylib' (no such file), '/opt/homebrew/anaconda3/envs/llm-medical-bias/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), os.pardir)))\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "ner_pipeline = pipeline(\"ner\", model='alvaroalon2/biobert_diseases_ner', device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"alvaroalon2/biobert_diseases_ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity': 'B-DISEASE',\n",
       "  'score': 0.9856285,\n",
       "  'index': 1,\n",
       "  'word': 'card',\n",
       "  'start': 0,\n",
       "  'end': 4},\n",
       " {'entity': 'I-DISEASE',\n",
       "  'score': 0.98711467,\n",
       "  'index': 2,\n",
       "  'word': '##iovascular',\n",
       "  'start': 4,\n",
       "  'end': 14},\n",
       " {'entity': 'I-DISEASE',\n",
       "  'score': 0.99860257,\n",
       "  'index': 3,\n",
       "  'word': 'is',\n",
       "  'start': 15,\n",
       "  'end': 17},\n",
       " {'entity': 'I-DISEASE',\n",
       "  'score': 0.9709489,\n",
       "  'index': 4,\n",
       "  'word': 'dead',\n",
       "  'start': 18,\n",
       "  'end': 22}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_pipeline(\"cardiovascular is dead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# Function to extract disease entities from a single question\n",
    "def retrieve_prefix(idx, word_ids, ner_output):\n",
    "    index = idx\n",
    "    prefix = ner_output[index]['word'].replace(\"##\",\"\")\n",
    "    #print(ner_output[index]['word'],word_ids[index+1])\n",
    "    #print(ner_output[index-1]['word'],word_ids[index])\n",
    "    while word_ids[index+1] == word_ids[index]:\n",
    "        #print(prefix)\n",
    "        prefix = ner_output[index-1]['word'].replace(\"##\",\"\") + prefix\n",
    "        index = index - 1\n",
    "    return prefix\n",
    "\n",
    "def retrieve_suffix(idx, word_ids, ner_output):\n",
    "    index = idx\n",
    "    suffix = ner_output[index]['word'].replace(\"##\",\"\")\n",
    "    #print(ner_output[index]['word'],word_ids[index+1])\n",
    "    #print(ner_output[index-1]['word'],word_ids[index])\n",
    "    while word_ids[index+1] == word_ids[index+2]:\n",
    "        #print(prefix)\n",
    "        suffix = suffix + ner_output[index+1]['word'].replace(\"##\",\"\") \n",
    "        index = index + 1\n",
    "    return suffix\n",
    "\n",
    "def there_is_same_word_after(idx, word_ids):\n",
    "    return word_ids[idx+1] == word_ids[idx+2]\n",
    "\n",
    "def chunk_text(text, tokenizer, max_length=512, stride=128):\n",
    "    tokens = tokenizer(text, add_special_tokens=False)\n",
    "    token_chunks = []\n",
    "    for i in range(0, len(tokens['input_ids']), max_length - stride):\n",
    "        chunk = tokens['input_ids'][i:i + max_length]\n",
    "        token_chunks.append(tokenizer.decode(chunk, skip_special_tokens=True))\n",
    "    return token_chunks\n",
    "\n",
    "# Function to extract disease entities from a single question chunk\n",
    "def extract_diseases_from_chunk(text):\n",
    "    word_ids = tokenizer(text).word_ids()\n",
    "    ner_output = ner_pipeline(text)\n",
    "    #print(ner_output)\n",
    "    disease_entities = []\n",
    "    current_entity = \"\"\n",
    "    disease_detected = False\n",
    "\n",
    "    for idx, token in enumerate(ner_output):\n",
    "        # If disease is currently being tracked\n",
    "        if disease_detected:\n",
    "            if token['entity'] == 'B-DISEASE':\n",
    "                if '##' in token['word']:\n",
    "                    current_entity += token['word'][2:]  # Remove the '##' from subwords\n",
    "                else:\n",
    "                    disease_entities.append(current_entity)\n",
    "                    current_entity = token['word']\n",
    "            elif token['entity'] == 'I-DISEASE':\n",
    "                if token['word'].startswith(\"##\"):\n",
    "                    current_entity += token['word'][2:]  # Remove the '##' from subwords\n",
    "                else:\n",
    "                    current_entity += \" \" + token['word']\n",
    "            elif token['entity'] == '0':\n",
    "                if there_is_same_word_after(idx - 1, word_ids):\n",
    "                    disease_entities.append(current_entity + retrieve_suffix(idx, word_ids, ner_output))\n",
    "                else:\n",
    "                    disease_entities.append(current_entity)\n",
    "                current_entity = ''\n",
    "                disease_detected = False\n",
    "            else:\n",
    "                raise Exception(f'Unknown NER Type Detected: {token}')\n",
    "        # If disease is not being tracked\n",
    "        else:\n",
    "            if token['entity'] == 'B-DISEASE':\n",
    "                disease_detected = True\n",
    "                current_entity = token['word']\n",
    "            elif token['entity'] == 'I-DISEASE':\n",
    "                disease_detected = True\n",
    "                current_entity += retrieve_prefix(idx - 1, word_ids, ner_output)\n",
    "                if token['word'].startswith(\"##\"):\n",
    "                    current_entity += token['word'][2:]  # Remove the '##' from subwords\n",
    "                else:\n",
    "                    current_entity += \" \" + token['word']\n",
    "\n",
    "    # Add the last entity if it exists\n",
    "    if current_entity:\n",
    "        disease_entities.append(current_entity)\n",
    "\n",
    "    return disease_entities\n",
    "\n",
    "# Main function to process text and extract disease entities\n",
    "def extract_diseases(text):\n",
    "    chunks = chunk_text(text, tokenizer)\n",
    "    all_diseases = []\n",
    "    for chunk in chunks:\n",
    "        diseases = extract_diseases_from_chunk(chunk)\n",
    "        all_diseases.extend(diseases)\n",
    "    # Deduplicate disease entities while preserving order\n",
    "    return list(dict.fromkeys(all_diseases))\n",
    "\n",
    "# Add disease NER to data\n",
    "def add_disease_ner(data):\n",
    "    for item in data:\n",
    "        question = item.get(\"question\", \"\")\n",
    "        item['disease'] = extract_diseases(question)\n",
    "   \n",
    "input_filepath = \"../\"+utils._DATASETS['MedQA']\n",
    "data = utils.load_dataset(input_filepath, 2000)\n",
    "add_disease_ner(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical Debt -- Converge this with llm-categorize.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New dataset with metadata saved to .././data/medqa/questions/US/4_options/phrases_no_exclude_test_with_category_metadata_disease_ner_two.jsonl\n",
      "Global pools saved to .././data/medqa/questions/US/4_options/global_pools_disease_ner_two.json\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "import time \n",
    "\n",
    "global_pools = {\n",
    "    \"diseases\": set(),\n",
    "    \"question_types\": set(),\n",
    "    \"medical_specialties\": set(),\n",
    "    \"severity_urgency\": set(),\n",
    "    \"patient_demographics\": set()\n",
    "}\n",
    "\n",
    "def save_dataset(data, input_filepath, date):\n",
    "    \"\"\"Save the augmented dataset to a JSONL file with '_with_category_metadata.jsonl' appended to the filename.\"\"\"\n",
    "    base_name = os.path.basename(input_filepath)\n",
    "    dir_name = os.path.dirname(input_filepath)\n",
    "    output_filename = f\"{os.path.splitext(base_name)[0]}_with_category_metadata_{date}.jsonl\"\n",
    "    output_filepath = os.path.join(dir_name, output_filename)\n",
    "    \n",
    "    with open(output_filepath, 'w') as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item) + '\\n')\n",
    "    \n",
    "    print(f\"New dataset with metadata saved to {output_filepath}\")\n",
    "\n",
    "def save_global_pools(input_filepath, date):\n",
    "    \"\"\"Save global pools to a JSON file for record-keeping in the same directory as the dataset.\"\"\"\n",
    "    dir_name = os.path.dirname(input_filepath)\n",
    "    output_filepath = os.path.join(dir_name, f\"global_pools_{date}.json\")\n",
    "    global_pools_serializable = {k: list(v) for k, v in global_pools.items()}\n",
    "    with open(output_filepath, 'w') as f:\n",
    "        json.dump(global_pools_serializable, f, indent=2)\n",
    "    print(f\"Global pools saved to {output_filepath}\")\n",
    "    \n",
    "save_dataset(data, input_filepath, 'disease_ner_two')\n",
    "\n",
    "# Save global pools of each metadata category\n",
    "save_global_pools(input_filepath, 'disease_ner_two')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', 'a', 'yo', '[SEP]']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"a yo\").tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " 0,\n",
       " 1,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 29,\n",
       " 29,\n",
       " 30,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " None]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = utils.load_dataset(input_filepath, 10)\n",
    "\n",
    "ner_output = ner_pipeline(data[0]['question'])\n",
    "tokenizer(data[0]['question']).word_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles for 'diabetes': 124113\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_pubmed_article_count(disease_term):\n",
    "    url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi\"\n",
    "    params = {\n",
    "        \"db\": \"pubmed\",\n",
    "        \"term\": disease_term,\n",
    "        \"retmode\": \"json\"\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    return int(data[\"esearchresult\"][\"count\"])\n",
    "\n",
    "# Example usage\n",
    "count = get_pubmed_article_count(\"osteoarthritis\")\n",
    "print(f\"Number of articles for 'diabetes': {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"concepts\": [\"Pelvic inflammatory disease\", \"Sepsis\", \"Disseminated intravascular coagulation\", \"Endotoxin\", \"Lipopolysaccharide\", \"Gram-negative bacteria\"]\\n}'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utils.query_gpt(\"A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower quadrant pain. Her temperature is 39.1°C (102.3°F), pulse is 126/min, respirations are 28/min, and blood pressure is 80/50 mm Hg. There is blood oozing around the site of a peripheral intravenous line. Pelvic examination shows mucopurulent discharge from the cervical os and left adnexal tenderness. Laboratory studies show:\\nPlatelet count 14,200/mm3\\nFibrinogen 83 mg/mL (N = 200–430 mg/dL)\\nD-dimer 965 ng/mL (N < 500 ng/mL)\\nWhen phenol is applied to a sample of the patient's blood at 90°C, a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain is identified. A blood culture is most likely to show which of the following?\\. For the above question, extract the knowledge concepts that this question is trying to test. Give me the biomedical concept names alone. Answer in json format under the key 'concepts'.\", json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"concepts\": [\\n        \"Pelvic inflammatory disease\",\\n        \"Sepsis\",\\n        \"Disseminated intravascular coagulation\",\\n        \"Endotoxin-induced coagulation cascade\",\\n        \"Gram-negative bacterial infection\"\\n    ]\\n}'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_text=\"A 39-year-old woman is brought to the emergency department because of fevers, chills, and left lower quadrant pain. Her temperature is 39.1°C (102.3°F), pulse is 126/min, respirations are 28/min, and blood pressure is 80/50 mm Hg. There is blood oozing around the site of a peripheral intravenous line. Pelvic examination shows mucopurulent discharge from the cervical os and left adnexal tenderness. Laboratory studies show:\\nPlatelet count 14,200/mm3\\nFibrinogen 83 mg/mL (N = 200–430 mg/dL)\\nD-dimer 965 ng/mL (N < 500 ng/mL)\\nWhen phenol is applied to a sample of the patient's blood at 90°C, a phosphorylated N-acetylglucosamine dimer with 6 fatty acids attached to a polysaccharide side chain is identified. A blood culture is most likely to show which of the following?\"\n",
    "prompt = f\"\"\"\n",
    "\n",
    "    {question_text}.\\n For the above question, extract the key knowledge concepts that this question is trying to test. Give me the biomedical concept names alone in their most concise form. Answer in json format under the key 'concepts'.\n",
    "    \"\"\"\n",
    "utils.query_gpt(prompt, json=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated mentions on the web: 4710000\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def get_bing_search_count(query, api_key):\n",
    "    url = \"https://api.bing.microsoft.com/v7.0/search\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": api_key}\n",
    "    params = {\"q\": query}\n",
    "    response = requests.get(url, headers=headers, params=params)\n",
    "    data = response.json()\n",
    "    return data.get(\"webPages\", {}).get(\"totalEstimatedMatches\", 0)\n",
    "\n",
    "api_key = \"28f95a9243864997a126f5fd6a4cdb6d\"\n",
    "count = get_bing_search_count(\"alzheimer's\", api_key)\n",
    "print(f\"Estimated mentions on the web: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-medical-bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
