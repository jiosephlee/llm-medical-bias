{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 25.0,
  "eval_steps": 500,
  "global_step": 300,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.08888888888888889,
      "grad_norm": 0.6757982969284058,
      "learning_rate": 0.0,
      "loss": 2.2995,
      "step": 1
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 0.7267673015594482,
      "learning_rate": 4e-05,
      "loss": 2.3871,
      "step": 2
    },
    {
      "epoch": 0.26666666666666666,
      "grad_norm": 0.6463132500648499,
      "learning_rate": 8e-05,
      "loss": 2.1799,
      "step": 3
    },
    {
      "epoch": 0.35555555555555557,
      "grad_norm": 0.5421069264411926,
      "learning_rate": 0.00012,
      "loss": 2.1992,
      "step": 4
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 0.5031198859214783,
      "learning_rate": 0.00016,
      "loss": 2.0087,
      "step": 5
    },
    {
      "epoch": 0.5333333333333333,
      "grad_norm": 0.5118108987808228,
      "learning_rate": 0.0002,
      "loss": 1.9759,
      "step": 6
    },
    {
      "epoch": 0.6222222222222222,
      "grad_norm": 0.4760119915008545,
      "learning_rate": 0.0001993220338983051,
      "loss": 1.7753,
      "step": 7
    },
    {
      "epoch": 0.7111111111111111,
      "grad_norm": 0.5729647278785706,
      "learning_rate": 0.00019864406779661017,
      "loss": 1.7727,
      "step": 8
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.7170876264572144,
      "learning_rate": 0.00019796610169491526,
      "loss": 1.6607,
      "step": 9
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 0.6656357049942017,
      "learning_rate": 0.00019728813559322035,
      "loss": 1.539,
      "step": 10
    },
    {
      "epoch": 0.9777777777777777,
      "grad_norm": 0.5850748419761658,
      "learning_rate": 0.00019661016949152545,
      "loss": 1.391,
      "step": 11
    },
    {
      "epoch": 1.0,
      "grad_norm": 1.2283494472503662,
      "learning_rate": 0.0001959322033898305,
      "loss": 1.7039,
      "step": 12
    },
    {
      "epoch": 1.0888888888888888,
      "grad_norm": 0.5108198523521423,
      "learning_rate": 0.0001952542372881356,
      "loss": 1.4127,
      "step": 13
    },
    {
      "epoch": 1.1777777777777778,
      "grad_norm": 0.5074647068977356,
      "learning_rate": 0.0001945762711864407,
      "loss": 1.4171,
      "step": 14
    },
    {
      "epoch": 1.2666666666666666,
      "grad_norm": 0.4776495397090912,
      "learning_rate": 0.0001938983050847458,
      "loss": 1.4192,
      "step": 15
    },
    {
      "epoch": 1.3555555555555556,
      "grad_norm": 0.5017142295837402,
      "learning_rate": 0.00019322033898305085,
      "loss": 1.328,
      "step": 16
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 0.4005048871040344,
      "learning_rate": 0.00019254237288135595,
      "loss": 1.2498,
      "step": 17
    },
    {
      "epoch": 1.5333333333333332,
      "grad_norm": 0.41902121901512146,
      "learning_rate": 0.000191864406779661,
      "loss": 1.2303,
      "step": 18
    },
    {
      "epoch": 1.6222222222222222,
      "grad_norm": 0.44861939549446106,
      "learning_rate": 0.0001911864406779661,
      "loss": 1.1915,
      "step": 19
    },
    {
      "epoch": 1.7111111111111112,
      "grad_norm": 0.5149414539337158,
      "learning_rate": 0.0001905084745762712,
      "loss": 1.2583,
      "step": 20
    },
    {
      "epoch": 1.8,
      "grad_norm": 0.48273685574531555,
      "learning_rate": 0.0001898305084745763,
      "loss": 1.0381,
      "step": 21
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 0.5691235065460205,
      "learning_rate": 0.00018915254237288136,
      "loss": 1.3387,
      "step": 22
    },
    {
      "epoch": 1.9777777777777779,
      "grad_norm": 0.5216394662857056,
      "learning_rate": 0.00018847457627118645,
      "loss": 1.1838,
      "step": 23
    },
    {
      "epoch": 2.0,
      "grad_norm": 1.3014525175094604,
      "learning_rate": 0.00018779661016949151,
      "loss": 1.2867,
      "step": 24
    },
    {
      "epoch": 2.088888888888889,
      "grad_norm": 0.4833652079105377,
      "learning_rate": 0.00018711864406779663,
      "loss": 1.1129,
      "step": 25
    },
    {
      "epoch": 2.1777777777777776,
      "grad_norm": 0.47990867495536804,
      "learning_rate": 0.0001864406779661017,
      "loss": 1.0307,
      "step": 26
    },
    {
      "epoch": 2.2666666666666666,
      "grad_norm": 0.4885942339897156,
      "learning_rate": 0.0001857627118644068,
      "loss": 1.0236,
      "step": 27
    },
    {
      "epoch": 2.3555555555555556,
      "grad_norm": 0.5002354979515076,
      "learning_rate": 0.00018508474576271186,
      "loss": 1.0472,
      "step": 28
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 0.5255188345909119,
      "learning_rate": 0.00018440677966101695,
      "loss": 0.9708,
      "step": 29
    },
    {
      "epoch": 2.533333333333333,
      "grad_norm": 0.5703387260437012,
      "learning_rate": 0.00018372881355932204,
      "loss": 1.0507,
      "step": 30
    },
    {
      "epoch": 2.6222222222222222,
      "grad_norm": 0.6609055995941162,
      "learning_rate": 0.00018305084745762714,
      "loss": 0.9919,
      "step": 31
    },
    {
      "epoch": 2.7111111111111112,
      "grad_norm": 0.5902702212333679,
      "learning_rate": 0.0001823728813559322,
      "loss": 0.96,
      "step": 32
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.7027382850646973,
      "learning_rate": 0.0001816949152542373,
      "loss": 0.9606,
      "step": 33
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 0.8431893587112427,
      "learning_rate": 0.00018101694915254239,
      "loss": 1.0134,
      "step": 34
    },
    {
      "epoch": 2.977777777777778,
      "grad_norm": 0.7839144468307495,
      "learning_rate": 0.00018033898305084748,
      "loss": 0.855,
      "step": 35
    },
    {
      "epoch": 3.0,
      "grad_norm": 2.1312122344970703,
      "learning_rate": 0.00017966101694915257,
      "loss": 0.8198,
      "step": 36
    },
    {
      "epoch": 3.088888888888889,
      "grad_norm": 0.6285037398338318,
      "learning_rate": 0.00017898305084745764,
      "loss": 0.7013,
      "step": 37
    },
    {
      "epoch": 3.1777777777777776,
      "grad_norm": 0.6447928547859192,
      "learning_rate": 0.00017830508474576273,
      "loss": 0.6398,
      "step": 38
    },
    {
      "epoch": 3.2666666666666666,
      "grad_norm": 0.6793172359466553,
      "learning_rate": 0.0001776271186440678,
      "loss": 0.7238,
      "step": 39
    },
    {
      "epoch": 3.3555555555555556,
      "grad_norm": 0.815601646900177,
      "learning_rate": 0.0001769491525423729,
      "loss": 0.7056,
      "step": 40
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 0.9295931458473206,
      "learning_rate": 0.00017627118644067798,
      "loss": 0.7513,
      "step": 41
    },
    {
      "epoch": 3.533333333333333,
      "grad_norm": 1.1022987365722656,
      "learning_rate": 0.00017559322033898307,
      "loss": 0.6624,
      "step": 42
    },
    {
      "epoch": 3.6222222222222222,
      "grad_norm": 0.9604623317718506,
      "learning_rate": 0.00017491525423728814,
      "loss": 0.6632,
      "step": 43
    },
    {
      "epoch": 3.7111111111111112,
      "grad_norm": 1.0477032661437988,
      "learning_rate": 0.00017423728813559323,
      "loss": 0.7315,
      "step": 44
    },
    {
      "epoch": 3.8,
      "grad_norm": 1.0546965599060059,
      "learning_rate": 0.0001735593220338983,
      "loss": 0.6332,
      "step": 45
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 1.0654513835906982,
      "learning_rate": 0.00017288135593220342,
      "loss": 0.716,
      "step": 46
    },
    {
      "epoch": 3.977777777777778,
      "grad_norm": 0.9643936157226562,
      "learning_rate": 0.00017220338983050848,
      "loss": 0.7029,
      "step": 47
    },
    {
      "epoch": 4.0,
      "grad_norm": 2.977640151977539,
      "learning_rate": 0.00017152542372881357,
      "loss": 0.7969,
      "step": 48
    },
    {
      "epoch": 4.088888888888889,
      "grad_norm": 0.9004967212677002,
      "learning_rate": 0.00017084745762711864,
      "loss": 0.5023,
      "step": 49
    },
    {
      "epoch": 4.177777777777778,
      "grad_norm": 0.8766012787818909,
      "learning_rate": 0.00017016949152542373,
      "loss": 0.4519,
      "step": 50
    },
    {
      "epoch": 4.266666666666667,
      "grad_norm": 0.8798707723617554,
      "learning_rate": 0.00016949152542372882,
      "loss": 0.4497,
      "step": 51
    },
    {
      "epoch": 4.355555555555555,
      "grad_norm": 1.0077060461044312,
      "learning_rate": 0.00016881355932203392,
      "loss": 0.3655,
      "step": 52
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 1.4170591831207275,
      "learning_rate": 0.00016813559322033898,
      "loss": 0.4349,
      "step": 53
    },
    {
      "epoch": 4.533333333333333,
      "grad_norm": 1.9420667886734009,
      "learning_rate": 0.00016745762711864408,
      "loss": 0.4165,
      "step": 54
    },
    {
      "epoch": 4.622222222222222,
      "grad_norm": 1.4866597652435303,
      "learning_rate": 0.00016677966101694914,
      "loss": 0.3399,
      "step": 55
    },
    {
      "epoch": 4.711111111111111,
      "grad_norm": 1.6173352003097534,
      "learning_rate": 0.00016610169491525423,
      "loss": 0.421,
      "step": 56
    },
    {
      "epoch": 4.8,
      "grad_norm": 1.757189393043518,
      "learning_rate": 0.00016542372881355933,
      "loss": 0.4014,
      "step": 57
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 1.2457735538482666,
      "learning_rate": 0.00016474576271186442,
      "loss": 0.3857,
      "step": 58
    },
    {
      "epoch": 4.977777777777778,
      "grad_norm": 1.2237805128097534,
      "learning_rate": 0.00016406779661016948,
      "loss": 0.3818,
      "step": 59
    },
    {
      "epoch": 5.0,
      "grad_norm": 2.9001450538635254,
      "learning_rate": 0.00016338983050847458,
      "loss": 0.3628,
      "step": 60
    },
    {
      "epoch": 5.088888888888889,
      "grad_norm": 0.8251526355743408,
      "learning_rate": 0.00016271186440677967,
      "loss": 0.2378,
      "step": 61
    },
    {
      "epoch": 5.177777777777778,
      "grad_norm": 0.9087392091751099,
      "learning_rate": 0.00016203389830508476,
      "loss": 0.3049,
      "step": 62
    },
    {
      "epoch": 5.266666666666667,
      "grad_norm": 0.9009873867034912,
      "learning_rate": 0.00016135593220338985,
      "loss": 0.2279,
      "step": 63
    },
    {
      "epoch": 5.355555555555555,
      "grad_norm": 0.8679332137107849,
      "learning_rate": 0.00016067796610169492,
      "loss": 0.231,
      "step": 64
    },
    {
      "epoch": 5.444444444444445,
      "grad_norm": 1.1585490703582764,
      "learning_rate": 0.00016,
      "loss": 0.2216,
      "step": 65
    },
    {
      "epoch": 5.533333333333333,
      "grad_norm": 0.9840874075889587,
      "learning_rate": 0.00015932203389830508,
      "loss": 0.2041,
      "step": 66
    },
    {
      "epoch": 5.622222222222222,
      "grad_norm": 1.9441245794296265,
      "learning_rate": 0.0001586440677966102,
      "loss": 0.2485,
      "step": 67
    },
    {
      "epoch": 5.711111111111111,
      "grad_norm": 1.7558430433273315,
      "learning_rate": 0.00015796610169491526,
      "loss": 0.2188,
      "step": 68
    },
    {
      "epoch": 5.8,
      "grad_norm": 1.883197546005249,
      "learning_rate": 0.00015728813559322036,
      "loss": 0.2231,
      "step": 69
    },
    {
      "epoch": 5.888888888888889,
      "grad_norm": 1.7467247247695923,
      "learning_rate": 0.00015661016949152542,
      "loss": 0.2261,
      "step": 70
    },
    {
      "epoch": 5.977777777777778,
      "grad_norm": 1.6975867748260498,
      "learning_rate": 0.00015593220338983051,
      "loss": 0.2264,
      "step": 71
    },
    {
      "epoch": 6.0,
      "grad_norm": 4.988358497619629,
      "learning_rate": 0.0001552542372881356,
      "loss": 0.3022,
      "step": 72
    },
    {
      "epoch": 6.088888888888889,
      "grad_norm": 1.4610756635665894,
      "learning_rate": 0.0001545762711864407,
      "loss": 0.1219,
      "step": 73
    },
    {
      "epoch": 6.177777777777778,
      "grad_norm": 0.8774946928024292,
      "learning_rate": 0.00015389830508474577,
      "loss": 0.1185,
      "step": 74
    },
    {
      "epoch": 6.266666666666667,
      "grad_norm": 0.9796251654624939,
      "learning_rate": 0.00015322033898305086,
      "loss": 0.1489,
      "step": 75
    },
    {
      "epoch": 6.355555555555555,
      "grad_norm": 0.8613181114196777,
      "learning_rate": 0.00015254237288135592,
      "loss": 0.1349,
      "step": 76
    },
    {
      "epoch": 6.444444444444445,
      "grad_norm": 1.0925425291061401,
      "learning_rate": 0.00015186440677966102,
      "loss": 0.1459,
      "step": 77
    },
    {
      "epoch": 6.533333333333333,
      "grad_norm": 1.0510001182556152,
      "learning_rate": 0.0001511864406779661,
      "loss": 0.1211,
      "step": 78
    },
    {
      "epoch": 6.622222222222222,
      "grad_norm": 1.0105773210525513,
      "learning_rate": 0.0001505084745762712,
      "loss": 0.1221,
      "step": 79
    },
    {
      "epoch": 6.711111111111111,
      "grad_norm": 0.8793664574623108,
      "learning_rate": 0.00014983050847457627,
      "loss": 0.1194,
      "step": 80
    },
    {
      "epoch": 6.8,
      "grad_norm": 1.1658486127853394,
      "learning_rate": 0.00014915254237288136,
      "loss": 0.1497,
      "step": 81
    },
    {
      "epoch": 6.888888888888889,
      "grad_norm": 0.9926239848136902,
      "learning_rate": 0.00014847457627118645,
      "loss": 0.1212,
      "step": 82
    },
    {
      "epoch": 6.977777777777778,
      "grad_norm": 1.1621392965316772,
      "learning_rate": 0.00014779661016949154,
      "loss": 0.1307,
      "step": 83
    },
    {
      "epoch": 7.0,
      "grad_norm": 2.06717586517334,
      "learning_rate": 0.0001471186440677966,
      "loss": 0.1821,
      "step": 84
    },
    {
      "epoch": 7.088888888888889,
      "grad_norm": 0.5970471501350403,
      "learning_rate": 0.0001464406779661017,
      "loss": 0.0709,
      "step": 85
    },
    {
      "epoch": 7.177777777777778,
      "grad_norm": 0.69284588098526,
      "learning_rate": 0.00014576271186440677,
      "loss": 0.0666,
      "step": 86
    },
    {
      "epoch": 7.266666666666667,
      "grad_norm": 0.990512490272522,
      "learning_rate": 0.00014508474576271186,
      "loss": 0.1034,
      "step": 87
    },
    {
      "epoch": 7.355555555555555,
      "grad_norm": 0.8830046057701111,
      "learning_rate": 0.00014440677966101695,
      "loss": 0.0904,
      "step": 88
    },
    {
      "epoch": 7.444444444444445,
      "grad_norm": 0.7880664467811584,
      "learning_rate": 0.00014372881355932205,
      "loss": 0.0981,
      "step": 89
    },
    {
      "epoch": 7.533333333333333,
      "grad_norm": 1.0464447736740112,
      "learning_rate": 0.00014305084745762714,
      "loss": 0.1101,
      "step": 90
    },
    {
      "epoch": 7.622222222222222,
      "grad_norm": 0.9028717875480652,
      "learning_rate": 0.0001423728813559322,
      "loss": 0.1035,
      "step": 91
    },
    {
      "epoch": 7.711111111111111,
      "grad_norm": 1.0663833618164062,
      "learning_rate": 0.0001416949152542373,
      "loss": 0.1106,
      "step": 92
    },
    {
      "epoch": 7.8,
      "grad_norm": 0.6203152537345886,
      "learning_rate": 0.0001410169491525424,
      "loss": 0.0716,
      "step": 93
    },
    {
      "epoch": 7.888888888888889,
      "grad_norm": 0.6851803660392761,
      "learning_rate": 0.00014033898305084748,
      "loss": 0.0891,
      "step": 94
    },
    {
      "epoch": 7.977777777777778,
      "grad_norm": 0.8854228854179382,
      "learning_rate": 0.00013966101694915255,
      "loss": 0.1063,
      "step": 95
    },
    {
      "epoch": 8.0,
      "grad_norm": 4.5211687088012695,
      "learning_rate": 0.00013898305084745764,
      "loss": 0.0749,
      "step": 96
    },
    {
      "epoch": 8.088888888888889,
      "grad_norm": 0.29733073711395264,
      "learning_rate": 0.0001383050847457627,
      "loss": 0.0509,
      "step": 97
    },
    {
      "epoch": 8.177777777777777,
      "grad_norm": 0.5492799878120422,
      "learning_rate": 0.0001376271186440678,
      "loss": 0.0443,
      "step": 98
    },
    {
      "epoch": 8.266666666666667,
      "grad_norm": 0.763515830039978,
      "learning_rate": 0.0001369491525423729,
      "loss": 0.058,
      "step": 99
    },
    {
      "epoch": 8.355555555555556,
      "grad_norm": 0.7991799116134644,
      "learning_rate": 0.00013627118644067798,
      "loss": 0.0728,
      "step": 100
    },
    {
      "epoch": 8.444444444444445,
      "grad_norm": 0.6988253593444824,
      "learning_rate": 0.00013559322033898305,
      "loss": 0.0718,
      "step": 101
    },
    {
      "epoch": 8.533333333333333,
      "grad_norm": 0.47820478677749634,
      "learning_rate": 0.00013491525423728814,
      "loss": 0.0486,
      "step": 102
    },
    {
      "epoch": 8.622222222222222,
      "grad_norm": 0.6336512565612793,
      "learning_rate": 0.0001342372881355932,
      "loss": 0.0589,
      "step": 103
    },
    {
      "epoch": 8.71111111111111,
      "grad_norm": 0.7146102786064148,
      "learning_rate": 0.00013355932203389833,
      "loss": 0.0487,
      "step": 104
    },
    {
      "epoch": 8.8,
      "grad_norm": 0.6905972957611084,
      "learning_rate": 0.0001328813559322034,
      "loss": 0.0664,
      "step": 105
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 0.6727564334869385,
      "learning_rate": 0.00013220338983050849,
      "loss": 0.0572,
      "step": 106
    },
    {
      "epoch": 8.977777777777778,
      "grad_norm": 0.9029513597488403,
      "learning_rate": 0.00013152542372881355,
      "loss": 0.0626,
      "step": 107
    },
    {
      "epoch": 9.0,
      "grad_norm": 4.298257350921631,
      "learning_rate": 0.00013084745762711864,
      "loss": 0.0699,
      "step": 108
    },
    {
      "epoch": 9.088888888888889,
      "grad_norm": 0.8948656320571899,
      "learning_rate": 0.00013016949152542374,
      "loss": 0.0595,
      "step": 109
    },
    {
      "epoch": 9.177777777777777,
      "grad_norm": 1.0073668956756592,
      "learning_rate": 0.00012949152542372883,
      "loss": 0.0438,
      "step": 110
    },
    {
      "epoch": 9.266666666666667,
      "grad_norm": 1.03274667263031,
      "learning_rate": 0.0001288135593220339,
      "loss": 0.0501,
      "step": 111
    },
    {
      "epoch": 9.355555555555556,
      "grad_norm": 0.3501851260662079,
      "learning_rate": 0.000128135593220339,
      "loss": 0.0385,
      "step": 112
    },
    {
      "epoch": 9.444444444444445,
      "grad_norm": 0.8087487816810608,
      "learning_rate": 0.00012745762711864405,
      "loss": 0.0394,
      "step": 113
    },
    {
      "epoch": 9.533333333333333,
      "grad_norm": 0.5911656022071838,
      "learning_rate": 0.00012677966101694917,
      "loss": 0.0386,
      "step": 114
    },
    {
      "epoch": 9.622222222222222,
      "grad_norm": 0.42520013451576233,
      "learning_rate": 0.00012610169491525426,
      "loss": 0.0395,
      "step": 115
    },
    {
      "epoch": 9.71111111111111,
      "grad_norm": 0.7183883786201477,
      "learning_rate": 0.00012542372881355933,
      "loss": 0.0547,
      "step": 116
    },
    {
      "epoch": 9.8,
      "grad_norm": 2.868537425994873,
      "learning_rate": 0.00012474576271186442,
      "loss": 0.0544,
      "step": 117
    },
    {
      "epoch": 9.88888888888889,
      "grad_norm": 0.7065227031707764,
      "learning_rate": 0.0001240677966101695,
      "loss": 0.0447,
      "step": 118
    },
    {
      "epoch": 9.977777777777778,
      "grad_norm": 0.8363568782806396,
      "learning_rate": 0.00012338983050847458,
      "loss": 0.0555,
      "step": 119
    },
    {
      "epoch": 10.0,
      "grad_norm": 3.219634771347046,
      "learning_rate": 0.00012271186440677967,
      "loss": 0.0449,
      "step": 120
    },
    {
      "epoch": 10.088888888888889,
      "grad_norm": 0.20970721542835236,
      "learning_rate": 0.00012203389830508477,
      "loss": 0.0253,
      "step": 121
    },
    {
      "epoch": 10.177777777777777,
      "grad_norm": 0.28879085183143616,
      "learning_rate": 0.00012135593220338983,
      "loss": 0.0348,
      "step": 122
    },
    {
      "epoch": 10.266666666666667,
      "grad_norm": 0.4728679060935974,
      "learning_rate": 0.00012067796610169492,
      "loss": 0.0289,
      "step": 123
    },
    {
      "epoch": 10.355555555555556,
      "grad_norm": 0.7810710668563843,
      "learning_rate": 0.00012,
      "loss": 0.041,
      "step": 124
    },
    {
      "epoch": 10.444444444444445,
      "grad_norm": 0.6439559459686279,
      "learning_rate": 0.0001193220338983051,
      "loss": 0.0484,
      "step": 125
    },
    {
      "epoch": 10.533333333333333,
      "grad_norm": 0.8282895684242249,
      "learning_rate": 0.00011864406779661017,
      "loss": 0.0605,
      "step": 126
    },
    {
      "epoch": 10.622222222222222,
      "grad_norm": 0.6594696044921875,
      "learning_rate": 0.00011796610169491527,
      "loss": 0.0383,
      "step": 127
    },
    {
      "epoch": 10.71111111111111,
      "grad_norm": 0.9622729420661926,
      "learning_rate": 0.00011728813559322033,
      "loss": 0.0575,
      "step": 128
    },
    {
      "epoch": 10.8,
      "grad_norm": 0.778003454208374,
      "learning_rate": 0.00011661016949152544,
      "loss": 0.0463,
      "step": 129
    },
    {
      "epoch": 10.88888888888889,
      "grad_norm": 0.914759635925293,
      "learning_rate": 0.0001159322033898305,
      "loss": 0.0577,
      "step": 130
    },
    {
      "epoch": 10.977777777777778,
      "grad_norm": 0.9677173495292664,
      "learning_rate": 0.0001152542372881356,
      "loss": 0.0517,
      "step": 131
    },
    {
      "epoch": 11.0,
      "grad_norm": 4.1350579261779785,
      "learning_rate": 0.00011457627118644068,
      "loss": 0.0496,
      "step": 132
    },
    {
      "epoch": 11.088888888888889,
      "grad_norm": 0.2895621359348297,
      "learning_rate": 0.00011389830508474577,
      "loss": 0.0286,
      "step": 133
    },
    {
      "epoch": 11.177777777777777,
      "grad_norm": 0.24128133058547974,
      "learning_rate": 0.00011322033898305085,
      "loss": 0.0298,
      "step": 134
    },
    {
      "epoch": 11.266666666666667,
      "grad_norm": 0.5088903903961182,
      "learning_rate": 0.00011254237288135594,
      "loss": 0.039,
      "step": 135
    },
    {
      "epoch": 11.355555555555556,
      "grad_norm": 0.6663327813148499,
      "learning_rate": 0.00011186440677966102,
      "loss": 0.0351,
      "step": 136
    },
    {
      "epoch": 11.444444444444445,
      "grad_norm": 0.4357492923736572,
      "learning_rate": 0.00011118644067796611,
      "loss": 0.0366,
      "step": 137
    },
    {
      "epoch": 11.533333333333333,
      "grad_norm": 0.5780312418937683,
      "learning_rate": 0.00011050847457627118,
      "loss": 0.0387,
      "step": 138
    },
    {
      "epoch": 11.622222222222222,
      "grad_norm": 0.9568763375282288,
      "learning_rate": 0.00010983050847457627,
      "loss": 0.0343,
      "step": 139
    },
    {
      "epoch": 11.71111111111111,
      "grad_norm": 0.43803533911705017,
      "learning_rate": 0.00010915254237288135,
      "loss": 0.0345,
      "step": 140
    },
    {
      "epoch": 11.8,
      "grad_norm": 0.4061976671218872,
      "learning_rate": 0.00010847457627118644,
      "loss": 0.0256,
      "step": 141
    },
    {
      "epoch": 11.88888888888889,
      "grad_norm": 0.7276462316513062,
      "learning_rate": 0.00010779661016949153,
      "loss": 0.0462,
      "step": 142
    },
    {
      "epoch": 11.977777777777778,
      "grad_norm": 0.5941165685653687,
      "learning_rate": 0.00010711864406779661,
      "loss": 0.0421,
      "step": 143
    },
    {
      "epoch": 12.0,
      "grad_norm": 1.4154196977615356,
      "learning_rate": 0.0001064406779661017,
      "loss": 0.0523,
      "step": 144
    },
    {
      "epoch": 12.088888888888889,
      "grad_norm": 0.5741814374923706,
      "learning_rate": 0.00010576271186440679,
      "loss": 0.0271,
      "step": 145
    },
    {
      "epoch": 12.177777777777777,
      "grad_norm": 0.6614696979522705,
      "learning_rate": 0.00010508474576271188,
      "loss": 0.0299,
      "step": 146
    },
    {
      "epoch": 12.266666666666667,
      "grad_norm": 0.5370013117790222,
      "learning_rate": 0.00010440677966101696,
      "loss": 0.0286,
      "step": 147
    },
    {
      "epoch": 12.355555555555556,
      "grad_norm": 0.9187875986099243,
      "learning_rate": 0.00010372881355932205,
      "loss": 0.04,
      "step": 148
    },
    {
      "epoch": 12.444444444444445,
      "grad_norm": 0.29871222376823425,
      "learning_rate": 0.00010305084745762712,
      "loss": 0.0265,
      "step": 149
    },
    {
      "epoch": 12.533333333333333,
      "grad_norm": 0.49373310804367065,
      "learning_rate": 0.00010237288135593222,
      "loss": 0.0284,
      "step": 150
    },
    {
      "epoch": 12.622222222222222,
      "grad_norm": 0.4411289095878601,
      "learning_rate": 0.00010169491525423729,
      "loss": 0.0351,
      "step": 151
    },
    {
      "epoch": 12.71111111111111,
      "grad_norm": 0.39246273040771484,
      "learning_rate": 0.00010101694915254238,
      "loss": 0.0253,
      "step": 152
    },
    {
      "epoch": 12.8,
      "grad_norm": 0.2219785451889038,
      "learning_rate": 0.00010033898305084746,
      "loss": 0.0273,
      "step": 153
    },
    {
      "epoch": 12.88888888888889,
      "grad_norm": 0.29543641209602356,
      "learning_rate": 9.966101694915255e-05,
      "loss": 0.0338,
      "step": 154
    },
    {
      "epoch": 12.977777777777778,
      "grad_norm": 0.5159617066383362,
      "learning_rate": 9.898305084745763e-05,
      "loss": 0.0374,
      "step": 155
    },
    {
      "epoch": 13.0,
      "grad_norm": 0.7779985070228577,
      "learning_rate": 9.830508474576272e-05,
      "loss": 0.0355,
      "step": 156
    },
    {
      "epoch": 13.088888888888889,
      "grad_norm": 0.239820197224617,
      "learning_rate": 9.76271186440678e-05,
      "loss": 0.0215,
      "step": 157
    },
    {
      "epoch": 13.177777777777777,
      "grad_norm": 0.6888024210929871,
      "learning_rate": 9.69491525423729e-05,
      "loss": 0.0328,
      "step": 158
    },
    {
      "epoch": 13.266666666666667,
      "grad_norm": 0.24478985369205475,
      "learning_rate": 9.627118644067797e-05,
      "loss": 0.0247,
      "step": 159
    },
    {
      "epoch": 13.355555555555556,
      "grad_norm": 0.5277138948440552,
      "learning_rate": 9.559322033898305e-05,
      "loss": 0.0305,
      "step": 160
    },
    {
      "epoch": 13.444444444444445,
      "grad_norm": 0.24677546322345734,
      "learning_rate": 9.491525423728815e-05,
      "loss": 0.0273,
      "step": 161
    },
    {
      "epoch": 13.533333333333333,
      "grad_norm": 0.44249990582466125,
      "learning_rate": 9.423728813559322e-05,
      "loss": 0.0264,
      "step": 162
    },
    {
      "epoch": 13.622222222222222,
      "grad_norm": 0.33182257413864136,
      "learning_rate": 9.355932203389832e-05,
      "loss": 0.0268,
      "step": 163
    },
    {
      "epoch": 13.71111111111111,
      "grad_norm": 0.18644598126411438,
      "learning_rate": 9.28813559322034e-05,
      "loss": 0.0211,
      "step": 164
    },
    {
      "epoch": 13.8,
      "grad_norm": 0.30595341324806213,
      "learning_rate": 9.220338983050847e-05,
      "loss": 0.0259,
      "step": 165
    },
    {
      "epoch": 13.88888888888889,
      "grad_norm": 0.28083929419517517,
      "learning_rate": 9.152542372881357e-05,
      "loss": 0.0279,
      "step": 166
    },
    {
      "epoch": 13.977777777777778,
      "grad_norm": 0.29913613200187683,
      "learning_rate": 9.084745762711865e-05,
      "loss": 0.029,
      "step": 167
    },
    {
      "epoch": 14.0,
      "grad_norm": 1.2502108812332153,
      "learning_rate": 9.016949152542374e-05,
      "loss": 0.0423,
      "step": 168
    },
    {
      "epoch": 14.088888888888889,
      "grad_norm": 0.21556299924850464,
      "learning_rate": 8.949152542372882e-05,
      "loss": 0.0167,
      "step": 169
    },
    {
      "epoch": 14.177777777777777,
      "grad_norm": 0.18543387949466705,
      "learning_rate": 8.88135593220339e-05,
      "loss": 0.0231,
      "step": 170
    },
    {
      "epoch": 14.266666666666667,
      "grad_norm": 0.1722179353237152,
      "learning_rate": 8.813559322033899e-05,
      "loss": 0.0198,
      "step": 171
    },
    {
      "epoch": 14.355555555555556,
      "grad_norm": 0.36984023451805115,
      "learning_rate": 8.745762711864407e-05,
      "loss": 0.0324,
      "step": 172
    },
    {
      "epoch": 14.444444444444445,
      "grad_norm": 0.19969581067562103,
      "learning_rate": 8.677966101694915e-05,
      "loss": 0.0201,
      "step": 173
    },
    {
      "epoch": 14.533333333333333,
      "grad_norm": 0.3833993375301361,
      "learning_rate": 8.610169491525424e-05,
      "loss": 0.0265,
      "step": 174
    },
    {
      "epoch": 14.622222222222222,
      "grad_norm": 0.5318225026130676,
      "learning_rate": 8.542372881355932e-05,
      "loss": 0.0261,
      "step": 175
    },
    {
      "epoch": 14.71111111111111,
      "grad_norm": 0.20343980193138123,
      "learning_rate": 8.474576271186441e-05,
      "loss": 0.0179,
      "step": 176
    },
    {
      "epoch": 14.8,
      "grad_norm": 0.233334019780159,
      "learning_rate": 8.406779661016949e-05,
      "loss": 0.0255,
      "step": 177
    },
    {
      "epoch": 14.88888888888889,
      "grad_norm": 0.21878962218761444,
      "learning_rate": 8.338983050847457e-05,
      "loss": 0.0222,
      "step": 178
    },
    {
      "epoch": 14.977777777777778,
      "grad_norm": 0.6423946022987366,
      "learning_rate": 8.271186440677966e-05,
      "loss": 0.026,
      "step": 179
    },
    {
      "epoch": 15.0,
      "grad_norm": 0.5274139046669006,
      "learning_rate": 8.203389830508474e-05,
      "loss": 0.0252,
      "step": 180
    },
    {
      "epoch": 15.088888888888889,
      "grad_norm": 0.1551029086112976,
      "learning_rate": 8.135593220338983e-05,
      "loss": 0.0202,
      "step": 181
    },
    {
      "epoch": 15.177777777777777,
      "grad_norm": 0.17991164326667786,
      "learning_rate": 8.067796610169493e-05,
      "loss": 0.0242,
      "step": 182
    },
    {
      "epoch": 15.266666666666667,
      "grad_norm": 0.19775204360485077,
      "learning_rate": 8e-05,
      "loss": 0.0193,
      "step": 183
    },
    {
      "epoch": 15.355555555555556,
      "grad_norm": 0.21448783576488495,
      "learning_rate": 7.93220338983051e-05,
      "loss": 0.0189,
      "step": 184
    },
    {
      "epoch": 15.444444444444445,
      "grad_norm": 0.4896646738052368,
      "learning_rate": 7.864406779661018e-05,
      "loss": 0.0158,
      "step": 185
    },
    {
      "epoch": 15.533333333333333,
      "grad_norm": 0.1670331060886383,
      "learning_rate": 7.796610169491526e-05,
      "loss": 0.0212,
      "step": 186
    },
    {
      "epoch": 15.622222222222222,
      "grad_norm": 0.19792933762073517,
      "learning_rate": 7.728813559322035e-05,
      "loss": 0.0163,
      "step": 187
    },
    {
      "epoch": 15.71111111111111,
      "grad_norm": 0.30398035049438477,
      "learning_rate": 7.661016949152543e-05,
      "loss": 0.0223,
      "step": 188
    },
    {
      "epoch": 15.8,
      "grad_norm": 0.19658952951431274,
      "learning_rate": 7.593220338983051e-05,
      "loss": 0.0175,
      "step": 189
    },
    {
      "epoch": 15.88888888888889,
      "grad_norm": 0.4055933654308319,
      "learning_rate": 7.52542372881356e-05,
      "loss": 0.0417,
      "step": 190
    },
    {
      "epoch": 15.977777777777778,
      "grad_norm": 0.25380900502204895,
      "learning_rate": 7.457627118644068e-05,
      "loss": 0.0282,
      "step": 191
    },
    {
      "epoch": 16.0,
      "grad_norm": 0.5772638320922852,
      "learning_rate": 7.389830508474577e-05,
      "loss": 0.0375,
      "step": 192
    },
    {
      "epoch": 16.08888888888889,
      "grad_norm": 0.12425500154495239,
      "learning_rate": 7.322033898305085e-05,
      "loss": 0.0159,
      "step": 193
    },
    {
      "epoch": 16.177777777777777,
      "grad_norm": 0.14470212161540985,
      "learning_rate": 7.254237288135593e-05,
      "loss": 0.0172,
      "step": 194
    },
    {
      "epoch": 16.266666666666666,
      "grad_norm": 0.26113924384117126,
      "learning_rate": 7.186440677966102e-05,
      "loss": 0.027,
      "step": 195
    },
    {
      "epoch": 16.355555555555554,
      "grad_norm": 0.15274882316589355,
      "learning_rate": 7.11864406779661e-05,
      "loss": 0.0173,
      "step": 196
    },
    {
      "epoch": 16.444444444444443,
      "grad_norm": 0.40399596095085144,
      "learning_rate": 7.05084745762712e-05,
      "loss": 0.0178,
      "step": 197
    },
    {
      "epoch": 16.533333333333335,
      "grad_norm": 0.2274380624294281,
      "learning_rate": 6.983050847457627e-05,
      "loss": 0.018,
      "step": 198
    },
    {
      "epoch": 16.622222222222224,
      "grad_norm": 0.1495862603187561,
      "learning_rate": 6.915254237288135e-05,
      "loss": 0.0199,
      "step": 199
    },
    {
      "epoch": 16.711111111111112,
      "grad_norm": 0.20856371521949768,
      "learning_rate": 6.847457627118645e-05,
      "loss": 0.0258,
      "step": 200
    },
    {
      "epoch": 16.8,
      "grad_norm": 0.28339046239852905,
      "learning_rate": 6.779661016949152e-05,
      "loss": 0.0284,
      "step": 201
    },
    {
      "epoch": 16.88888888888889,
      "grad_norm": 0.27622517943382263,
      "learning_rate": 6.71186440677966e-05,
      "loss": 0.0266,
      "step": 202
    },
    {
      "epoch": 16.977777777777778,
      "grad_norm": 0.2302788645029068,
      "learning_rate": 6.64406779661017e-05,
      "loss": 0.0227,
      "step": 203
    },
    {
      "epoch": 17.0,
      "grad_norm": 0.41939589381217957,
      "learning_rate": 6.576271186440678e-05,
      "loss": 0.0124,
      "step": 204
    },
    {
      "epoch": 17.08888888888889,
      "grad_norm": 0.14180906116962433,
      "learning_rate": 6.508474576271187e-05,
      "loss": 0.0187,
      "step": 205
    },
    {
      "epoch": 17.177777777777777,
      "grad_norm": 0.1920175552368164,
      "learning_rate": 6.440677966101695e-05,
      "loss": 0.0229,
      "step": 206
    },
    {
      "epoch": 17.266666666666666,
      "grad_norm": 0.1474781483411789,
      "learning_rate": 6.372881355932203e-05,
      "loss": 0.0193,
      "step": 207
    },
    {
      "epoch": 17.355555555555554,
      "grad_norm": 0.1655988097190857,
      "learning_rate": 6.305084745762713e-05,
      "loss": 0.0192,
      "step": 208
    },
    {
      "epoch": 17.444444444444443,
      "grad_norm": 0.13226042687892914,
      "learning_rate": 6.237288135593221e-05,
      "loss": 0.017,
      "step": 209
    },
    {
      "epoch": 17.533333333333335,
      "grad_norm": 0.16019178926944733,
      "learning_rate": 6.169491525423729e-05,
      "loss": 0.014,
      "step": 210
    },
    {
      "epoch": 17.622222222222224,
      "grad_norm": 0.17021887004375458,
      "learning_rate": 6.101694915254238e-05,
      "loss": 0.0164,
      "step": 211
    },
    {
      "epoch": 17.711111111111112,
      "grad_norm": 0.14306345582008362,
      "learning_rate": 6.033898305084746e-05,
      "loss": 0.0138,
      "step": 212
    },
    {
      "epoch": 17.8,
      "grad_norm": 0.15581893920898438,
      "learning_rate": 5.966101694915255e-05,
      "loss": 0.0187,
      "step": 213
    },
    {
      "epoch": 17.88888888888889,
      "grad_norm": 0.191217303276062,
      "learning_rate": 5.8983050847457634e-05,
      "loss": 0.0204,
      "step": 214
    },
    {
      "epoch": 17.977777777777778,
      "grad_norm": 0.18620988726615906,
      "learning_rate": 5.830508474576272e-05,
      "loss": 0.0214,
      "step": 215
    },
    {
      "epoch": 18.0,
      "grad_norm": 0.7624273896217346,
      "learning_rate": 5.76271186440678e-05,
      "loss": 0.0282,
      "step": 216
    },
    {
      "epoch": 18.08888888888889,
      "grad_norm": 0.1546453833580017,
      "learning_rate": 5.6949152542372884e-05,
      "loss": 0.0176,
      "step": 217
    },
    {
      "epoch": 18.177777777777777,
      "grad_norm": 0.183871790766716,
      "learning_rate": 5.627118644067797e-05,
      "loss": 0.0214,
      "step": 218
    },
    {
      "epoch": 18.266666666666666,
      "grad_norm": 0.15087799727916718,
      "learning_rate": 5.5593220338983056e-05,
      "loss": 0.0201,
      "step": 219
    },
    {
      "epoch": 18.355555555555554,
      "grad_norm": 0.11985763162374496,
      "learning_rate": 5.4915254237288135e-05,
      "loss": 0.0145,
      "step": 220
    },
    {
      "epoch": 18.444444444444443,
      "grad_norm": 0.18514318764209747,
      "learning_rate": 5.423728813559322e-05,
      "loss": 0.0232,
      "step": 221
    },
    {
      "epoch": 18.533333333333335,
      "grad_norm": 0.16298696398735046,
      "learning_rate": 5.355932203389831e-05,
      "loss": 0.0181,
      "step": 222
    },
    {
      "epoch": 18.622222222222224,
      "grad_norm": 0.12706273794174194,
      "learning_rate": 5.288135593220339e-05,
      "loss": 0.0164,
      "step": 223
    },
    {
      "epoch": 18.711111111111112,
      "grad_norm": 0.15040437877178192,
      "learning_rate": 5.220338983050848e-05,
      "loss": 0.0214,
      "step": 224
    },
    {
      "epoch": 18.8,
      "grad_norm": 0.1334621012210846,
      "learning_rate": 5.152542372881356e-05,
      "loss": 0.0181,
      "step": 225
    },
    {
      "epoch": 18.88888888888889,
      "grad_norm": 0.1662566214799881,
      "learning_rate": 5.0847457627118643e-05,
      "loss": 0.0156,
      "step": 226
    },
    {
      "epoch": 18.977777777777778,
      "grad_norm": 0.14159251749515533,
      "learning_rate": 5.016949152542373e-05,
      "loss": 0.0176,
      "step": 227
    },
    {
      "epoch": 19.0,
      "grad_norm": 0.20420512557029724,
      "learning_rate": 4.9491525423728815e-05,
      "loss": 0.0182,
      "step": 228
    },
    {
      "epoch": 19.08888888888889,
      "grad_norm": 0.15357191860675812,
      "learning_rate": 4.88135593220339e-05,
      "loss": 0.018,
      "step": 229
    },
    {
      "epoch": 19.177777777777777,
      "grad_norm": 0.19103069603443146,
      "learning_rate": 4.813559322033899e-05,
      "loss": 0.0179,
      "step": 230
    },
    {
      "epoch": 19.266666666666666,
      "grad_norm": 0.16087281703948975,
      "learning_rate": 4.745762711864407e-05,
      "loss": 0.0265,
      "step": 231
    },
    {
      "epoch": 19.355555555555554,
      "grad_norm": 0.1447564959526062,
      "learning_rate": 4.677966101694916e-05,
      "loss": 0.0198,
      "step": 232
    },
    {
      "epoch": 19.444444444444443,
      "grad_norm": 0.1488998830318451,
      "learning_rate": 4.610169491525424e-05,
      "loss": 0.0164,
      "step": 233
    },
    {
      "epoch": 19.533333333333335,
      "grad_norm": 0.14090055227279663,
      "learning_rate": 4.542372881355932e-05,
      "loss": 0.0199,
      "step": 234
    },
    {
      "epoch": 19.622222222222224,
      "grad_norm": 0.16535541415214539,
      "learning_rate": 4.474576271186441e-05,
      "loss": 0.0197,
      "step": 235
    },
    {
      "epoch": 19.711111111111112,
      "grad_norm": 0.16236133873462677,
      "learning_rate": 4.4067796610169495e-05,
      "loss": 0.0179,
      "step": 236
    },
    {
      "epoch": 19.8,
      "grad_norm": 0.14401188492774963,
      "learning_rate": 4.3389830508474574e-05,
      "loss": 0.0176,
      "step": 237
    },
    {
      "epoch": 19.88888888888889,
      "grad_norm": 0.14514321088790894,
      "learning_rate": 4.271186440677966e-05,
      "loss": 0.0129,
      "step": 238
    },
    {
      "epoch": 19.977777777777778,
      "grad_norm": 0.16158796846866608,
      "learning_rate": 4.2033898305084746e-05,
      "loss": 0.0173,
      "step": 239
    },
    {
      "epoch": 20.0,
      "grad_norm": 0.26672789454460144,
      "learning_rate": 4.135593220338983e-05,
      "loss": 0.0077,
      "step": 240
    },
    {
      "epoch": 20.08888888888889,
      "grad_norm": 0.13523907959461212,
      "learning_rate": 4.067796610169492e-05,
      "loss": 0.0179,
      "step": 241
    },
    {
      "epoch": 20.177777777777777,
      "grad_norm": 0.14746247231960297,
      "learning_rate": 4e-05,
      "loss": 0.0153,
      "step": 242
    },
    {
      "epoch": 20.266666666666666,
      "grad_norm": 0.15225662291049957,
      "learning_rate": 3.932203389830509e-05,
      "loss": 0.016,
      "step": 243
    },
    {
      "epoch": 20.355555555555554,
      "grad_norm": 0.0848919153213501,
      "learning_rate": 3.8644067796610175e-05,
      "loss": 0.0112,
      "step": 244
    },
    {
      "epoch": 20.444444444444443,
      "grad_norm": 0.1826402246952057,
      "learning_rate": 3.7966101694915254e-05,
      "loss": 0.0232,
      "step": 245
    },
    {
      "epoch": 20.533333333333335,
      "grad_norm": 0.17501889169216156,
      "learning_rate": 3.728813559322034e-05,
      "loss": 0.0202,
      "step": 246
    },
    {
      "epoch": 20.622222222222224,
      "grad_norm": 0.18110467493534088,
      "learning_rate": 3.6610169491525426e-05,
      "loss": 0.0198,
      "step": 247
    },
    {
      "epoch": 20.711111111111112,
      "grad_norm": 0.18494710326194763,
      "learning_rate": 3.593220338983051e-05,
      "loss": 0.018,
      "step": 248
    },
    {
      "epoch": 20.8,
      "grad_norm": 0.14503537118434906,
      "learning_rate": 3.52542372881356e-05,
      "loss": 0.0203,
      "step": 249
    },
    {
      "epoch": 20.88888888888889,
      "grad_norm": 0.1527281254529953,
      "learning_rate": 3.4576271186440676e-05,
      "loss": 0.0175,
      "step": 250
    },
    {
      "epoch": 20.977777777777778,
      "grad_norm": 0.17254236340522766,
      "learning_rate": 3.389830508474576e-05,
      "loss": 0.0156,
      "step": 251
    },
    {
      "epoch": 21.0,
      "grad_norm": 0.5792056322097778,
      "learning_rate": 3.322033898305085e-05,
      "loss": 0.0302,
      "step": 252
    },
    {
      "epoch": 21.08888888888889,
      "grad_norm": 0.17945076525211334,
      "learning_rate": 3.2542372881355934e-05,
      "loss": 0.0205,
      "step": 253
    },
    {
      "epoch": 21.177777777777777,
      "grad_norm": 0.15893131494522095,
      "learning_rate": 3.186440677966101e-05,
      "loss": 0.0192,
      "step": 254
    },
    {
      "epoch": 21.266666666666666,
      "grad_norm": 0.18068188428878784,
      "learning_rate": 3.1186440677966106e-05,
      "loss": 0.021,
      "step": 255
    },
    {
      "epoch": 21.355555555555554,
      "grad_norm": 0.1309238076210022,
      "learning_rate": 3.050847457627119e-05,
      "loss": 0.0149,
      "step": 256
    },
    {
      "epoch": 21.444444444444443,
      "grad_norm": 0.1609637290239334,
      "learning_rate": 2.9830508474576274e-05,
      "loss": 0.0218,
      "step": 257
    },
    {
      "epoch": 21.533333333333335,
      "grad_norm": 0.11811161786317825,
      "learning_rate": 2.915254237288136e-05,
      "loss": 0.0132,
      "step": 258
    },
    {
      "epoch": 21.622222222222224,
      "grad_norm": 0.17464028298854828,
      "learning_rate": 2.8474576271186442e-05,
      "loss": 0.0158,
      "step": 259
    },
    {
      "epoch": 21.711111111111112,
      "grad_norm": 0.13908031582832336,
      "learning_rate": 2.7796610169491528e-05,
      "loss": 0.0151,
      "step": 260
    },
    {
      "epoch": 21.8,
      "grad_norm": 0.14620617032051086,
      "learning_rate": 2.711864406779661e-05,
      "loss": 0.0182,
      "step": 261
    },
    {
      "epoch": 21.88888888888889,
      "grad_norm": 0.1358851045370102,
      "learning_rate": 2.6440677966101696e-05,
      "loss": 0.015,
      "step": 262
    },
    {
      "epoch": 21.977777777777778,
      "grad_norm": 0.13740763068199158,
      "learning_rate": 2.576271186440678e-05,
      "loss": 0.016,
      "step": 263
    },
    {
      "epoch": 22.0,
      "grad_norm": 0.4503135681152344,
      "learning_rate": 2.5084745762711865e-05,
      "loss": 0.0132,
      "step": 264
    },
    {
      "epoch": 22.08888888888889,
      "grad_norm": 0.10800081491470337,
      "learning_rate": 2.440677966101695e-05,
      "loss": 0.0148,
      "step": 265
    },
    {
      "epoch": 22.177777777777777,
      "grad_norm": 0.2126515805721283,
      "learning_rate": 2.3728813559322036e-05,
      "loss": 0.0174,
      "step": 266
    },
    {
      "epoch": 22.266666666666666,
      "grad_norm": 0.13310760259628296,
      "learning_rate": 2.305084745762712e-05,
      "loss": 0.0158,
      "step": 267
    },
    {
      "epoch": 22.355555555555554,
      "grad_norm": 0.1338469237089157,
      "learning_rate": 2.2372881355932205e-05,
      "loss": 0.0136,
      "step": 268
    },
    {
      "epoch": 22.444444444444443,
      "grad_norm": 0.14930036664009094,
      "learning_rate": 2.1694915254237287e-05,
      "loss": 0.0153,
      "step": 269
    },
    {
      "epoch": 22.533333333333335,
      "grad_norm": 0.14385239779949188,
      "learning_rate": 2.1016949152542373e-05,
      "loss": 0.0158,
      "step": 270
    },
    {
      "epoch": 22.622222222222224,
      "grad_norm": 0.14195145666599274,
      "learning_rate": 2.033898305084746e-05,
      "loss": 0.016,
      "step": 271
    },
    {
      "epoch": 22.711111111111112,
      "grad_norm": 0.17930911481380463,
      "learning_rate": 1.9661016949152545e-05,
      "loss": 0.021,
      "step": 272
    },
    {
      "epoch": 22.8,
      "grad_norm": 0.1334490031003952,
      "learning_rate": 1.8983050847457627e-05,
      "loss": 0.0147,
      "step": 273
    },
    {
      "epoch": 22.88888888888889,
      "grad_norm": 0.1466618925333023,
      "learning_rate": 1.8305084745762713e-05,
      "loss": 0.018,
      "step": 274
    },
    {
      "epoch": 22.977777777777778,
      "grad_norm": 0.19424818456172943,
      "learning_rate": 1.76271186440678e-05,
      "loss": 0.0229,
      "step": 275
    },
    {
      "epoch": 23.0,
      "grad_norm": 0.5129978060722351,
      "learning_rate": 1.694915254237288e-05,
      "loss": 0.0179,
      "step": 276
    },
    {
      "epoch": 23.08888888888889,
      "grad_norm": 0.14142821729183197,
      "learning_rate": 1.6271186440677967e-05,
      "loss": 0.015,
      "step": 277
    },
    {
      "epoch": 23.177777777777777,
      "grad_norm": 0.1314881443977356,
      "learning_rate": 1.5593220338983053e-05,
      "loss": 0.0165,
      "step": 278
    },
    {
      "epoch": 23.266666666666666,
      "grad_norm": 0.1740553081035614,
      "learning_rate": 1.4915254237288137e-05,
      "loss": 0.0172,
      "step": 279
    },
    {
      "epoch": 23.355555555555554,
      "grad_norm": 0.1823374330997467,
      "learning_rate": 1.4237288135593221e-05,
      "loss": 0.0189,
      "step": 280
    },
    {
      "epoch": 23.444444444444443,
      "grad_norm": 0.11804287135601044,
      "learning_rate": 1.3559322033898305e-05,
      "loss": 0.0148,
      "step": 281
    },
    {
      "epoch": 23.533333333333335,
      "grad_norm": 0.16271574795246124,
      "learning_rate": 1.288135593220339e-05,
      "loss": 0.0158,
      "step": 282
    },
    {
      "epoch": 23.622222222222224,
      "grad_norm": 0.11030644178390503,
      "learning_rate": 1.2203389830508475e-05,
      "loss": 0.0132,
      "step": 283
    },
    {
      "epoch": 23.711111111111112,
      "grad_norm": 0.16742020845413208,
      "learning_rate": 1.152542372881356e-05,
      "loss": 0.0185,
      "step": 284
    },
    {
      "epoch": 23.8,
      "grad_norm": 0.1539010852575302,
      "learning_rate": 1.0847457627118644e-05,
      "loss": 0.0182,
      "step": 285
    },
    {
      "epoch": 23.88888888888889,
      "grad_norm": 0.2396184504032135,
      "learning_rate": 1.016949152542373e-05,
      "loss": 0.0194,
      "step": 286
    },
    {
      "epoch": 23.977777777777778,
      "grad_norm": 0.13578741252422333,
      "learning_rate": 9.491525423728814e-06,
      "loss": 0.0138,
      "step": 287
    },
    {
      "epoch": 24.0,
      "grad_norm": 0.3250972032546997,
      "learning_rate": 8.8135593220339e-06,
      "loss": 0.0222,
      "step": 288
    },
    {
      "epoch": 24.08888888888889,
      "grad_norm": 0.14024260640144348,
      "learning_rate": 8.135593220338983e-06,
      "loss": 0.0146,
      "step": 289
    },
    {
      "epoch": 24.177777777777777,
      "grad_norm": 0.12179547548294067,
      "learning_rate": 7.4576271186440685e-06,
      "loss": 0.0126,
      "step": 290
    },
    {
      "epoch": 24.266666666666666,
      "grad_norm": 0.13877426087856293,
      "learning_rate": 6.779661016949153e-06,
      "loss": 0.0135,
      "step": 291
    },
    {
      "epoch": 24.355555555555554,
      "grad_norm": 0.1424354463815689,
      "learning_rate": 6.101694915254238e-06,
      "loss": 0.018,
      "step": 292
    },
    {
      "epoch": 24.444444444444443,
      "grad_norm": 0.1883004754781723,
      "learning_rate": 5.423728813559322e-06,
      "loss": 0.019,
      "step": 293
    },
    {
      "epoch": 24.533333333333335,
      "grad_norm": 0.1537894904613495,
      "learning_rate": 4.745762711864407e-06,
      "loss": 0.0173,
      "step": 294
    },
    {
      "epoch": 24.622222222222224,
      "grad_norm": 0.13196897506713867,
      "learning_rate": 4.067796610169492e-06,
      "loss": 0.0148,
      "step": 295
    },
    {
      "epoch": 24.711111111111112,
      "grad_norm": 0.15588884055614471,
      "learning_rate": 3.3898305084745763e-06,
      "loss": 0.0172,
      "step": 296
    },
    {
      "epoch": 24.8,
      "grad_norm": 0.15132813155651093,
      "learning_rate": 2.711864406779661e-06,
      "loss": 0.0186,
      "step": 297
    },
    {
      "epoch": 24.88888888888889,
      "grad_norm": 0.1587650030851364,
      "learning_rate": 2.033898305084746e-06,
      "loss": 0.0179,
      "step": 298
    },
    {
      "epoch": 24.977777777777778,
      "grad_norm": 0.1492483913898468,
      "learning_rate": 1.3559322033898304e-06,
      "loss": 0.0147,
      "step": 299
    },
    {
      "epoch": 25.0,
      "grad_norm": 0.19971533119678497,
      "learning_rate": 6.779661016949152e-07,
      "loss": 0.0165,
      "step": 300
    }
  ],
  "logging_steps": 1,
  "max_steps": 300,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 25,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.936640855425843e+16,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
